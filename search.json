[
  {
    "objectID": "project-work.html",
    "href": "project-work.html",
    "title": "Project work",
    "section": "",
    "text": "Here shows group project‚Äôs proposal.\n\nGroup 1:\nGroup 2:\nGroup 3:\nGroup 4:"
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Useful links",
    "section": "",
    "text": "RStudio containers\nüîó on Duke Container Manager\n\n\nCourse GitHub organization\nüîó on GitHub\n\n\nDiscussion forum\nüîó on Sakai\n\n\nLecture streaming and recordings\nüîó on Panopto\n\n\nGradebook\nüîó on Sakai\n\n\nVirtual meetings\nüîó on Sakai"
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "MSSC 6250 - Spring 2024",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (‚ÄúPublic License‚Äù). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 ‚Äì Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter‚Äôs License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 ‚Äì Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor ‚Äì Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor ‚Äì Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter‚Äôs License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 ‚Äì License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter‚Äôs License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter‚Äôs License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter‚Äôs License You apply.\n\n\n\nSection 4 ‚Äì Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 ‚Äì Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 ‚Äì Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 ‚Äì Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 ‚Äì Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the ‚ÄúLicensor.‚Äù The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark ‚ÄúCreative Commons‚Äù or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "project-description-5780.html",
    "href": "project-description-5780.html",
    "title": "MSSC 5780 Project Description",
    "section": "",
    "text": "The final project written report and participation of MATH 4780 project presentation account for 220 points of the total 1200 points."
  },
  {
    "objectID": "project-description-5780.html#timeline-and-things-to-do",
    "href": "project-description-5780.html#timeline-and-things-to-do",
    "title": "MSSC 5780 Project Description",
    "section": "Timeline and Things to Do",
    "text": "Timeline and Things to Do\n\nTeam up! You work as a team of two or three. No individual project. One of you, please email me your group member list by Friday, 11/17 11:59 PM.\n\nYou lose 20 points if you miss the deadline.\nYou will be randomly assigned to a group if you do not belong to any group before the deadline.\n\nProposal. Please send me a one-page PDF describing what you are going to do for your project (no word limit) with your project title by Friday, 12/1 11:59 PM.\n\nYou lose 20 points if you miss the deadline.\n\nMaterial. Please submit your report and code to D2L by Tuesday, 12/12 11:59 PM.\n\nYou receive 0 point if you miss the deadline."
  },
  {
    "objectID": "project-description-5780.html#project-writing",
    "href": "project-description-5780.html#project-writing",
    "title": "MSSC 5780 Project Description",
    "section": "Project Writing",
    "text": "Project Writing\nYour project can be in either of the following categories:\n\nData Analysis (DA) using one or more regression methods learned in class.\nIntroduce a new regression model/method/algorithm (RM) and compare it with the model/method/algorithms learned in class.\n\n\nRegression Content\nIf you choose to do DA, your project should:\n\nInclude regression diagnostics: model adequacy checking, residual diagnostics, leverage and influence diagnostics, and collinearity diagnostics.\nExplain how you deal with violation of assumptions and collinearity issue if it exists.\nDemonstrate that your final selected model has no violation of assumptions and answers your questions well through any inference (estimation and testing) or prediction methods. If some issues remain, explain why they cannot be fixed, and what is the limitation of your model due to these issues. Provide suggestions for improving your analysis.\nThe chosen data set cannot be any data set used in class, the textbook or homework assignments.\nBelow are a list of data repositories you may start with, but you are encouraged to explore more and find your favorite one.\n\nTidyTuesday\nKaggle\nAwesome Public Datasets\nHarvard Dataverse\nUCI Machine Learning Repository\nFiveThirtyEight\n\n\nIf you choose to do RM, your project should:\n\nExplain the model/method. Define and explain the notations, parameters, and model/method assumptions. Compare it with the model/method learned in class.\nShow the model/method with graphics and possible geometrical meaning. Compare it with the model/method learned in class.\nShow how to perform parameter estimation and response prediction. Discuss the methods used for training/fitting the model, and compare the prediction performance with MLR.\nInterpret the fitted and prediction results.\nDiscuss the advantages and disadvantages of the model/method. Under what situations the model/method performs the best/worst?\nDemonstrate how to fit the model and implement its relevant method/algorithm via a simulation study or data analysis. The data set could be a data set used in class.\nPossible topics being considered include but not limited to:\n\nBayesian linear regression\nGaussian process regression\nLASSO\nPoisson regression\nRegression trees\nMultivariate adaptive regression splines\nGeneralized additive model\nRobust regression\nQuantile regression\nPartial Least Squares Regression\n\n\n\n\nPaper Structure\nIf you choose to do DA, your report should include the following sections:\n\nIntroduction: State why you think the questions you would like to answer are important or interesting, and why you think the method(s) you consider is an appropriate one to answer your questions.\nData: Describe the selected data set. Perform a thorough exploratory data analysis.\nAnalysis: Include the Regression Content by\n\nExplaining the chosen model/method.\nShowing why the chosen model(s) is appropriate and better than others.\nAnswering your research questions by the analysis result.\n\nConclusion: Restate your research question, and summarize how you learn from data to answer your questions. What is the contribution of this project? Discuss any limitation of your model/method, and how it could be improved for better inference or prediction results.\nReferences/Bibliography: Include a detailed list of references, including papers, books, websites, code, and any idea/work that is not produced by yourself.\n\nIf you choose to do RM, your report should include the following sections:\n\nIntroduction: State why you choose to learn this new method. Provide an overview and little history of the method. Describe the intuition and idea of the method. What are the pros and cons of the method?\nModel/Method: Provide the mathematical expression of the model. Address the points mentioned in Regression Content.\nSimulation: Do a simulation study, and compare the chosen method with other methods learned in class. Determine which method performs better under what conditions. Address the points mentioned in Regression Content.\nDiscussion: Based on the simulation results, discuss the advantages and disadvantages of the chosen method. Discuss any variants of the chosen method.\nReferences/Bibliography: Include a detailed list of references, including papers, books, websites, code, and any idea/work that is not produced by yourself.\n\n\n\nFormat\n\nExcept the project title and section title, the font size is 12 pt.\nYour paper should have your project title and your name on the first page. Date, Abstract, Keywords are optional.\nPlease use 1.5 or double line spacing.\nYour report, including everything, should have at least 10 pages, but no more than 13 pages.\nYour code should NOT be included in the paper.\n\n\n\nCode\n\nYour code should be able to reproduce all the numerical results, outputs, tables, and figures shown in the report, including the source of the raw data (where you find and load the data) if the project is about data analysis."
  },
  {
    "objectID": "project-description-5780.html#project-evaluation",
    "href": "project-description-5780.html#project-evaluation",
    "title": "MSSC 5780 Project Description",
    "section": "Project Evaluation",
    "text": "Project Evaluation\nYour project will be evaluated based on\n\nContent:\n\nThe quality of research question and relevancy of data to those questions? For example, the relationship between human height and weight is a BAD question. An elementary-school height and weight data set is a BAD data set.\nThe quality of the chosen model. For example, one-way ANOVA is a BAD model.\n\nCorrectness, Completeness and Complexity:\n\nAre the regression methods carried out and explained correctly?\nDoes project include rigorous analysis and models? Simple linear regression model lacks complexity.\n\nWriting: The quality of the regression model/method presentation, visualization, writing, and explanations.\nFormat: Does the report follow the required format?\nCreativity and Critical Thought: Is the project carefully thought out? Are the limitations carefully considered? Does it appear that time and effort went into the planning and implementation of the project?\nReproducibility: Can your code reproduce what you show in the paper?\nReference: Do you cite others work properly?"
  },
  {
    "objectID": "project-description-5780.html#assessment-policy",
    "href": "project-description-5780.html#assessment-policy",
    "title": "MSSC 5780 Project Description",
    "section": "Assessment Policy",
    "text": "Assessment Policy\n\nYou evaluate group performance based on the four criteria:\n\nProject Content and Organization (8 pts)\nSlides Quality (4 pts)\nOral Presentation Skill and Delivery (4 pts)\nInteractions and Q&A (4 pts)\n\nThe total points of a project presentation is 20 points.\nEvaluation sheets will be provided on the presentation day.\nContent and Organization\n\nClear visualization that helps find out relationship of variables and specification of models\nAll questions are answered accurately by the models\nDiscuss how and why the models are chosen\nApply sophisticated models and detailed analysis including diagnostics\nAll ideas are presented in logical order\n\nSlides Quality\n\nSlides show code and output beautifully\nSlides clearly aid the speaker in telling a coherent story\nAll tables and graphics are informative and related to the topic and make it easier to understand\nAttractive design, layout, and neatness.\n\nOral Presentation Skill\n\nGood volume and energy\nProper pace and diction\nAvoidance of distracting gestures\n\nInteractions and Q&A\n\nGood eye contact with audience\nExcellent listening skills\nAnswers audience questions with authority and accuracy\n\nAfter you evaluate group project presentations, you rank them from 1st to the last based on their earned points.\nNo two groups receive the same ranking. If you give two or more groups some points, you still need to give them a different ranking, deciding which teams deserve a higher ranking according to your personal preference.\n\nDr.¬†Yu reserves the right to make changes to the project policy."
  },
  {
    "objectID": "weeks/week-6.html",
    "href": "weeks/week-6.html",
    "title": "Week 6",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-6.html#participate",
    "href": "weeks/week-6.html#participate",
    "title": "Week 6",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides -Visualizing Data\n openintro::loans_full_schema"
  },
  {
    "objectID": "weeks/week-6.html#reading-and-resources",
    "href": "weeks/week-6.html#reading-and-resources",
    "title": "Week 6",
    "section": "Reading and Resources",
    "text": "Reading and Resources\nüìñ More add-on üì¶: ggplot2 extensions\nüìñ The R Graph Gallery\nüìñ R Graphics Cookbook\nüìñ R CHARTS"
  },
  {
    "objectID": "weeks/week-6.html#exercise",
    "href": "weeks/week-6.html#exercise",
    "title": "Week 6",
    "section": "Exercise",
    "text": "Exercise\nüìã Lab-13 Visualization\n penguins.csv\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-7.html",
    "href": "weeks/week-7.html",
    "title": "Week 7",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-7.html#participate",
    "href": "weeks/week-7.html#participate",
    "title": "Week 7",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Interactive Data Vaisualization\nüñ•Ô∏è Slides - Data Wrangling (one data frame)\n Data - loans\n Data - Murders"
  },
  {
    "objectID": "weeks/week-7.html#reading-and-resources",
    "href": "weeks/week-7.html#reading-and-resources",
    "title": "Week 7",
    "section": "Reading and Resources",
    "text": "Reading and Resources\nüìñ R for Data Science - Data Transformation"
  },
  {
    "objectID": "weeks/week-7.html#exercise",
    "href": "weeks/week-7.html#exercise",
    "title": "Week 7",
    "section": "Exercise",
    "text": "Exercise\nüìã Lab-14 Interactive Visualization (Present)\nüìã Lab-15 dplyr\n Data - Murders\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-5.html",
    "href": "weeks/week-5.html",
    "title": "Week 5",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-5.html#participate",
    "href": "weeks/week-5.html#participate",
    "title": "Week 5",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Data Importing\nüñ•Ô∏è Slides - ggplot2"
  },
  {
    "objectID": "weeks/week-5.html#reading-and-resources",
    "href": "weeks/week-5.html#reading-and-resources",
    "title": "Week 5",
    "section": "Reading and Resources",
    "text": "Reading and Resources\nüìñ R for Data Science - Data Import\nüìñ R for Data Science - Data Visualization"
  },
  {
    "objectID": "weeks/week-5.html#exercise",
    "href": "weeks/week-5.html#exercise",
    "title": "Week 5",
    "section": "Exercise",
    "text": "Exercise\nüìã Lab-10 Data Importing\nüìã Lab-11 ggplot2\nüìã Lab-12 Faceting\n penguins.csv\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-4.html",
    "href": "weeks/week-4.html",
    "title": "Week 4",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-4.html#participate",
    "href": "weeks/week-4.html#participate",
    "title": "Week 4",
    "section": "Participate",
    "text": "Participate"
  },
  {
    "objectID": "weeks/week-4.html#reading-and-resources",
    "href": "weeks/week-4.html#reading-and-resources",
    "title": "Week 4",
    "section": "Reading and Resources",
    "text": "Reading and Resources"
  },
  {
    "objectID": "weeks/week-4.html#perform",
    "href": "weeks/week-4.html#perform",
    "title": "Week 4",
    "section": "Perform",
    "text": "Perform\n\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-1.html",
    "href": "weeks/week-1.html",
    "title": "Week 1",
    "section": "",
    "text": "üìñ Read the syllabus\nüìñ Get your laptop and computing environment ready!"
  },
  {
    "objectID": "weeks/week-1.html#reading-and-resources",
    "href": "weeks/week-1.html#reading-and-resources",
    "title": "Week 1",
    "section": "Reading and Resources",
    "text": "Reading and Resources"
  },
  {
    "objectID": "weeks/week-1.html#participate",
    "href": "weeks/week-1.html#participate",
    "title": "Week 1",
    "section": "Participate",
    "text": "Participate"
  },
  {
    "objectID": "weeks/week-1.html#perform",
    "href": "weeks/week-1.html#perform",
    "title": "Week 1",
    "section": "Perform",
    "text": "Perform\n\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-3.html",
    "href": "weeks/week-3.html",
    "title": "Week 3",
    "section": "",
    "text": "Important\n\n\n\n\nPresenting Lab 07-Plotting for extra points!"
  },
  {
    "objectID": "weeks/week-3.html#participate",
    "href": "weeks/week-3.html#participate",
    "title": "Week 3",
    "section": "Participate",
    "text": "Participate"
  },
  {
    "objectID": "weeks/week-3.html#reading-and-resources",
    "href": "weeks/week-3.html#reading-and-resources",
    "title": "Week 3",
    "section": "Reading and Resources",
    "text": "Reading and Resources"
  },
  {
    "objectID": "weeks/week-3.html#exercise",
    "href": "weeks/week-3.html#exercise",
    "title": "Week 3",
    "section": "Exercise",
    "text": "Exercise\n\n\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Week 2",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-2.html#reading-and-resources",
    "href": "weeks/week-2.html#reading-and-resources",
    "title": "Week 2",
    "section": "Reading and Resources",
    "text": "Reading and Resources"
  },
  {
    "objectID": "weeks/week-2.html#participate",
    "href": "weeks/week-2.html#participate",
    "title": "Week 2",
    "section": "Participate",
    "text": "Participate"
  },
  {
    "objectID": "weeks/week-2.html#perform",
    "href": "weeks/week-2.html#perform",
    "title": "Week 2",
    "section": "Perform",
    "text": "Perform\n\n\n\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-10.html",
    "href": "weeks/week-10.html",
    "title": "Week 10",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-10.html#participate",
    "href": "weeks/week-10.html#participate",
    "title": "Week 10",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Probability and Statistics\nüñ•Ô∏è Slides - Linear Regression\n ggplot2::mpg"
  },
  {
    "objectID": "weeks/week-10.html#exercise",
    "href": "weeks/week-10.html#exercise",
    "title": "Week 10",
    "section": "Exercise",
    "text": "Exercise\nüìã Lab-18 Probability\nüìã Lab-19 Confidence Interval\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-11.html",
    "href": "weeks/week-11.html",
    "title": "Week 11",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-11.html#participate",
    "href": "weeks/week-11.html#participate",
    "title": "Week 11",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Linear Regression\n ggplot2::mpg"
  },
  {
    "objectID": "weeks/week-11.html#exercise",
    "href": "weeks/week-11.html#exercise",
    "title": "Week 11",
    "section": "Exercise",
    "text": "Exercise\nüìã Lab-20 Simple Linear Regression\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-13.html",
    "href": "weeks/week-13.html",
    "title": "Week 13",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-13.html#participate",
    "href": "weeks/week-13.html#participate",
    "title": "Week 13",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - K Nearest Neighbors\n Data - body"
  },
  {
    "objectID": "weeks/week-13.html#reading",
    "href": "weeks/week-13.html#reading",
    "title": "Week 13",
    "section": "Reading",
    "text": "Reading\nüìñ"
  },
  {
    "objectID": "weeks/week-13.html#exercise",
    "href": "weeks/week-13.html#exercise",
    "title": "Week 13",
    "section": "Exercise",
    "text": "Exercise\nüìã Lab-22 K Nearest Neighbors\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-12.html",
    "href": "weeks/week-12.html",
    "title": "Week 12",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-12.html#participate",
    "href": "weeks/week-12.html#participate",
    "title": "Week 12",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Logistic Regression\n Data - body"
  },
  {
    "objectID": "weeks/week-12.html#reading",
    "href": "weeks/week-12.html#reading",
    "title": "Week 12",
    "section": "Reading",
    "text": "Reading\nüìñ"
  },
  {
    "objectID": "weeks/week-12.html#exercise",
    "href": "weeks/week-12.html#exercise",
    "title": "Week 12",
    "section": "Exercise",
    "text": "Exercise\nüìã Lab-21 Logistic Regression\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-9.html",
    "href": "weeks/week-9.html",
    "title": "Week 9",
    "section": "",
    "text": "Important\n\n\n\n\n\n\nHappy spring break!\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-8.html",
    "href": "weeks/week-8.html",
    "title": "Week 8",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-8.html#participate",
    "href": "weeks/week-8.html#participate",
    "title": "Week 8",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Data Wrangling - two data frames\nüñ•Ô∏è Slides - Tidying Data\n Data - Pop_x and elec_vote_y\n\nlibrary(tidyverse)\nlibrary(dslabs)\npop_x <- murders |> \n    slice(1:6) |>\n    select(state, population)\n\nelec_vote_y <- results_us_election_2016 |> \n    filter(state %in% c(\"Alabama\", \"Alaska\", \"Arizona\", \n                        \"California\", \"Connecticut\", \"Delaware\")) |> \n    select(state, electoral_votes) |> \n    rename(elec_vote = electoral_votes)\n\n Data - Customers"
  },
  {
    "objectID": "weeks/week-8.html#reading-and-resources",
    "href": "weeks/week-8.html#reading-and-resources",
    "title": "Week 8",
    "section": "Reading and Resources",
    "text": "Reading and Resources\nüìñ R for Data Science - Joins\nüìñ R for Data Science - Data tidying"
  },
  {
    "objectID": "weeks/week-8.html#exercise",
    "href": "weeks/week-8.html#exercise",
    "title": "Week 8",
    "section": "Exercise",
    "text": "Exercise\nüìã Lab-16 Joining tables\n https://www.jaredlander.com/data/DiamondColors.csv\n ggplot2::diamonds\nüìã Lab-17 tidyr\n trump.csv\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-15.html",
    "href": "weeks/week-15.html",
    "title": "Week 15",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-15.html#participate",
    "href": "weeks/week-15.html#participate",
    "title": "Week 15",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - K-Means Clustering\n Data - clus_data"
  },
  {
    "objectID": "weeks/week-15.html#reading",
    "href": "weeks/week-15.html#reading",
    "title": "Week 15",
    "section": "Reading",
    "text": "Reading\nüìñ"
  },
  {
    "objectID": "weeks/week-15.html#exercise",
    "href": "weeks/week-15.html#exercise",
    "title": "Week 15",
    "section": "Exercise",
    "text": "Exercise\nüìã Lab-24 K-Means Clustering\n palmerpenguins::penguins\n\nlibrary(palmerpenguins)\npeng <- penguins[complete.cases(penguins), ] |> \n    select(flipper_length_mm, bill_length_mm)\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-14.html",
    "href": "weeks/week-14.html",
    "title": "Week 14",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-14.html#participate",
    "href": "weeks/week-14.html#participate",
    "title": "Week 14",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Principal Component Analysis\n USArrests"
  },
  {
    "objectID": "weeks/week-14.html#reading",
    "href": "weeks/week-14.html#reading",
    "title": "Week 14",
    "section": "Reading",
    "text": "Reading\nüìñ"
  },
  {
    "objectID": "weeks/week-14.html#exercise",
    "href": "weeks/week-14.html#exercise",
    "title": "Week 14",
    "section": "Exercise",
    "text": "Exercise\nüìã Lab-23 Principal Component Analysis\n iris\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "activity-work.html#class-activity-2",
    "href": "activity-work.html#class-activity-2",
    "title": "Class activity work",
    "section": "Class activity 2",
    "text": "Class activity 2"
  },
  {
    "objectID": "activity-work.html#class-activity-3",
    "href": "activity-work.html#class-activity-3",
    "title": "Class activity work",
    "section": "Class activity 3",
    "text": "Class activity 3"
  },
  {
    "objectID": "activity-work.html#class-activity-4",
    "href": "activity-work.html#class-activity-4",
    "title": "Class activity work",
    "section": "Class activity 4",
    "text": "Class activity 4"
  },
  {
    "objectID": "project-tips-resources.html",
    "href": "project-tips-resources.html",
    "title": "Project tips + resources",
    "section": "",
    "text": "To be announced."
  },
  {
    "objectID": "activity-description.html#format",
    "href": "activity-description.html#format",
    "title": "Class activity description",
    "section": "Format",
    "text": "Format"
  },
  {
    "objectID": "activity-description.html#presentation-materials",
    "href": "activity-description.html#presentation-materials",
    "title": "Class activity description",
    "section": "Presentation Materials",
    "text": "Presentation Materials"
  },
  {
    "objectID": "activity-description.html#evaluation",
    "href": "activity-description.html#evaluation",
    "title": "Class activity description",
    "section": "Evaluation",
    "text": "Evaluation"
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "To access computing resources for the introductory data science courses offered by the Duke University Department of Statistical Science, go to the Duke Container Manager website, cmgr.oit.duke.edu/containers.\nIf this is your first time accessing the containers, click on reserve STA210 on the Reservations available menu on the right. You only need to do this once, and when you do, you‚Äôll see this container moved to the My reservations menu on the left.\nNext, click on STA210 under My reservations to access the RStudio instance you‚Äôll use for the course."
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "MSSC 6250: Statistical Machine Learning",
    "section": "",
    "text": "The course covers supervised learning and unsupervised learning models and algorithms. Supervised learning methods include various regression and classification methods, and unsupervised learning methods involves dimension reduction and clustering techniques. Topics include Bayesian linear regression, shrinkage and regularization, regression splines, Gaussian processes, logistic regression, discriminant analysis, nearest neighbors, tree-based methods, principal components, K-means, Gaussian mixture clustering, neural networks, etc."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MSSC 6250 - Statistical Machine Learning",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nTo Do\nSlides\nIn-class Activities\nHomework\nProject\n\n\n\n\n1\nTue, Jan 16\nSyllabus/Overview of Statistical Learning\nüìñ\nüñ•Ô∏èüñ•Ô∏è\n\n\n\n\n\n\nThu, Jan 18\nBias-variance trade-off\n\nüñ•Ô∏è\n\n\n\n\n\n2\nTue, Jan 23\nLinear Regression (review)\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, Jan 25\nLinear Regression (optimization)\n\n\n\n\n\n\n\n3\nTue, Jan 30\nRidge Regression\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, Feb 1\nCross-Validation\n\n\n\n\n\n\n\n4\nTue, Feb 6\nFeature Selection\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, Feb 8\nLASSO\n\n\n\n\n\n\n\n5\nTue, Feb 13\nSplines\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, Feb 15\nGeneralized Additive Models\n\n\n\n\n\n\n\n6\nTue, Feb 20\nBayesian Linear Regression\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, Feb 22\nBayesian Linear Regression\n\n\n\n\n\n\n\n7\nTue, Feb 27\nLogistic Regression\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, Feb 29\nPoisson Regression\n\n\n\n\n\n\n\n8\nTue, Mar 5\nDiscriminant Analysis\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, Mar 7\nNaive Bayes\n\n\n\n\n\n\n\n9\nTue, Mar 12\nNO CLASS: Spring break\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, Mar 14\nNO CLASS: Spring break\n\n\n\n\n\n\n\n10\nTue, Mar 19\nK-Nearest Neighbors Regression\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, Mar 21\nK-Nearest Neighbors Classification\n\n\n\n\n\n\n\n11\nTue, Mar 26\nGaussian Process Regression\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, Mar 28\nNO CLASS: Easter break\n\n\n\n\n\n\n\n12\nTue, Apr 2\nSupport Vector Machine\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, Apr 4\nSupport Vector Machine\n\n\n\n\n\n\n\n13\nTue, Apr 9\nCART and Bagging\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, Apr 11\nRandom Forests and Boosting\n\n\n\n\n\n\n\n14\nTue, Apr 16\nPrincipal Component Analysis\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, Apr 18\nRecommender Systems\n\n\n\n\n\n\n\n15\nTue, Apr 23\nK-Means Clustering\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, Apr 25\nModel-based Clustering\n\n\n\n\n\n\n\n16\nTue, Apr 30\nNeural Networks\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n16\nThu, May 2\nDeep Learning\n\n\n\n\n\n\n\n\n\nI reserve the right to make changes to the schedule."
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Teaching team",
    "section": "",
    "text": "Dr.¬†Mine √áetinkaya-Rundel (she/her) is Professor of the Practice and Director of Undergraduate Studies at the Department of Statistical Science at Duke University. Mine‚Äôs work focuses on innovation in statistics and data science pedagogy, with an emphasis on computing, reproducible research, student-centered learning, and open-source education as well as pedagogical approaches for enhancing retention of women and under-represented minorities in STEM. Mine also works with RStudio as a Developer Educator.\n\n\n\nOffice hours\nLocation\n\n\n\n\nMondays 10:30 am - 11:30 am\nZoom\n\n\nThursdays 11:00 am - 12:00 pm\nOld Chem 213\n\n\n\nIf these times don‚Äôt work for you or you‚Äôd like to schedule a one-on-one meeting, you can do so at bit.ly/meet-mine."
  },
  {
    "objectID": "course-team.html#teaching-assistants",
    "href": "course-team.html#teaching-assistants",
    "title": "Teaching team",
    "section": "Teaching assistants",
    "text": "Teaching assistants\n\n\n\n\nName\nOffice hours\nLocation\n\n\n\n\n\nMartha Aboagye\nThursday 7 - 9 pm\nZoom\n\n\n\nRichard Fremgen\nLead TA for Section 3 (5:15 pm)\nTues 7 - 9 pm\nZoom\n\n\n\nEmily Gentles\nLead TA for Section 1 (1:45pm)\nWednesday 1 - 3 pm\nOld Chem 203B (On Zoom until January 18)\n\n\n\nSara Mehta\nMon 2 - 4 pm\nZoom\n\n\n\nRick Presman\nHead TA\nLead TA for Section 2 (3:30pm)\nThursday 3:30 - 4:30 pm\nFri 9 - 10 am\nZoom\n\n\n\nShari Tian\nTue 8 - 10 am\nZoom\n\n\n\nAaditya Warrier\nMon 12pm - 1pm\nFri 1:30 - 2:30 pm\nZoom"
  },
  {
    "objectID": "slides/06-lasso-vs.html#when-ols-doesnt-work-well",
    "href": "slides/06-lasso-vs.html#when-ols-doesnt-work-well",
    "title": "Feature Selection and LASSO \n",
    "section": "When OLS Doesn‚Äôt Work Well",
    "text": "When OLS Doesn‚Äôt Work Well\nWhen \\(p \\gg n\\), it is often that many of the features in the model are not associated with the response.\n\nModel Interpretability: By removing irrelevant features \\(X_j\\)s, i.e., setting the corresponding \\(\\beta_j\\)s to zero, we can obtain a model that is more easily interpreted. (Feature/Variable selection)\nLeast squares is unlikely to yield any coefficient estimates that are exactly zero."
  },
  {
    "objectID": "slides/06-lasso-vs.html#three-classes-of-methods",
    "href": "slides/06-lasso-vs.html#three-classes-of-methods",
    "title": "Feature Selection and LASSO \n",
    "section": "Three Classes of Methods",
    "text": "Three Classes of Methods\n\n\nSubset Selection. Identify a subset of all the candidate predictors with the best OLS performance.\n\n\nAll possible selection olsrr::ols_step_all_possible()\n\nBest subset olsrr::ols_step_best_subset()\n\nForward stepwise selection olsrr::ols_step_forward_p()\n\nBackward stepwise elimination olsrr::ols_step_backward_p()\n\nHybrid stepwise selection olsrr::ols_step_both_p()\n\n\n\n\n\n\n\nShrinkage. Fit a model that forces some coefficients to be shrunk to zero.\n\n\nLasso glmnet::glmnet()\n\n\n\n\n\n\n\n\nDimension Reduction. Find \\(m\\) representative features that are linear combinations of the \\(p\\) original predictors (\\(m \\ll p\\)), then fit least squares.\n\n\nPrincipal component regression (Unsupervised) pls::pcr()\n\nPartial least squares (Supervised) pls::plsr()\n\n\n\n\nWe then fit a model using least squares on the reduced set of variables."
  },
  {
    "objectID": "slides/06-lasso-vs.html#variable-selection",
    "href": "slides/06-lasso-vs.html#variable-selection",
    "title": "Feature Selection and LASSO \n",
    "section": "Variable Selection",
    "text": "Variable Selection\n\nWe have a large pool of candidate regressors, of which only a few are likely to be important.\n\nTwo ‚Äúconflicting‚Äù goals in model building:\n\nas many features as possible for better predictive performance on new data (smaller bias).\n\nas few regressors as possible because as the number of regressors increases,\n\n\n\\(\\mathrm{Var}(\\hat{y})\\) will increase (larger variance)\ncost more in data collecting and maintaining\nmore model complexity\n\n\n\nA compromise between the two hopefully leads to the ‚Äúbest‚Äù regression equation.\n\nWhat does best mean?\n\n\nThere is no unique definition of ‚Äúbest‚Äù, and different methods specify different subsets of the candidate regressors as best."
  },
  {
    "objectID": "slides/06-lasso-vs.html#model-selection-criteria",
    "href": "slides/06-lasso-vs.html#model-selection-criteria",
    "title": "Feature Selection and LASSO \n",
    "section": "Model Selection Criteria",
    "text": "Model Selection Criteria\n\nThe full (largest) model has \\(M\\) candidate regressors.\nThere are \\(M \\choose d\\) possible subset models of \\(d\\) coefficients.\nThere are totally \\(2^M\\) possible subset models.\nAn evaluation metric should consider Goodness of Fit and Model Complexity:\n\n\nGoodness of Fit: The more regressors, the better\n\n\nComplexity Penalty: The less regressors, the better\n\n\n\nEvaluate subset models:\n\n\n\\(R_{adj}^2\\) \\(\\uparrow\\)\n\nMallow‚Äôs \\(C_p\\) \\(\\downarrow\\)\n\nInformation Criterion (AIC, BIC) \\(\\downarrow\\)\n\n\nPREdiction Sum of Squares (PRESS) \\(\\downarrow\\) (Allen, D.M. (1974))\n\n\n\n\n\n\n\n\n\nNote\n\n\nAll the measures are not appropriate in the high-dimensional (\\(n \\ll p\\)) setting."
  },
  {
    "objectID": "slides/06-lasso-vs.html#selection-criteria-mallows-c_p-statistic-downarrow",
    "href": "slides/06-lasso-vs.html#selection-criteria-mallows-c_p-statistic-downarrow",
    "title": "Feature Selection and LASSO \n",
    "section": "Selection Criteria: Mallow‚Äôs \\(C_p\\) Statistic \\(\\downarrow\\)\n",
    "text": "Selection Criteria: Mallow‚Äôs \\(C_p\\) Statistic \\(\\downarrow\\)\n\nFor a model with \\(d\\) predictors,\n\\[\\begin{align} C_p &= \\frac{SS_{res}(d)}{\\hat{\\sigma}^2} - n + 2d \\\\ &= d + \\frac{(s^2 - \\hat{\\sigma}^2)(n-d)}{\\hat{\\sigma}^2} \\end{align}\\]\n\n\\(\\hat{\\sigma}^2\\) is the variance estimate from the full model, i.e., \\(\\hat{\\sigma}^2 = MS_{res}(M)\\).\n\\(s^2\\) is the variance estimate from the model with \\(d\\) coefficients, i.e., \\(s^2 = MS_{res}(d)\\).\nFavors the candidate model with the smallest \\(C_p\\).\n\nFor unbiased models that \\(E[\\hat{y}_i] = E[y_i]\\), \\(C_p = d\\).\n\nAll of the errors in \\(\\hat{y}_i\\) is variance, and the model is not underfitted.\n\n\n\n\n\n\\(C_p\\) in ISL is defined as \\(C_p = \\frac{1}{n}(SS_{res} + 2d\\hat{\\sigma}^2)\\) which is equivalent to the definition given above."
  },
  {
    "objectID": "slides/06-lasso-vs.html#mallows-c_p-plot",
    "href": "slides/06-lasso-vs.html#mallows-c_p-plot",
    "title": "Feature Selection and LASSO \n",
    "section": "Mallow‚Äôs \\(C_p\\) Plot",
    "text": "Mallow‚Äôs \\(C_p\\) Plot\n\n\n\n\n\n\n\n\n\n\n\n\nModel A is a heavily biased model.\nModel D is the poorest performer.\nModel B and C are reasonable.\nModel C has \\(C_p < 3\\) which implies \\(MS_{res}(3) < MS_{res}(M)\\)"
  },
  {
    "objectID": "slides/06-lasso-vs.html#selection-criteria-information-criterion-downarrow",
    "href": "slides/06-lasso-vs.html#selection-criteria-information-criterion-downarrow",
    "title": "Feature Selection and LASSO \n",
    "section": "Selection Criteria: Information Criterion \\(\\downarrow\\)\n",
    "text": "Selection Criteria: Information Criterion \\(\\downarrow\\)\n\nFor a model with \\(d\\) predictors,\n\nAkaike information criterion (AIC) is \\[\\text{AIC} = n \\ln \\left( \\frac{SS_{res}(d)}{n} \\right) + 2d\\]\nBayesian information criterion (BIC) is \\[\\text{BIC} = n \\ln \\left( \\frac{SS_{res}(d)}{n} \\right) + d \\ln (n)\\]\nBIC penalizes more when adding more variables as the sample size increases.\nBIC tends to choose models with less features.\n\n\n\n\\(\\text{AIC}\\) is proportional to \\(C_p\\)."
  },
  {
    "objectID": "slides/06-lasso-vs.html#selection-criteria-press-downarrow",
    "href": "slides/06-lasso-vs.html#selection-criteria-press-downarrow",
    "title": "Feature Selection and LASSO \n",
    "section": "Selection Criteria: PRESS \\(\\downarrow\\)\n",
    "text": "Selection Criteria: PRESS \\(\\downarrow\\)\n\n\nPREdiction Sum of Squares (PRESS)\n\\(\\text{PRESS}_d = \\sum_{i=1}^n[y_i - \\hat{y}_{(i)}]^2 = \\sum_{i=1}^n\\left( \\frac{e_i}{1-h_{ii}}\\right)^2\\) where \\(e_i = y_i - \\hat{y}_i\\).1\nPRESS is in fact the LOOCV estimate of test MSE!\n\\(R_{pred, d}^2 = 1 - \\frac{PRESS_d}{SS_T}\\)\n\\(\\text{Absolute PRESS}_p = \\sum_{i=1}^n|y_i - \\hat{y}_{(i)}|\\) can also be considered when some large prediction errors are too influential."
  },
  {
    "objectID": "slides/06-lasso-vs.html#selection-methods-all-possible-selection",
    "href": "slides/06-lasso-vs.html#selection-methods-all-possible-selection",
    "title": "Feature Selection and LASSO \n",
    "section": "Selection Methods: All Possible Selection",
    "text": "Selection Methods: All Possible Selection\n\nAssume the intercept is in all models.\nIf there are \\(M\\) possible regressors, we investigate all \\(2^M - 1\\) possible regression equations.\nUse the selection criteria to determine some candidate models and complete regression analysis on them.\n\n\n\n\nmanpower <- read.csv(file = \"./data/manpower.csv\", header = TRUE)\nlm_full <- lm(y ~ ., data = manpower)\nolsrr_all <- olsrr::ols_step_all_possible(lm_full)\nnames(olsrr_all)\n\n [1] \"mindex\"     \"n\"          \"predictors\" \"rsquare\"    \"adjr\"      \n [6] \"predrsq\"    \"cp\"         \"aic\"        \"sbic\"       \"sbc\"       \n[11] \"msep\"       \"fpe\"        \"apc\"        \"hsp\"       \n\n\n\n\n\n\n\n\n\n\n\nn: number of predictors\n\npredictors: predictors in the model\n\nrsquare: R-square of the model\n\nadjr: adjusted R-square of the model\n\npredrsq: predicted R-square of the model\n\ncp: Mallow‚Äôs Cp\n\naic: AIC\n\nsbic: Sawa BIC\n\nsbc: Schwarz BIC (the one we defined)"
  },
  {
    "objectID": "slides/06-lasso-vs.html#section-1",
    "href": "slides/06-lasso-vs.html#section-1",
    "title": "Feature Selection and LASSO \n",
    "section": "",
    "text": "Index N     Predictors R-Square Adj. R-Square Mallow's Cp\n3      1 1             x3    0.972         0.970       20.38\n1      2 1             x1    0.971         0.970       21.20\n2      3 1             x2    0.893         0.886      114.97\n4      4 1             x4    0.884         0.877      125.87\n5      5 1             x5    0.335         0.290      785.26\n10     6 2          x2 x3    0.987         0.985        4.94\n6      7 2          x1 x2    0.986         0.984        5.66\n14     8 2          x3 x5    0.985         0.983        7.29\n9      9 2          x1 x5    0.984         0.982        8.16\n13    10 2          x3 x4    0.975         0.972       18.57\n8     11 2          x1 x4    0.974         0.970       20.04\n7     12 2          x1 x3    0.973         0.969       21.99\n11    13 2          x2 x4    0.931         0.921       72.29\n12    14 2          x2 x5    0.924         0.913       80.30\n15    15 2          x4 x5    0.910         0.898       96.50\n23    16 3       x2 x3 x5    0.990         0.988        2.92\n18    17 3       x1 x2 x5    0.989         0.987        3.71\n16    18 3       x1 x2 x3    0.987         0.984        6.21\n22    19 3       x2 x3 x4    0.987         0.984        6.90\n17    20 3       x1 x2 x4    0.986         0.983        7.66\n20    21 3       x1 x3 x5    0.985         0.982        8.97\n25    22 3       x3 x4 x5    0.985         0.982        9.00\n21    23 3       x1 x4 x5    0.985         0.981        9.41\n19    24 3       x1 x3 x4    0.978         0.974       16.80\n24    25 3       x2 x4 x5    0.952         0.941       48.28\n30    26 4    x2 x3 x4 x5    0.991         0.988        4.03\n28    27 4    x1 x2 x4 x5    0.991         0.987        4.26\n27    28 4    x1 x2 x3 x5    0.991         0.987        4.35\n26    29 4    x1 x2 x3 x4    0.988         0.984        7.54\n29    30 4    x1 x3 x4 x5    0.985         0.980       10.92\n31    31 5 x1 x2 x3 x4 x5    0.991         0.987        6.00"
  },
  {
    "objectID": "slides/06-lasso-vs.html#other-subset-selection-methods",
    "href": "slides/06-lasso-vs.html#other-subset-selection-methods",
    "title": "Feature Selection and LASSO \n",
    "section": "Other Subset Selection Methods",
    "text": "Other Subset Selection Methods\n\n\nAll possible selection olsrr::ols_step_all_possible()\n\nConsider all possible subsets of regressors.\n\n\n\nBest subset olsrr::ols_step_best_subset()\n\nIdentify the best model of each model size.\n\n\n\nForward selection olsrr::ols_step_forward_p()\n\nBegins with no regressors, and insert regressors into the model one at a time.\n\n\n\nBackward elimination olsrr::ols_step_backward_p()\n\nBegin with the full model with all possible regressors, then remove regressors from the model one at a time.\n\n\n\nStepwise regression olsrr::ols_step_both_p()\n\nDo forward selection, but refit the model when a new predictor is inserted. Remove a predictor that becomes redundant from the refit model.\n\n\nCheck Variable Selection Methods"
  },
  {
    "objectID": "slides/06-lasso-vs.html#why-lasso",
    "href": "slides/06-lasso-vs.html#why-lasso",
    "title": "Feature Selection and LASSO \n",
    "section": "Why Lasso?",
    "text": "Why Lasso?\n\nAll possible selection may be computationally infeasible.\nOther subset selection methods do not explore all possible subset models. (No global solution)\nRidge regression does shrink coefficients, but still include all predictors.\nLasso regularizes coefficients so that some coefficients are shrunk to zero, doing feature selection.\nLike ridge regression, for a given \\(\\lambda\\), Lasso only fits a single model."
  },
  {
    "objectID": "slides/06-lasso-vs.html#what-is-lasso",
    "href": "slides/06-lasso-vs.html#what-is-lasso",
    "title": "Feature Selection and LASSO \n",
    "section": "What is Lasso?",
    "text": "What is Lasso?\nDifferent from the Ridge regression that adds \\(\\ell_2\\) norm, Lasso adds \\(\\ell_1\\) penalty on the parameters:\n\\[\\begin{align}\n\\widehat{\\boldsymbol \\beta}^\\text{l} =& \\, \\, \\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol \\beta} \\lVert \\mathbf{y}- \\mathbf{X}\\boldsymbol \\beta\\rVert_2^2 + n \\lambda \\lVert\\boldsymbol \\beta\\rVert_1\\\\\n=& \\, \\, \\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol \\beta} \\frac{1}{n} \\sum_{i=1}^n (y_i - x_i' \\boldsymbol \\beta)^2 + \\lambda \\sum_{j=1}^p |\\beta_j|,\n\\end{align}\\]\n\nThe \\(\\ell_1\\) penalty forces some of the coefficient estimates to be exactly equal to zero when \\(\\lambda\\) is sufficiently large, yielding sparse models."
  },
  {
    "objectID": "slides/06-lasso-vs.html#ell_2-vs.-ell_1",
    "href": "slides/06-lasso-vs.html#ell_2-vs.-ell_1",
    "title": "Feature Selection and LASSO \n",
    "section": "\n\\(\\ell_2\\) vs.¬†\\(\\ell_1\\)\n",
    "text": "\\(\\ell_2\\) vs.¬†\\(\\ell_1\\)\n\n\nRidge shrinks big coefficients much more than lasso.\nLasso has larger penalty on small coefficients.\n\n\n\nRidge shrinks big coefficients much more than lasso.\nLasso has larger penalty on small coefficients. This is one of the intuitions why Lasso has exact zero coefficients after shrinkage. Because when punishing small coefficients a lot more, it may be better to set them exactly equal to zero to get smaller loss or objective value."
  },
  {
    "objectID": "slides/06-lasso-vs.html#elemstatlearnprostate-data",
    "href": "slides/06-lasso-vs.html#elemstatlearnprostate-data",
    "title": "Feature Selection and LASSO \n",
    "section": "\nElemStatLearn::prostate Data",
    "text": "ElemStatLearn::prostate Data\n\n\n\n\n\n\n\n\nlcavol\n      lweight\n      age\n      lbph\n      svi\n      lcp\n      gleason\n      pgg45\n      lpsa\n      train\n    \n\n\n-0.580\n2.77\n50\n-1.386\n0\n-1.39\n6\n0\n-0.431\nTRUE\n\n\n-0.994\n3.32\n58\n-1.386\n0\n-1.39\n6\n0\n-0.163\nTRUE\n\n\n-0.511\n2.69\n74\n-1.386\n0\n-1.39\n7\n20\n-0.163\nTRUE\n\n\n-1.204\n3.28\n58\n-1.386\n0\n-1.39\n6\n0\n-0.163\nTRUE\n\n\n0.751\n3.43\n62\n-1.386\n0\n-1.39\n6\n0\n0.372\nTRUE\n\n\n-1.050\n3.23\n50\n-1.386\n0\n-1.39\n6\n0\n0.765\nTRUE\n\n\n0.737\n3.47\n64\n0.615\n0\n-1.39\n6\n0\n0.765\nFALSE\n\n\n0.693\n3.54\n58\n1.537\n0\n-1.39\n6\n0\n0.854\nTRUE"
  },
  {
    "objectID": "slides/06-lasso-vs.html#cv.glmnetalpha-1",
    "href": "slides/06-lasso-vs.html#cv.glmnetalpha-1",
    "title": "Feature Selection and LASSO \n",
    "section": "cv.glmnet(alpha = 1)",
    "text": "cv.glmnet(alpha = 1)\n\nlasso_fit <- cv.glmnet(x = data.matrix(prostate[, 1:8]), y = prostate$lpsa, nfolds = 10, \n                       alpha = 1)\n\n\nCode    plot(lasso_fit)\n    plot(lasso_fit$glmnet.fit, \"lambda\")\n\n\ngenridge Package"
  },
  {
    "objectID": "slides/06-lasso-vs.html#lasso-coefficients",
    "href": "slides/06-lasso-vs.html#lasso-coefficients",
    "title": "Feature Selection and LASSO \n",
    "section": "Lasso Coefficients",
    "text": "Lasso Coefficients\n\nlambda.min contains more nonzero parameters.\nLarger penalty \\(\\lambda\\) forces more parameters to be zero, and the model is more ‚Äúsparse‚Äù.\n\n\n\n\ncoef(lasso_fit, s = \"lambda.min\")\n\n9 x 1 sparse Matrix of class \"dgCMatrix\"\n                  s1\n(Intercept)  0.17999\nlcavol       0.56099\nlweight      0.61908\nage         -0.02069\nlbph         0.09531\nsvi          0.75180\nlcp         -0.09912\ngleason      0.04745\npgg45        0.00432\n\n\n\n\ncoef(lasso_fit, s = \"lambda.1se\")\n\n9 x 1 sparse Matrix of class \"dgCMatrix\"\n               s1\n(Intercept) 1.101\nlcavol      0.433\nlweight     0.202\nage         .    \nlbph        .    \nsvi         0.271\nlcp         .    \ngleason     .    \npgg45       ."
  },
  {
    "objectID": "slides/06-lasso-vs.html#one-variable-lasso-and-shrinkage-concept",
    "href": "slides/06-lasso-vs.html#one-variable-lasso-and-shrinkage-concept",
    "title": "Feature Selection and LASSO \n",
    "section": "One-Variable Lasso and Shrinkage: Concept",
    "text": "One-Variable Lasso and Shrinkage: Concept\n\nLasso solution does not have an analytic or closed form in general.\nConsider the univariate regression model\n\n\\[\\underset{\\beta}{\\mathop{\\mathrm{arg\\,min}}}  \\quad \\frac{1}{n} \\sum_{i=1}^n (y_i - x_i \\beta)^2 + \\lambda |\\beta|\\]\nWith some derivation, and also utilize the OLS solution of the loss function, we have\n\\[\\begin{align}\n&\\frac{1}{n} \\sum_{i=1}^n (y_i - x_i \\beta)^2 \\\\\n=& \\frac{1}{n} \\sum_{i=1}^n (y_i - x_i b + x_i b - x_i \\beta)^2 \\\\\n=& \\frac{1}{n} \\sum_{i=1}^n \\Big[ \\underbrace{(y_i - x_i b)^2}_{\\text{I}} + \\underbrace{2(y_i - x_i b)(x_i b - x_i \\beta)}_{\\text{II}} + \\underbrace{(x_i b - x_i \\beta)^2}_{\\text{III}} \\Big]\n\\end{align}\\]"
  },
  {
    "objectID": "slides/06-lasso-vs.html#one-variable-lasso-and-shrinkage-concept-1",
    "href": "slides/06-lasso-vs.html#one-variable-lasso-and-shrinkage-concept-1",
    "title": "Feature Selection and LASSO \n",
    "section": "One-Variable Lasso and Shrinkage: Concept",
    "text": "One-Variable Lasso and Shrinkage: Concept\n\\[\\begin{align}\n& \\sum_{i=1}^n 2(y_i - x_i b)(x_i b - x_i \\beta)\n= (b - \\beta) {\\color{OrangeRed}{\\sum_{i=1}^n 2(y_i - x_i b)x_i}}\n= (b - \\beta) {\\color{OrangeRed}{0}} = 0\n\\end{align}\\]\nOur original problem reduces to just the third term and the penalty\n\\[\\begin{align}\n&\\underset{\\beta}{\\mathop{\\mathrm{arg\\,min}}}  \\quad \\frac{1}{n} \\sum_{i=1}^n (x_ib - x_i \\beta)^2 + \\lambda |\\beta| \\\\\n=&\\underset{\\beta}{\\mathop{\\mathrm{arg\\,min}}}  \\quad \\frac{1}{n} \\left[ \\sum_{i=1}^n x_i^2 \\right] (b - \\beta)^2 + \\lambda |\\beta|\n\\end{align}\n\\] Without loss of generality, assume that \\(x\\) is standardized with mean 0 and variance \\(\\frac{1}{n}\\sum_{i=1}^n x_i^2 = 1\\).\nb is arbitrary We learned that to solve for an optimizer, we can set the gradient to be zero. However, the function is not everywhere differentiable"
  },
  {
    "objectID": "slides/06-lasso-vs.html#one-variable-lasso-and-shrinkage-concept-2",
    "href": "slides/06-lasso-vs.html#one-variable-lasso-and-shrinkage-concept-2",
    "title": "Feature Selection and LASSO \n",
    "section": "One-Variable Lasso and Shrinkage: Concept",
    "text": "One-Variable Lasso and Shrinkage: Concept\nThis leads to a general problem of\n\\[\\underset{\\beta}{\\mathop{\\mathrm{arg\\,min}}}  \\quad (\\beta - b)^2 + \\lambda |\\beta|,\\] For \\(\\beta > 0\\),\n\\[\\begin{align}\n0 =& \\frac{\\partial}{\\partial \\beta} \\,\\, \\left[(\\beta - b)^2 + \\lambda |\\beta| \\right] = 2 (\\beta - b) + \\lambda \\\\\n\\Longrightarrow \\quad \\beta =&\\, b - \\lambda/2\n\\end{align}\\]\n\\[\\begin{align}\n\\widehat{\\beta}^\\text{l} &=\n        \\begin{cases}\n        b - \\lambda/2 & \\text{if} \\quad b > \\lambda/2 \\\\\n        0 & \\text{if} \\quad |b| \\le \\lambda/2 \\\\\n        b + \\lambda/2 & \\text{if} \\quad b < -\\lambda/2 \\\\\n        \\end{cases}\n\\end{align}\\]\n\nLasso provides a soft-thresholding solution.\nWhen \\(\\lambda\\) is large enough, \\(\\widehat{\\beta}^\\text{l}\\) will be shrunk to zero.\n\nb is arbitrary We learned that to solve for an optimizer, we can set the gradient to be zero. However, the function is not everywhere differentiable"
  },
  {
    "objectID": "slides/06-lasso-vs.html#objective-function",
    "href": "slides/06-lasso-vs.html#objective-function",
    "title": "Feature Selection and LASSO \n",
    "section": "Objective function",
    "text": "Objective function\nThe objective function is \\((\\beta - 1)^2\\). Once the penalty is larger than 2, the optimizer would stay at 0.\n\n\n\n\n\n\nBased on our analysis, once the penalty is larger than 2, the optimizer would stay at 0."
  },
  {
    "objectID": "slides/06-lasso-vs.html#variable-selection-property-and-shrinkage",
    "href": "slides/06-lasso-vs.html#variable-selection-property-and-shrinkage",
    "title": "Feature Selection and LASSO \n",
    "section": "Variable Selection Property and Shrinkage",
    "text": "Variable Selection Property and Shrinkage\n\nThe proportion of times a variable has a non-zero parameter estimation.\n\\(\\mathbf{y}= \\mathbf{X}' \\boldsymbol \\beta + \\epsilon = \\sum_{j = 1}^p X_j \\times 0.4^{\\sqrt{j}} + \\epsilon\\)\n\\(p = 20\\), \\(\\epsilon \\sim N(0, 1)\\) and replicate 100 times.\n\n\n\nSince Lasso shrinks some parameter to exactly zero, it has the variable selection property ‚Äî the ones that are nonzero are the ones being selected. This is a very nice properly in high-dimensional data analysis, where we cannot estimate the effects of all variables.\nWe can then look at the proportion of times a variable has a non-zero parameter estimation"
  },
  {
    "objectID": "slides/06-lasso-vs.html#bias-variance-trade-off",
    "href": "slides/06-lasso-vs.html#bias-variance-trade-off",
    "title": "Feature Selection and LASSO \n",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off"
  },
  {
    "objectID": "slides/06-lasso-vs.html#constrained-optimization",
    "href": "slides/06-lasso-vs.html#constrained-optimization",
    "title": "Feature Selection and LASSO \n",
    "section": "Constrained Optimization",
    "text": "Constrained Optimization\n\nFor every value of \\(\\lambda\\), there is some \\(s\\) such that the two optimization problems are equivalent, giving the same coefficient estimates.\nThe \\(\\ell_1\\) and \\(\\ell_2\\) penalties form a constraint region that \\(\\beta_j\\) can move around or budget for how large \\(\\beta_j\\) can be.\nLarger \\(s\\) (smaller \\(\\lambda\\)) means a larger region \\(\\beta_j\\) can freely move.\n\n\n\nLasso \\[\\begin{align}\n\\min_{\\boldsymbol \\beta} \\,\\,& \\lVert \\mathbf{y}- \\mathbf{X}\\boldsymbol \\beta\\rVert_2^2 + n\\lambda\\lVert\\boldsymbol \\beta\\rVert_1\n\\end{align}\\]\n\n\\[\\begin{align}\n\\min_{\\boldsymbol \\beta} \\,\\,& \\lVert \\mathbf{y}- \\mathbf{X}\\boldsymbol \\beta\\rVert_2^2\\\\\n\\text{s.t.} \\,\\, & \\sum_{j=1}^p|\\beta_j| \\leq s\n\\end{align}\\]\n\nRidge \\[\\begin{align}\n\\min_{\\boldsymbol \\beta} \\,\\,& \\lVert \\mathbf{y}- \\mathbf{X}\\boldsymbol \\beta\\rVert_2^2 + n\\lambda\\lVert\\boldsymbol \\beta\\rVert_2^2\n\\end{align}\\]\n\n\\[\\begin{align}\n\\min_{\\boldsymbol \\beta} \\,\\,& \\lVert \\mathbf{y}- \\mathbf{X}\\boldsymbol \\beta\\rVert_2^2\\\\\n\\text{s.t.} \\,\\, & \\sum_{j=1}^p \\beta_j^2 \\leq s\n\\end{align}\\]"
  },
  {
    "objectID": "slides/06-lasso-vs.html#geometric-representation-of-optimization",
    "href": "slides/06-lasso-vs.html#geometric-representation-of-optimization",
    "title": "Feature Selection and LASSO \n",
    "section": "Geometric Representation of Optimization",
    "text": "Geometric Representation of Optimization\n\nWhat do the constraints look like geometrically?\n\n\nWhen \\(p = 2\\),\n\nthe \\(\\ell_1\\) constraint is \\(|\\beta_1| + |\\beta_2| \\leq s\\) (diamond)\nthe \\(\\ell_2\\) constraint is \\(\\beta_1^2 + \\beta_2^2 \\leq s\\) (circle)\n\n\n\n\n\n\n\n\nSource: https://stats.stackexchange.com/questions/350046/the-graphical-intuiton-of-the-lasso-in-case-p-2\n\n\n\n\n\nthe solution has to stay within the shaded area. The objective function is shown with the contour, and once the contained area is sufficiently small, some Œ≤ parameter will be shrunk to exactly zero. On the other hand, the Ridge regression also has a similar interpretation. However, since the constrained areas is a circle, it will never for the estimated parameters to be zero."
  },
  {
    "objectID": "slides/06-lasso-vs.html#way-of-shrinking-p-1-and-standardized-x",
    "href": "slides/06-lasso-vs.html#way-of-shrinking-p-1-and-standardized-x",
    "title": "Feature Selection and LASSO \n",
    "section": "Way of Shrinking (\\(p = 1\\) and standardized \\(x\\))",
    "text": "Way of Shrinking (\\(p = 1\\) and standardized \\(x\\))\n\n\nLasso Soft-thresholding\n\n\\[\\begin{align}\n\\widehat{\\beta}^\\text{l} &=\n        \\begin{cases}\n        b - \\lambda/2 & \\text{if} \\quad b > \\lambda/2 \\\\\n        0 & \\text{if} \\quad |b| < \\lambda/2 \\\\\n        b + \\lambda/2 & \\text{if} \\quad b < -\\lambda/2 \\\\\n        \\end{cases}\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\n\nRidge Proportional shrinkage\n\\[\\begin{align}\n\\widehat{\\beta}^\\text{r} = \\dfrac{b}{1+\\lambda}\\end{align}\\]"
  },
  {
    "objectID": "slides/06-lasso-vs.html#predictive-performance",
    "href": "slides/06-lasso-vs.html#predictive-performance",
    "title": "Feature Selection and LASSO \n",
    "section": "Predictive Performance",
    "text": "Predictive Performance\nPerform well (lower test MSE) when\n\n\nLasso (‚Äî)\n\nA relatively small number of \\(\\beta_j\\)s are substantially large, and the remaining \\(\\beta_k\\)s are small or equal to zero.\nReduce more bias\n\n\n\n\n\nISL Fig. 6.9\n\n\n\n\n\nRidge (‚Ä¶)\n\nThe response is a function of many predictors, all with coefficients of roughly equal size.\nReduce more variance\n\n\n\n\n\nISL Fig. 6.8\n\n\n\n\n\n\nRidge focus more on variance stablization Lasso focus more on model selection and interpretbility higher R^2 (larger lambda), more overfitting, so low bias high variance"
  },
  {
    "objectID": "slides/06-lasso-vs.html#bayesian-interpretation",
    "href": "slides/06-lasso-vs.html#bayesian-interpretation",
    "title": "Feature Selection and LASSO \n",
    "section": "Bayesian Interpretation",
    "text": "Bayesian Interpretation\n\n\nLasso\n\nRidge\n\n\n\n\nLet‚Äôs wait until we discuss Bayesian Regression!"
  },
  {
    "objectID": "slides/06-lasso-vs.html#notes-of-lasso",
    "href": "slides/06-lasso-vs.html#notes-of-lasso",
    "title": "Feature Selection and LASSO \n",
    "section": "Notes of Lasso",
    "text": "Notes of Lasso\n\n\n\n\n\n\nWarning\n\n\n\n\nEven Lasso does feature selection, do not add predictors that are known to be not associated with the response in any way.\nCurse of dimensionality. The test MSE tends to increase as the dimensionality \\(p\\) increases, unless the additional features are truly associated with the response.\nDo not conclude that the predictors with non-zero coefficients selected by Lasso and other selection methods predict the response more effectively than other predictors not included in the model.\n\n\n\n\n\nThis is just one of many possible models for predicting response, and that it must be further validated on independent data sets."
  },
  {
    "objectID": "slides/06-lasso-vs.html#other-topics",
    "href": "slides/06-lasso-vs.html#other-topics",
    "title": "Feature Selection and LASSO \n",
    "section": "Other Topics",
    "text": "Other Topics\n\n\nCombination \\(\\ell_1\\) and \\(\\ell_2\\): Elastic net penalty (Zou and Hastie, 2005)\n\n\n\\[\\lambda \\left[ (1 - \\alpha) \\lVert \\boldsymbol \\beta\\rVert_2^2 + \\alpha \\lVert\\boldsymbol \\beta\\lVert_1 \\right]\\]\n\n\n\nGeneral \\(\\ell_q\\) penalty: \\(\\lambda \\sum_{j=1}^p |\\beta_j|^q\\)\n\n\n\n\n\n\nESL Fig. 3.12"
  },
  {
    "objectID": "slides/06-lasso-vs.html#other-topics-1",
    "href": "slides/06-lasso-vs.html#other-topics-1",
    "title": "Feature Selection and LASSO \n",
    "section": "Other Topics",
    "text": "Other Topics\n\n\nAlgorithms\n\nShooting algorithm (Fu 1998)\nLeast angle regression (LAR) (Efron et al.¬†2004)\nCoordinate descent (Friedman et al 2010) (used in glmnet)\n\n\n\n\n\n\nVariants\n\nAdaptive Lasso\nGroup Lasso\nBayesian Lasso, etc.\n\n\n\nLasso may suffer in the case where two variables are strongly correlated. The situation is similar to OLS, however, in Lasso, it would only select one out of the two, instead of letting both parameter estimates to be large. This is not preferred in some practical situations such as genetic studies because expressions of genes from the same pathway may have large correlation, but biologist want to identify all of them instead of just one. The Ridge penalty may help in this case because it naturally considers the correlation structure. The following simulation may show the effect. - The Lasso problem is convex, although it may not be strictly convex in Œ≤ when p is large - The solution is a global minimum, but may not be the unique global one - The Lasso solution is unique under conditions of the covariance matrix\n\n\n\nmssc6250-s24.github.io/website"
  },
  {
    "objectID": "slides/03-bias-var.html#supervised-learning-1",
    "href": "slides/03-bias-var.html#supervised-learning-1",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "Supervised Learning",
    "text": "Supervised Learning\nSupervised learning investigates and models the relationships between responses and inputs."
  },
  {
    "objectID": "slides/03-bias-var.html#relationship-as-functions",
    "href": "slides/03-bias-var.html#relationship-as-functions",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "Relationship as Functions",
    "text": "Relationship as Functions\n\nRepresent relationships between variables using functions y = f(x).\n\nPlug in the inputs and receive the output.\n\ny = f(x) = 3x + 7 is a function with input x and output y.\nIf x = 5, y = 3 \\times 5 + 7 = 22.\n\n\n\n\n\nIn mathematics, how do we describe a Relationship Between Variables? We use a function. Right.\nThe function y = f(x) gives us the relationship between an output Y and one or more inputs x.\n\nYou plug in the values of inputs and receive back the output value.\nFor example, the formula y = f(x) = 3x + 7 is a function with input x and output y. If x = 5, y = 3 \\times 5 + 7 = 22.\n\n\nBecause this is a linear function, we know that x and y are linearly related.\nWith a value of x, I can give you 100% correct value of y, which is right on this straight line. right. In other words, their relationship is 100% deterministic."
  },
  {
    "objectID": "slides/03-bias-var.html#different-relationships",
    "href": "slides/03-bias-var.html#different-relationships",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "Different Relationships",
    "text": "Different Relationships\n\n\n\nCan you come up with any real-world examples describing relationships between variables deterministically?\n\n\nThe relationship between x and y can be more than linear.\nThe relationship can be also quadratic, cubic or any other possible relationship."
  },
  {
    "objectID": "slides/03-bias-var.html#different-relationships-1",
    "href": "slides/03-bias-var.html#different-relationships-1",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "Different Relationships",
    "text": "Different Relationships\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere I give your two examples. The first example is the conversion of F and C degrees. Their relationship is linear and F = 32 + 1.8 C.\nSo you give me a C degree, I can tell you its corresponding F degree fro sure. Right.\nThe second example comes from physics. the displacement of an object is a quadratic function of time.\nSo here s(t) = v0 * t + 0.5 * a * t^2. v0 is the initial velocity, and a is acceleration, and t is time.\nAgain the relationship between displacement and time is 100% deterministic.\nA value of time corresponds to an unique value of displacement given v0 and a."
  },
  {
    "objectID": "slides/03-bias-var.html#relationship-between-variables-is-not-perfect",
    "href": "slides/03-bias-var.html#relationship-between-variables-is-not-perfect",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "Relationship between Variables is Not Perfect",
    "text": "Relationship between Variables is Not Perfect\n\n\n\nCan you provide some real examples that the variables are related each other, but not perfectly related?"
  },
  {
    "objectID": "slides/03-bias-var.html#relationship-between-variables-is-not-perfect-1",
    "href": "slides/03-bias-var.html#relationship-between-variables-is-not-perfect-1",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "Relationship between Variables is Not Perfect",
    "text": "Relationship between Variables is Not Perfect\n\n\nüíµ In general, one with more years of education earns more.\nüíµ Any two with the same years of education may have different annual income.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere I give you a simple example: the relationship between income and years of education.\nüíµ In general, one with more years of education earns more.\nüíµ Any two with the same years of education may have different annual income.\nBecause your income level depends on so many other factors, not just years of education.\nSo when you plot the scatter plot of the two variables, you will find that there is some trend, but the data are sort of scattered or jittered or variated around some function that describes the relationship between income the years of education.\n\nRed dots are observed values or the years of education and income pairs."
  },
  {
    "objectID": "slides/03-bias-var.html#variation-around-the-functionmodel",
    "href": "slides/03-bias-var.html#variation-around-the-functionmodel",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "Variation around the Function/Model",
    "text": "Variation around the Function/Model\n\n\nWhat are the unexplained variation coming from?\n\n\n\n\n\n\nOther factors accounting for parts of variability of income.\n\nAdding more explanatory variables to a model can reduce the variation size around the model.\n\n\nPure measurement error.\nJust that randomness plays a big role. ü§î\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat other factors (variables) may affect a person‚Äôs income?\n\n\n\nyour income = f(years of education, major, GPA, college, parent's income, ...)\n\nAnd the data Variation around the Function, or in general the regression model is just as important as the model, if not more!\n\nBasically, what statistics does is explain variation in the context of what remains unexplained.\nThe scatter plot suggests that there might be other factors that account for large parts of variability.\nIf that is the case, adding more explanatory variables ( Xs ) to a model can sometimes usefully reduce the size of the scatter around the model.\nPerhaps just that randomness plays a big role."
  },
  {
    "objectID": "slides/03-bias-var.html#supervised-learning-mapping",
    "href": "slides/03-bias-var.html#supervised-learning-mapping",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "Supervised Learning Mapping",
    "text": "Supervised Learning Mapping\n\n\n\nExplain the relationship between X and Y and make predictions through a model Y = f(X) + \\epsilon\n\n\n\\epsilon: irreducible random error (Aleatoric Uncertainty)\n\nindependent of X\n\n\nmean zero with some variance.\n\n\n\nf(\\cdot): unknown function1 describing the relationship between X and the mean of Y. \n\n\n\n\nIn Intro Stats, what is the form of f and what assumptions you made on the random error \\epsilon ?\n\n\n\n\n\nf(X) = \\beta_0 + \\beta_1X with unknown parameters \\beta_0 and \\beta_1.\n\n\\epsilon \\sim N(0, \\sigma^2).\n\n\nOK. Now after collecting the data of the variables we are interested, we know their relationship, most of the time, is not perfect, and stochastic in some way and in some sense.\nAnd how do we model such stochastic relationship? Well the answer is a regression model.\nSuppose we are interested in the relationship between two variables, call X and Y. In particular, we like to know how changes of X affect value of Y, or we want to use X to predict Y.\nIn this sense, Y is called response, outcome, label, dependent variable, e.g., income\n\n\nX is called predictor, covariate, feature, regressor, explanatory or independent variable, e.g., years of education, which is known and fixed.\nExplain the relationship between X and Y and make predictions through a model Y = f(X) + \\epsilon. This is a very general regression model we can built to learn the relationship b/w x and y.\n\nf(\\cdot) is fixed but unknown and describes the true relationship between X and Y. \n\n\n\\epsilon is a irreducible random error which is assumed to be independent of X and has mean zero with some variance.\n\n\\epsilon is used to represent those measurement errors or the variation that cannot be explained or captured by the predictor X.\nIntro Stats:\n\n\nf(X) = \\beta_0 + \\beta_1X with unknown parameters \\beta_0 and \\beta_1.\n\n\\epsilon \\sim N(0, \\sigma^2).\n\n\n\nX and Y are assumed to be linearly related, which may not be correct.\nNext week, we will learn simple linear regression from the scratch and in much more detail. Here I just give you an overview.\n\n\nf(\\cdot) is assumed fixed from frequentist point of view; f(\\cdot) is random in the Bayesian framework."
  },
  {
    "objectID": "slides/03-bias-var.html#true-unknown-function-f-of-the-model-y-fx-epsilon",
    "href": "slides/03-bias-var.html#true-unknown-function-f-of-the-model-y-fx-epsilon",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "True Unknown Function f of the Model Y = f(X) + \\epsilon\n",
    "text": "True Unknown Function f of the Model Y = f(X) + \\epsilon\n\n\n\n\nBlue curve: true underlying relationship between (the mean) income and years of education.\n\nBlack lines: error associated with each observation\n\n\n\nBig problem: f(x) is unknown and needs to be estimated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs go back to the income-education example. Red dots are observed values or the years of education and income pairs. Suppose we like to use a regression model y = f(x) + \\epsilon to describe the relationship between income and education.\nAnd the *Blue** curve on the right shows the true underlying relationship between income and years of education, which is the function f in our regression model.\nAnd each Black vertical line indicates an error associated with each observation.\nSo again, each red dot or observation is the value of the function f(x) plus some random error with its magnitude shown in a black vertical line.\nAgain, in regression, we assume years of education is fixed. It is income level that varies around the function f."
  },
  {
    "objectID": "slides/03-bias-var.html#how-to-estimate-f",
    "href": "slides/03-bias-var.html#how-to-estimate-f",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "How to Estimate f?",
    "text": "How to Estimate f?\n\nUse training data \\mathcal{D} = \\{ (x_i, y_i) \\}_{i=1}^n to train or teach our model to learn f.\nUse test data \\mathcal{D}_{test} = \\{ (x_j, y_j) \\}_{j=1}^m to test or evaluate how well the model makes inference or prediction.\n\n\n\nModels are either parametric or nonparametric.\n\n\n\n\n\nParametric methods involve a two-step model-based approach:\n\n1Ô∏è‚É£ Make an assumption about the shape of f, e.g.¬†linear regression  f(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p \n\n2Ô∏è‚É£ Use \\mathcal{D} to train the model, e.g., learn the parameters \\beta_j, j = 0, \\dots, p using least squares.\n\n\n\n\n\n\n\nNonparametric methods do not make assumptions about the shape of f.\n\nSeek an estimate of f that gets close to the data points without being too rough or wiggly."
  },
  {
    "objectID": "slides/03-bias-var.html#parametric-vs.-nonparametric-models",
    "href": "slides/03-bias-var.html#parametric-vs.-nonparametric-models",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "Parametric vs.¬†Nonparametric Models",
    "text": "Parametric vs.¬†Nonparametric Models\n\n\n\nParametric (Linear regression)\n\n\n\n\n\n\n\n\n\n\n\nNonparametric (LOESS)"
  },
  {
    "objectID": "slides/03-bias-var.html#no-free-lunch",
    "href": "slides/03-bias-var.html#no-free-lunch",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "No Free Lunch",
    "text": "No Free Lunch\n\n\nThere is no free lunch in machine learning: no one method dominates all others over all possible data sets.\n\n\n\nAll models are wrong, but some are useful. ‚Äì George Box (1919-2013)\n\n\n\nFor any given training data, decide which method produces the best results.\nSelecting the best approach is one of the most challenging parts of machine learning.\nNeed some way to measure how well its predictions actually match the training/test data.\n\n\n\n\nNumeric y: mean square error (MSE) for y with \\hat{f} the estimated function of f \\text{MSE}_{\\texttt{Tr}} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{f}(x_i))^2, \\quad \\quad \\text{MSE}_{\\texttt{Te}} = \\frac{1}{m} \\sum_{j=1}^m (y_j - \\hat{f}(x_j))^2\n\n\n\n\n\nAre \\text{MSE}_{\\texttt{Tr}} and \\text{MSE}_{\\texttt{Te}} the same? When to use which?"
  },
  {
    "objectID": "slides/03-bias-var.html#mean-square-error",
    "href": "slides/03-bias-var.html#mean-square-error",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "Mean Square Error",
    "text": "Mean Square Error\n\n\n\\text{MSE}_{\\texttt{Tr}} measures how much \\hat{f}(x_i) is close to the training data y_i (goodness of fit). However, most of the time\n\n\n\n\nWe do not care how well the method works on the training data.\n\n\n\n\nWe are interested in the predictive accuracy when we apply our method to previously unseen test data.\n\n\nWe want to know whether \\hat{f}(x_j) is (approximately) equal to y_j, where (x_j, y_j) is previously unseen or a test data point not used in training our model.\n\n\n\n\n\\text{MSE}_{\\texttt{Tr}} or \\text{MSE}_{\\texttt{Te}} is smaller?\n\n\n\n\n\n\\text{MSE}_{\\texttt{Tr}} < \\text{MSE}_{\\texttt{Te}}.\n\n\nMost statistical learning methods either directly or indirectly seek to minimize the training MSE.\nTraining data are the information we have and probably only have. Without other constraints or information about how we train the model, we tend to make use of all possible information in the training data to train our model.\nWe want to know whether \\hat{f}(x_j) is (approximately) equal to y_j, where (x_j, y_j) is a previously unseen test observation not used to train the model."
  },
  {
    "objectID": "slides/03-bias-var.html#model-complexityflexibility",
    "href": "slides/03-bias-var.html#model-complexityflexibility",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "Model Complexity/Flexibility",
    "text": "Model Complexity/Flexibility\n\nA more complex model produces a more flexible or wiggly regression curve \\hat{f}(x) that matches the training data better.\n\ny = \\beta_0+ \\beta_1x + \\beta_2x^2 + \\cdots + \\beta_{10}x^{10} + \\epsilon is more complex than y = \\beta_0+ \\beta_1x + \\epsilon\n\n\n\n\n\nOverfitting: A too complex model fits the training data extremely well and too hard, picking up some patterns and variations simply caused by random noises that are not the properties of the true f, and not existed in the any unseen test data.  \n\n\n\n\nUnderfitting: A model that is too simple to capture complex patterns or shapes of the true f(x). The estimate \\hat{f}(x) is rigid and far away from data."
  },
  {
    "objectID": "slides/03-bias-var.html#section-2",
    "href": "slides/03-bias-var.html#section-2",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "",
    "text": "How \\text{MSE}_{\\texttt{Tr}} and \\text{MSE}_{\\texttt{Te}} change with model complexity?"
  },
  {
    "objectID": "slides/03-bias-var.html#model-complexityflexibility-and-mse",
    "href": "slides/03-bias-var.html#model-complexityflexibility-and-mse",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "Model Complexity/Flexibility and MSE",
    "text": "Model Complexity/Flexibility and MSE\n\nIt‚Äôs common that no test data are available. Can we select a model that minimize \\text{MSE}_{\\texttt{Tr}}, since the training data and test data appear to be closed related?\n\n\n\n\n\n\n\\text{MSE}_{\\texttt{Tr}} (gray) is decreasing with the complexity.\n\n\\text{MSE}_{\\texttt{Te}} (red) is U-shaped: goes down then up with the complexity.\n\n\n\nMSE\nOverfit\nUnderfit\n\n\n\nTrain\ntiny\nbig\n\n\nTest\nbig\nbig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo! There is no guarantee that the method with the lowest \\text{MSE}_{\\texttt{Tr}} will also have the lowest \\text{MSE}_{\\texttt{Te}}."
  },
  {
    "objectID": "slides/03-bias-var.html#bias-variance-tradeoff",
    "href": "slides/03-bias-var.html#bias-variance-tradeoff",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "Bias-Variance Tradeoff",
    "text": "Bias-Variance Tradeoff\nGiven any new input x_0,\n\n\\text{MSE}_{\\hat{f}} = E\\left[\\left(\\hat{f}(x_0) - f(x_0)\\right)^2\\right] = \\left[\\text{Bias}\\left(\\hat{f}(x_0) \\right)\\right]^2 + \\text{Var}\\left(\\hat{f}(x_0)\\right)\nwhere \\text{Bias}\\left(\\hat{f}(x_0) \\right) = E\\left[ \\hat{f}(x_0)\\right] - f(x_0).\n\n\n\nThe expected test MSE of y_0 at x_0 is \\text{MSE}_{y_0} = E\\left[\\left(y_0 - \\hat{f}(x_0)\\right)^2\\right] = \\text{MSE}_{\\hat{f}} + \\text{Var}(\\epsilon)\n\n\n\n\n\n\n\nNote\n\n\n\nWe never know the true expected test MSE, and prefer the model with the smallest expected test MSE estimate.\n\n\n\n\n\n\n\nOverfitting: Low bias and High variance\nUnderfitting: High bias and Low variance\n\n\nMSE can be decomposed into two effects/measures\nMSE is a combination of two performance measures."
  },
  {
    "objectID": "slides/03-bias-var.html#lab-bias-variance-tradeoff",
    "href": "slides/03-bias-var.html#lab-bias-variance-tradeoff",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "\nLab: Bias-Variance Tradeoff",
    "text": "Lab: Bias-Variance Tradeoff\n\nModel 1: Under-fitting y = \\beta_0+\\beta_1x+\\epsilon\nModel 2: Right-fitting y = \\beta_0+\\beta_1x+ \\beta_2x^2 + \\epsilon\nModel 3: Over-fitting y = \\beta_0+\\beta_1x+ \\beta_2x^2 + \\cdots + \\beta_9x^9 + \\epsilon\nTo see expectation/bias and variance, we need replicates of training data."
  },
  {
    "objectID": "slides/07-splines.html#when-y-and-x-are-not-linearly-associated",
    "href": "slides/07-splines.html#when-y-and-x-are-not-linearly-associated",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "When \\(y\\) and \\(x\\) are Not Linearly Associated",
    "text": "When \\(y\\) and \\(x\\) are Not Linearly Associated\n\nLinear models can describe non-linear relationship.\nFeature engineering: When \\(y\\) and \\(x\\) are not linearly associated, we transform \\(x\\) (sometimes also \\(y\\)) so that \\(y\\) and the transformed \\(x\\) become linearly related.\n\n\n\nPopular methods\n\nPolynominal regression (MSSC 5780)\nPiecewise regression (MSSC 5780)\nLocal regression (MSSC 5780)\nBasis function approach\nRegression splines\nSmoothing splines\nGeneral additive models (GAMs)"
  },
  {
    "objectID": "slides/07-splines.html#polynomial-regression",
    "href": "slides/07-splines.html#polynomial-regression",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\n\nThe \\(d\\)th-order (degree) polynomial model in one variable is \\[y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\cdots + \\beta_dx^d + \\epsilon\\]\n\n\n\n\nWe extend the simple linear regression by mapping \\(x\\) to \\((x, x^2, \\dots, x^d)\\).\n\n\n\n‚ÄúBayesian Deep Learning and a Probabilistic Perspective of Generalization‚Äù Wilson and Izmailov (2020) for the rationale of choosing a super high-order polynomial as the regression model.\nExtra flexibility produces undesirable results at the boundaries."
  },
  {
    "objectID": "slides/07-splines.html#basis-function-approach",
    "href": "slides/07-splines.html#basis-function-approach",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "Basis Function Approach",
    "text": "Basis Function Approach\n\nA set of basis functions or transformations that can be applied to a variable \\(x\\):\\[[b_1(x), b_2(x), \\dots , b_K(x)]\\]\n\n\nAdaptive basis function model: \\[y = \\beta_0 + \\beta_1 b_1(x) + \\beta_2 b_2(x) + \\cdots + \\beta_K b_K(x) + \\epsilon\\]\n\n\n\nIs polynomial regression a basis function approach?\n\n\n\n\nPolynomial basis: \\(\\phi(x) = [1, x, x^2, \\dots, x^K]\\)\n\n\nFourier series basis: \\(\\phi(x) = [1, \\cos(\\omega_1x + \\psi_1), \\cos(\\omega_2x + \\psi_2), \\dots]\\)\n\nB-Spline basis\n\n\n\n\n\n\n\nImportant\n\n\n\n\\([b_1(x), b_2(x), \\dots , b_K(x)]\\) are fixed and known."
  },
  {
    "objectID": "slides/07-splines.html#piecewise-regression",
    "href": "slides/07-splines.html#piecewise-regression",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "Piecewise regression",
    "text": "Piecewise regression\n\nPolynomial regression imposes a global structure on the non-linear function.\nPiecewise regression allows structural changes in different parts of the range of \\(x\\)\n\n\\[y=\\begin{cases}\n    \\beta_{01} + \\beta_{11}x+ \\beta_{21}x^2 +\\epsilon     & \\quad \\text{if } x < \\xi\\\\\n    \\beta_{02} + \\beta_{12}x+ \\beta_{22}x^2+\\beta_{32}x^3+\\epsilon      & \\quad \\text{if } x \\ge \\xi\n  \\end{cases}\\]\n\nThe joint points of pieces are called knots.\nUsing more knots leads to a more flexible piecewise polynomial.\n\n\nWith \\(K\\) different knots, how many different polynomials do we have?\n\n\nA polynomial regression may provide a poor fit, and increasing the order does not improve the situation.\nThis may happen when the regression function behaves differently in different parts of the range of x ."
  },
  {
    "objectID": "slides/07-splines.html#u.s.-birth-rate-from-1917-to-2003",
    "href": "slides/07-splines.html#u.s.-birth-rate-from-1917-to-2003",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "U.S. Birth Rate from 1917 to 20031\n",
    "text": "U.S. Birth Rate from 1917 to 20031\n\n\n\n\n\n     Year Birthrate\n1917 1917       183\n1918 1918       184\n1919 1919       163\n1920 1920       180\n1921 1921       181\n1922 1922       173\n1923 1923       168\n1924 1924       177\n1925 1925       172\n1926 1926       170\n1927 1927       164\n1928 1928       152\n1929 1929       145\n1930 1930       145\n1931 1931       139\n1932 1932       132\n1933 1933       126\n1934 1934       130\n1935 1935       130\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe data is only for illustrating ideas of different methods. The methods introduced here are best for inference about relationship between variables of some physical or natural system, not mainly for time series forecasting."
  },
  {
    "objectID": "slides/07-splines.html#a-polynomial-regression-provide-a-poor-fit",
    "href": "slides/07-splines.html#a-polynomial-regression-provide-a-poor-fit",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "A Polynomial Regression Provide a Poor Fit",
    "text": "A Polynomial Regression Provide a Poor Fit\n\nlmfit3 <- lm(Birthrate ~ poly(Year - mean(Year), degree = 3, \n                              raw = TRUE),  \n             data = birthrates)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nraw if true, use raw and not orthogonal polynomials.\n\nA polynomial regression may provide a poor fit, and increasing the order does not improve the situation.\nThis may happen when the regression function behaves differently in different parts of the range of x."
  },
  {
    "objectID": "slides/07-splines.html#piecewise-polynomials-3-knots-at-1936-60-78",
    "href": "slides/07-splines.html#piecewise-polynomials-3-knots-at-1936-60-78",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "Piecewise Polynomials: 3 knots at 1936, 60, 78",
    "text": "Piecewise Polynomials: 3 knots at 1936, 60, 78\n\n\n\n\\[y=\\begin{cases}\n    \\beta_{01} + \\beta_{11}x+ \\epsilon & \\quad \\text{if } x < 1936\\\\\n    \\beta_{02} + \\beta_{12}x+\\epsilon  & \\quad \\text{if } 1936 \\le x < 1960 \\\\\n    \\beta_{03} + \\beta_{13}x+\\epsilon  & \\quad \\text{if } 1960 \\le x < 1978 \\\\\n    \\beta_{04} + \\beta_{14}x+\\epsilon  & \\quad \\text{if } 1978 \\le x\n  \\end{cases}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nAny issue of piecewise polynomials?"
  },
  {
    "objectID": "slides/07-splines.html#splines",
    "href": "slides/07-splines.html#splines",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "Splines",
    "text": "Splines\nSplines of degree \\(d\\) are smooth piecewise polynomials of degree \\(d\\) with continuity in derivatives (smoothing) up to degree \\(d-1\\) at each knot.\n\nHow do we turn the piecewise regression of degree 1 into a regression spline?\n\n\\[y=\\begin{cases}\n    \\beta_{01} + \\beta_{11}x+ \\epsilon & \\quad \\text{if } x < 1936\\\\\n    \\beta_{02} + \\beta_{12}x+\\epsilon  & \\quad \\text{if } 1936 \\le x < 1960 \\\\\n    \\beta_{03} + \\beta_{13}x+\\epsilon  & \\quad \\text{if } 1960 \\le x < 1978 \\\\\n    \\beta_{04} + \\beta_{14}x+\\epsilon  & \\quad \\text{if } 1978 \\le x\n  \\end{cases}\\]\n\n\n\n\nFor splines of degree 1, we require continuous piecewise linear function\n\n\n\n\\[\\begin{align}\n\\beta_{01} + \\beta_{11}1936 &= \\beta_{02} + \\beta_{12}1936\\\\\n\\beta_{02} + \\beta_{12}1960 &= \\beta_{03} + \\beta_{13}1960\\\\\n\\beta_{03} + \\beta_{13}1978 &= \\beta_{04} + \\beta_{14}1978\n\\end{align}\\]"
  },
  {
    "objectID": "slides/07-splines.html#splinesbs",
    "href": "slides/07-splines.html#splinesbs",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "splines::bs()",
    "text": "splines::bs()\n\npar(mar = c(2,2,2,0))\n\n\n\nThe function is continuous everywhere, also at knots \\(\\xi_1, \\xi_2,\\) and \\(\\xi_3\\), i.e.¬†\\(f_{k}(\\xi_k^-) = f_{k+1}(\\xi_k^+)\\).\nLinear everywhere except \\(\\xi_1, \\xi_2,\\) and \\(\\xi_3\\).\nHas a different slope for each region."
  },
  {
    "objectID": "slides/07-splines.html#splines-as-adaptive-basis-function-model",
    "href": "slides/07-splines.html#splines-as-adaptive-basis-function-model",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "Splines as Adaptive Basis Function Model",
    "text": "Splines as Adaptive Basis Function Model\nFor linear splines with 3 knots,\n\\[\\begin{align}\nb_1(x) &= x\\\\\nb_2(x) &= (x - \\xi_1)_+\\\\\nb_3(x) &= (x - \\xi_2)_+\\\\\nb_4(x) &= (x - \\xi_3)_+\n\\end{align}\\] where\n\n\n\n\\((x - \\xi_k)_+\\) denotes the positive part:\n\n\\[(x - \\xi_k)_+=\\begin{cases}\n    x - \\xi_k & \\quad \\text{if }x > \\xi_k\\\\\n    0  & \\quad \\text{otherwise}\n  \\end{cases}\\]\n\\[\\begin{align} y &= \\beta_0 + \\beta_1 b_1(x) + \\beta_2 b_2(x) + \\beta_3 b_3(x) + \\beta_4 b_4(x) + \\epsilon\\\\\n&=\\beta_0 + \\beta_1 x + \\beta_2 (x - \\xi_1)_+ + \\beta_3 (x - \\xi_2)_+ + \\beta_4 (x - \\xi_3)_+ + \\epsilon\n\\end{align}\\]"
  },
  {
    "objectID": "slides/07-splines.html#linear-splines-basis-functions",
    "href": "slides/07-splines.html#linear-splines-basis-functions",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "Linear Splines Basis Functions",
    "text": "Linear Splines Basis Functions"
  },
  {
    "objectID": "slides/07-splines.html#splines-as-adaptive-basis-function-model-1",
    "href": "slides/07-splines.html#splines-as-adaptive-basis-function-model-1",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "Splines as Adaptive Basis Function Model",
    "text": "Splines as Adaptive Basis Function Model\nWith degree \\(d\\) and \\(K\\) knots, the regression spline with first \\(d-1\\) derivatives being continuous at the knots can be represented as\n\\[\\begin{align} y &= \\beta_0 + \\beta_1 b_1(x) + \\beta_2 b_2(x) + \\dots + \\beta_d b_d(x) + \\beta_{d+1}b_{d+1}(x) + \\dots + \\beta_{d+K}b_{d+K}(x) + \\epsilon\n\\end{align}\\] where\n\n\\(b_j(x) = x^j, j = 1, 2, \\dots, d\\)\n\\(b_{d+s} = (x - \\xi_s)^{d}_+, s = 1, \\dots, K\\) (truncated power basis function)\n\n\nCan you write the model with \\(d = 2\\) and \\(k = 3\\)?"
  },
  {
    "objectID": "slides/07-splines.html#cubic-splines",
    "href": "slides/07-splines.html#cubic-splines",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "Cubic Splines1\n",
    "text": "Cubic Splines1\n\n\nThe cubic spline is a spline of degree 3 with first 2 derivatives are continuous at the knots.\n\n\\[\\begin{align} y &= \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\sum_{k = 1}^K \\beta_{k+3} (x - \\xi_k)_+^3 + \\epsilon\n\\end{align}\\]\n\nSmooth at knots because\n\n\\(f(\\xi_k^-) = f(\\xi_k^+)\\)\n\\(f'(\\xi_k^-) = f'(\\xi_k^+)\\)\n\\(f''(\\xi_k^-) = f''(\\xi_k^+)\\)\n\\(f^{(3)}(\\xi_k^-) \\ne f^{(3)}(\\xi_k^+)\\)\n\n\nCubic splines are popular because most human eyes cannot detect the discontinuity at the knots."
  },
  {
    "objectID": "slides/07-splines.html#degrees-of-freedom",
    "href": "slides/07-splines.html#degrees-of-freedom",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "Degrees of Freedom",
    "text": "Degrees of Freedom\n\nIn regression, (effective) degrees of freedom (df) can be viewed as the number of regression coeffcients that are freely to move.\nIt measures the model complexity/flexiblity. The larger df is, the more flexible the model is.\n\n\n\n\nPiecewise linear: 3 knots, 4 linear regressions, and 8 dfs.\n\n\n\n\\[y=\\begin{cases}\n    \\beta_{01} + \\beta_{11}x+ \\epsilon & \\quad \\text{if } x < 1936\\\\\n    \\beta_{02} + \\beta_{12}x+\\epsilon  & \\quad \\text{if } 1936 \\le x < 1960 \\\\\n    \\beta_{03} + \\beta_{13}x+\\epsilon  & \\quad \\text{if } 1960 \\le x < 1978 \\\\\n    \\beta_{04} + \\beta_{14}x+\\epsilon  & \\quad \\text{if } 1978 \\le x\n  \\end{cases}\\]\n\n\n\n\n\nLinear splines (continuous piecewise linear): 8 - 3 constraints = 5 dfs.\n\n\n\n\\[\\begin{align}\n\\beta_{01} + \\beta_{11}1936 &= \\beta_{02} + \\beta_{12}1936\\\\\n\\beta_{02} + \\beta_{12}1960 &= \\beta_{03} + \\beta_{13}1960\\\\\n\\beta_{03} + \\beta_{13}1978 &= \\beta_{04} + \\beta_{14}1978\n\\end{align}\\]"
  },
  {
    "objectID": "slides/07-splines.html#degrees-of-freedom-1",
    "href": "slides/07-splines.html#degrees-of-freedom-1",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "Degrees of Freedom",
    "text": "Degrees of Freedom\n\nWith degree \\(d\\) and \\(K\\) knots,\n\n\\[\\begin{align} \\text{df} &= \\text{(# regions)} \\times \\text{(# coefficients)} - \\text{(# constraints)} \\times \\text{(# knots)}\\\\\n&=(K + 1)(d + 1) - dK\\\\\n&= K + d + 1 \\end{align} \\]\n\n\n\n\nCubic splines:\n\nWe have 3 constraints:\n\ncontinuity \\(f(\\xi ^ {-}) = f(\\xi ^ {+})\\)\n\n1st derivative \\(f'(\\xi ^ {-}) = f'(\\xi ^ {+})\\)\n\n2nd derivative \\(f''(\\xi ^ {-}) = f''(\\xi ^ {+})\\)\n\n\n\\(\\text{df} = \\text{(# regions)} \\times \\text{(# coef)} - \\text{(# constraints)} \\times \\text{(# knots)} = (K + 1)4 - 3K = 4 + K\\)"
  },
  {
    "objectID": "slides/07-splines.html#cubic-splines-1",
    "href": "slides/07-splines.html#cubic-splines-1",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "Cubic Splines",
    "text": "Cubic Splines\n\ncub_sp <- lm(Birthrate ~ splines::bs(Year, degree = 3, knots = c(1936, 1960, 1978)), \n             data = birthrates)"
  },
  {
    "objectID": "slides/07-splines.html#natural-splines",
    "href": "slides/07-splines.html#natural-splines",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "Natural Splines",
    "text": "Natural Splines\n\nSplines1 tends to produce erratic and undesirable results near the boundaries, and huge variance at the outer range of the predictors.\n\n\n\nA natural spline is a regression spline with additional boundary constraints:\n\nThe natural function is linear at the boundary (in the region where \\(X\\) is smaller than the smallest knot, or larger than the largest knot).\n\n\n\n\n\n\n\nNatural splines generally produce more stable estimates at the boundaries.\nAssuming linearity near the boundary is reasonable since there is less information available there.\n\n\nsplines::ns()\n\n\n\n\nNatural Cubic Spline (NCS) forces the 2nd and 3rd derivatives to be zero at the boundaries.\nThe constraints frees up 4 dfs.\nThe df of NCS is \\(K\\).\n\n\nEspecially for high-degree polynomials."
  },
  {
    "objectID": "slides/07-splines.html#spline-and-natural-spline-comparison",
    "href": "slides/07-splines.html#spline-and-natural-spline-comparison",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "Spline and Natural Spline Comparison",
    "text": "Spline and Natural Spline Comparison\n\nCubic Spline vs.¬†Natural Cubic Spline with the same degrees of freedom 6."
  },
  {
    "objectID": "slides/07-splines.html#b-splines-basis-functions",
    "href": "slides/07-splines.html#b-splines-basis-functions",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "\nB-Splines Basis Functions",
    "text": "B-Splines Basis Functions\n\nB-splines use a different basis representation for regression splines.\n\n\n\n\nbs(x, df = NULL, knots = NULL, \n   degree = 3, intercept = FALSE)\n\n\n\ndf: the number of basis\n\nknots: the quantiles of \\(x\\)\n\n\n\n\n\n\n\n\n\n\n\n\nns(x, df = NULL, knots = NULL, \n   degree = 3, intercept = FALSE)\n\n\n\ndf: the number of basis\n\nknots: the quantiles of \\(x\\)\n\n\n\n\n\n\n\n\n\n\n\n\nB-splines is more computationally efficient"
  },
  {
    "objectID": "slides/07-splines.html#practical-issues",
    "href": "slides/07-splines.html#practical-issues",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "Practical Issues",
    "text": "Practical Issues\n\n\nHow many knots should we use\n\n\nAs few knots as possible, with at least 5 data points per segment (Wold, 1974)\nCross-validation: e.g., choose the \\(K\\) giving the smallest \\(MSE_{CV}\\)\n\n\n\n\n\n\nWhere to place the knots\n\n\nNo more than one extreme or inflection point per segment (Wold, 1974)\nIf possible, the extreme points should be centered in the segment\n\nMore knots in places where the function might vary most rapidly, and fewer knots where it seems more stable\nPlace knots in a uniform fashion\n\nSpecify the desired df, and have the software place the corresponding number of knots at uniform quantiles of the data (bs(), ns())\n\n\n\n\n\n\n\nWhat is the degree of functions in each region\n\nUse \\(d < 4\\) to avoid overly flexible curve fitting\nCubic spline is popular\n\n\n\nToo many knots can overfit Regression splines often give superior results to polynomial regression"
  },
  {
    "objectID": "slides/07-splines.html#roughness-penalty-approach",
    "href": "slides/07-splines.html#roughness-penalty-approach",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "Roughness Penalty Approach",
    "text": "Roughness Penalty Approach\n\nOur goal is inference or curve fitting rather than out of range prediction or forecasting.\nIs there a method that we can select the number and location of knots automatically?\n\nConsider this criterion for fitting a smooth function \\(g(x)\\) to some data:\n\\[\\begin{equation} \\min_{g} \\sum_{i=1}^n \\left( y_i - g(x_i) \\right)^2 + \\lambda \\int g''(t)^2 \\, dt \\end{equation}\\]\n\nThe first term is \\(SS_{res}\\), and tries to make \\(g(x)\\) match the data at each \\(x_i\\). (Goodness-of-fit)\nThe second term with \\(\\lambda \\ge 0\\) is a roughness penalty and controls how wiggly \\(g(x)\\) is.\n\n\n\n\n\nLoss + Penalty"
  },
  {
    "objectID": "slides/07-splines.html#roughness-penalty-approach-1",
    "href": "slides/07-splines.html#roughness-penalty-approach-1",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "Roughness Penalty Approach",
    "text": "Roughness Penalty Approach\n\\[\\begin{equation} \\min_{g} \\sum_{i=1}^n \\left( y_i - g(x_i) \\right)^2 + \\lambda \\int g''(t)^2 \\, dt \\end{equation}\\]\n\nHow \\(\\lambda\\) affects the shape of \\(g(x)\\)?\n\n\nThe smaller \\(\\lambda\\) is, the more wiggly \\(g(x)\\) is, eventually interpolating \\(y_i\\) when \\(\\lambda = 0\\).\nAs \\(\\lambda \\rightarrow \\infty\\), \\(g(x)\\) becomes linear.\nThe function \\(g\\) that minimizes the objective is known as a smoothing spline.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe solution \\(\\hat{g}\\) is a natural cubic spline, with knots at \\(x_1, x_2, \\dots , x_n\\)!\nBut, it is not the same NCS gotten from the basis function approach.\nIt is a shrunken version where \\(\\lambda\\) controls the level of shrinkage."
  },
  {
    "objectID": "slides/07-splines.html#properties-of-smoothing-splines",
    "href": "slides/07-splines.html#properties-of-smoothing-splines",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "Properties of Smoothing splines",
    "text": "Properties of Smoothing splines\n\nSmoothing splines avoid the knot-selection issue, leaving a single \\(\\lambda\\) to be chosen.\n\n\n\nLinear regression\n\n\\(\\hat{\\mathbf{y}} = \\bf H \\mathbf{y}\\) where \\(\\bf H\\) is the hat matrix\nThe degrees of freedom \\((\\text{# coef})\\) is \\[p = \\sum_{i=1}^n {\\bf H}_{ii}\\]\nPRESS for model selection\n\n\\[\\begin{align} PRESS &= \\sum_{i = 1}^n \\left( y_i - \\hat{y}_i^{(-i)}\\right)^2 \\\\\n&= \\sum_{i = 1}^n \\left[ \\frac{y_i - \\hat{y}_i}{1 - {\\bf H}_{ii}}\\right]^2\\end{align}\\]\n\nSmoothing splines\n\n\\(\\hat{\\mathbf{g}}_{\\lambda} = {\\bf S}_{\\lambda} \\mathbf{y}\\) where \\(\\bf {\\bf S}_{\\lambda}\\) is the smoother matrix.\nThe effective degrees of freedom \\((\\text{# coef})\\) is \\[df_{\\lambda} = \\sum_{i=1}^n \\{{\\bf S}_{\\lambda}\\}_{ii} \\in (0, n)\\]\nLOOCV for choosing \\(\\lambda\\)1\n\n\\[\\begin{align} (SS_{res})_{CV}(\\lambda) &= \\sum_{i = 1}^n \\left( y_i - \\hat{g}_{\\lambda}^{(-i)}(x_i)\\right)^2 \\\\\n&= \\sum_{i = 1}^n \\left[ \\frac{y_i - \\hat{g}_{\\lambda}(x_i)}{1 - \\{{\\bf S}_{\\lambda}\\}_{ii}}\\right]^2 \\end{align}\\]\n\n\nGCV can be used too."
  },
  {
    "objectID": "slides/07-splines.html#smooth.spline",
    "href": "slides/07-splines.html#smooth.spline",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "smooth.spline()",
    "text": "smooth.spline()\n\nsmooth.spline(x, y, df, lambda, cv = FALSE)\n\n\n\ncv = TRUE use LOOCV; cv = FALSE use GCV\n\n\nfit <- smooth.spline(birthrates$Year, birthrates$Birthrate)\n\n\n\nfit$df\n\n[1] 60.8\n\n\noverfitting since the birthrate data has little variation in adjacent years"
  },
  {
    "objectID": "slides/07-splines.html#smooth.spline-1",
    "href": "slides/07-splines.html#smooth.spline-1",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "smooth.spline()",
    "text": "smooth.spline()\n\npar(mar = c(2, 2, 0, 0))"
  },
  {
    "objectID": "slides/07-splines.html#extending-splines-to-multiple-variables",
    "href": "slides/07-splines.html#extending-splines-to-multiple-variables",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "Extending Splines to Multiple Variables",
    "text": "Extending Splines to Multiple Variables\n\nAll methods discussed so far are extensions of simple linear regression.\nHow to flexibly predict \\(y\\) or nonlinear regression function on the basis of several predictors \\(x_1, \\dots, x_p\\)?\n\n\nMaintaining the additive structure of linear models, but allowing nonlinear functions of each of the variables.\n\\[y = \\beta_0 + f_1(x_1) + f_2(x_2) + \\dots + f_p(x_p) + \\epsilon\\]\n\nThis is a generalized additive model.\n\nIt‚Äôs general: it can be modelling response with other distributions, binary, counts, positive values, for example.\nIt‚Äôs additive: calculate a separate \\(f_j\\) for each \\(x_j\\), and add together all of their contributions.\n\n\n\n\n\n\n\n\\(f_1(x_1) = x_1\\); \\(f_2(x_2) = x_2 + x_2^2\\); \\(f_3(x_3) = \\text{cubic splines}\\)"
  },
  {
    "objectID": "slides/07-splines.html#wage-data-in-isl",
    "href": "slides/07-splines.html#wage-data-in-isl",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "\nWage data in ISL",
    "text": "Wage data in ISL\n\nlibrary(ISLR2)\nattach(Wage)\ndplyr::glimpse(Wage)\n\nRows: 3,000\nColumns: 11\n$ year       <int> 2006, 2004, 2003, 2003, 2005, 2008, 2009, 2008, 2006, 2004,‚Ä¶\n$ age        <int> 18, 24, 45, 43, 50, 54, 44, 30, 41, 52, 45, 34, 35, 39, 54,‚Ä¶\n$ maritl     <fct> 1. Never Married, 1. Never Married, 2. Married, 2. Married,‚Ä¶\n$ race       <fct> 1. White, 1. White, 1. White, 3. Asian, 1. White, 1. White,‚Ä¶\n$ education  <fct> 1. < HS Grad, 4. College Grad, 3. Some College, 4. College ‚Ä¶\n$ region     <fct> 2. Middle Atlantic, 2. Middle Atlantic, 2. Middle Atlantic,‚Ä¶\n$ jobclass   <fct> 1. Industrial, 2. Information, 1. Industrial, 2. Informatio‚Ä¶\n$ health     <fct> 1. <=Good, 2. >=Very Good, 1. <=Good, 2. >=Very Good, 1. <=‚Ä¶\n$ health_ins <fct> 2. No, 2. No, 1. Yes, 1. Yes, 1. Yes, 1. Yes, 1. Yes, 1. Ye‚Ä¶\n$ logwage    <dbl> 4.32, 4.26, 4.88, 5.04, 4.32, 4.85, 5.13, 4.72, 4.78, 4.86,‚Ä¶\n$ wage       <dbl> 75.0, 70.5, 131.0, 154.7, 75.0, 127.1, 169.5, 111.7, 118.9,‚Ä¶"
  },
  {
    "objectID": "slides/07-splines.html#gamgam",
    "href": "slides/07-splines.html#gamgam",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "gam::gam()",
    "text": "gam::gam()\n\nlibrary(gam)\ngam.m3 <- gam(wage ~ s(year, df = 4) + s(age , df = 5) + education,\n              data = Wage)\n\n\ns() for smoothing splines\nCoefficients not that interesting; fitted functions are.\n\n\nupper and lower pointwise twice-standard-error curves are included\nThe left-hand panel indicates that holding age and education fixed, wage tends to increase slightly with year; this may be due to inflation. The center panel indicates that holding education and year fixed, wage tends to be highest for intermediate values of age, and lowest for the very young and very old."
  },
  {
    "objectID": "slides/07-splines.html#gam-fitting",
    "href": "slides/07-splines.html#gam-fitting",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "GAM fitting",
    "text": "GAM fitting\n\nNatural splines lm(wage ~ ns(year, df = 5) + ns(age, df = 5) + education)\nCan mix terms, e.g., smoothing splines and local regression\n\ngam(wage ~ s(year, df = 5) + lo(age, span = .5) + education)\n\nCan add interactions, e.g.¬†ns(age, df = 5):ns(year, df = 5)\nUse anova() to compare models."
  },
  {
    "objectID": "slides/07-splines.html#summary-of-gams",
    "href": "slides/07-splines.html#summary-of-gams",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "Summary of GAMs",
    "text": "Summary of GAMs\n\nAllow to fit a non-linear \\(f_j\\) to each \\(X_j\\), so that we can automatically model non-linear relationships that standard linear regression will miss.\nNo need to manually try out many different transformations on each variable individually.\nThe non-linear fits can potentially make more accurate predictions for the response.\nThe smoothness of the function \\(f_j\\) for the variable \\(X_j\\) can be summarized via degrees of freedom.\nThe model is additive. For fully general models, we look for more flexible approaches such as random forests and boosting.\n\n\nBecause the model is additive, we can examine the effect of each \\(X_j\\) on \\(Y\\) individually while holding all of the other variables fixed.\nThe smoothness of the function \\(f_j\\) for the variable \\(X_j\\) can be summarized via degrees of freedom."
  },
  {
    "objectID": "slides/07-splines.html#other-topics",
    "href": "slides/07-splines.html#other-topics",
    "title": "Splines and General Additive Models „Ä∞Ô∏è",
    "section": "Other Topics",
    "text": "Other Topics\n\nBasis spline (B-spline) definition (de Boor, 1978): more computationally efficient\nAlgorithms for fitting GAMs using smoothing splines: Backfitting, ISL Sec 7.9 Exercise 11.\nGAMs for binary response and classification.\nMultivariate Adaptive Regression Splines (MARS)\n\nHow many knots should be used Where to place the knots What is the degree of functions in each region\nwe can control that using the df parameter. We use a total of 6 parameters, chosen by the function automatically. However, this does not seems to perform better than the knots we implemented. The choice of knots can be crucial.\n\n\n\nmssc6250-s24.github.io/website"
  },
  {
    "objectID": "slides/04-linear-reg.html#linear-regression",
    "href": "slides/04-linear-reg.html#linear-regression",
    "title": "Linear Regression üìà",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nLinear regression is an approach to supervised learning.\nIt assumes that the dependence of \\(Y\\) on \\(X_1, X_2, \\dots, X_k\\)1 is linear.\nTrue regression functions \\(f(x)\\) are never linear!\nAlthough it is overly simplistic, the linear model has distinct advantages in terms of its interpretability and often shows good predictive performance.\n\nlinear regression is extremely useful both conceptually and practically\nThe textbook uses \\(p\\) instead of \\(k\\) to denote the number of predictors in the model. We use \\(k\\) here, but switch to \\(p\\) starting next week."
  },
  {
    "objectID": "slides/04-linear-reg.html#why-multiple-linear-regression-mlr-model",
    "href": "slides/04-linear-reg.html#why-multiple-linear-regression-mlr-model",
    "title": "Linear Regression üìà",
    "section": "Why Multiple Linear Regression (MLR) Model",
    "text": "Why Multiple Linear Regression (MLR) Model\n\nOur target response may be affected by several factors.\nTotal sales \\((Y)\\) and amount of money spent on advertising on YouTube (YT) \\((X_1)\\), Facebook (FB) \\((X_2)\\), Instagram (IG) \\((X_3)\\).1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredict sales based on the three advertising expenditures and see which medium is more effective.\nIn ISL, the predictors are TV, radio and newspaper."
  },
  {
    "objectID": "slides/04-linear-reg.html#fit-separate-simple-linear-regression-slr-models",
    "href": "slides/04-linear-reg.html#fit-separate-simple-linear-regression-slr-models",
    "title": "Linear Regression üìà",
    "section": "Fit Separate Simple Linear Regression (SLR) Models",
    "text": "Fit Separate Simple Linear Regression (SLR) Models\n\n\n\n‚ùå Fitting a separate SLR model for each predictor is not satisfactory."
  },
  {
    "objectID": "slides/04-linear-reg.html#dont-fit-a-separate-simple-linear-regression",
    "href": "slides/04-linear-reg.html#dont-fit-a-separate-simple-linear-regression",
    "title": "Linear Regression üìà",
    "section": "Don‚Äôt Fit a Separate Simple Linear Regression",
    "text": "Don‚Äôt Fit a Separate Simple Linear Regression\n\nüëâ How to make a single prediction of sales given levels of the 3 advertising media budgets?\n\n How to predict the sales when the amount spent on YT is 50, 100 on FB and 30 on IG? \n\n\n\n\n\nüëâ Each regression equation ignores the other 2 media in forming coefficient estimates.\n\n The effect of FB advertising on sales may be increased or decreased when YT and IG advertising are in the model. \n IG advertising may have no impact on sales when YT and FB advertising are in the model. \n\n\n\n\n\n\nüëçüëç Better approach: extend the SLR model so that it can directly accommodate multiple predictors."
  },
  {
    "objectID": "slides/04-linear-reg.html#multiple-linear-regression-model",
    "href": "slides/04-linear-reg.html#multiple-linear-regression-model",
    "title": "Linear Regression üìà",
    "section": "Multiple Linear Regression Model",
    "text": "Multiple Linear Regression Model\n\nThe population MLR model: \\[Y_i= \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\dots + \\beta_kX_{ik} + \\epsilon_i\\]  \n\nIn the advertising example, \\(k = 3\\) and \\[\\texttt{sales} = \\beta_0 + \\beta_1 \\times \\texttt{YouTube} + \\beta_2 \\times  \\texttt{Facebook} + \\beta_3 \\times \\texttt{Instagram} + \\epsilon\\]\n\nModel assumptions:\n\n\\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\)\n\n\n\n\n\nHow many parameters are there in the model?"
  },
  {
    "objectID": "slides/04-linear-reg.html#sample-mlr-model",
    "href": "slides/04-linear-reg.html#sample-mlr-model",
    "title": "Linear Regression üìà",
    "section": "Sample MLR Model",
    "text": "Sample MLR Model\n\nGiven the training sample \\((x_{11}, \\dots, x_{1k}, y_1), (x_{21}, \\dots, x_{2k}, y_2), \\dots, (x_{n1}, \\dots, x_{nk}, y_n),\\)\n\nThe sample MLR model to be trained: \\[\\begin{align*}\ny_i &= \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_k x_{ik} + \\epsilon_i \\\\\n&= \\beta_0 + \\sum_{j=1}^k\\beta_j x_{ij} + \\epsilon_i, \\quad i = 1, 2, \\dots, n\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/04-linear-reg.html#regression-hyperplane",
    "href": "slides/04-linear-reg.html#regression-hyperplane",
    "title": "Linear Regression üìà",
    "section": "Regression Hyperplane",
    "text": "Regression Hyperplane\n\n\nSLR: regression line\nMLR: regression hyperplane or response surface\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_{2}x_2 + \\epsilon\\)\n\\(E(y \\mid x_1, x_2) = 50 + 10x_1 + 7x_2\\)"
  },
  {
    "objectID": "slides/04-linear-reg.html#response-surface",
    "href": "slides/04-linear-reg.html#response-surface",
    "title": "Linear Regression üìà",
    "section": "Response Surface",
    "text": "Response Surface\n\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_{2}x_2 + \\beta_{11}x_1^2 + \\beta_{22}x_2^2 + \\beta_{12}x_1x_2 + \\epsilon\\)\n\\(E(y \\mid x_1, x_2) = 800+10x_1+7x_2 -8.5x_1^2-5x_2^2- 4x_1x_2\\)\nüòé ü§ì A linear regression model can describe a complex nonlinear relationship between the response and predictors!"
  },
  {
    "objectID": "slides/04-linear-reg.html#ordinary-least-squares-estimation-of-the-coefficients",
    "href": "slides/04-linear-reg.html#ordinary-least-squares-estimation-of-the-coefficients",
    "title": "Linear Regression üìà",
    "section": "Ordinary Least Squares Estimation of the Coefficients",
    "text": "Ordinary Least Squares Estimation of the Coefficients\n\\[\\begin{align*}\ny_i &= \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_k x_{ik} + \\epsilon_i \\\\\n&= \\beta_0 + \\sum_{j=1}^k\\beta_j x_{ij} + \\epsilon_i, \\quad i = 1, 2, \\dots, n\n\\end{align*}\\]\n\nThe least-squares function Sum of Squared Residuals (\\(SS_{res}\\))1 is \\[SS_{res}(\\alpha_0, \\alpha_1, \\dots, \\alpha_k) = \\sum_{i=1}^n\\left(y_i - \\alpha_0 - \\sum_{j=1}^k\\alpha_j x_{ij}\\right)^2\\]\n\n\n\\(SS_{res}\\) must be minimized with respect to the coefficients, i.e., \\[(b_0, b_1, \\dots, b_k) = \\underset{{\\alpha_0, \\alpha_1, \\dots, \\alpha_k}}{\\mathrm{arg \\, min}}  SS_{res}(\\alpha_0, \\alpha_1, \\dots, \\alpha_k)\\]\n\nIn ISL, RSS is used for Residual Sum of Squares."
  },
  {
    "objectID": "slides/04-linear-reg.html#geometry-of-least-squares-estimation",
    "href": "slides/04-linear-reg.html#geometry-of-least-squares-estimation",
    "title": "Linear Regression üìà",
    "section": "Geometry of Least Squares Estimation",
    "text": "Geometry of Least Squares Estimation"
  },
  {
    "objectID": "slides/04-linear-reg.html#least-squares-normal-equations",
    "href": "slides/04-linear-reg.html#least-squares-normal-equations",
    "title": "Linear Regression üìà",
    "section": "Least-squares Normal Equations",
    "text": "Least-squares Normal Equations\n\\[\\begin{align*}\n\\left.\\frac{\\partial SS_{res}}{\\partial\\alpha_0}\\right\\vert_{b_0, b_1, \\dots, b_k} &= -2 \\sum_{i=1}^n\\left(y_i - b_0 - \\sum_{j=1}^k b_j x_{ij}\\right) = 0\\\\\n\\left.\\frac{\\partial SS_{res}}{\\partial\\alpha_j}\\right\\vert_{b_0, b_1, \\dots, b_k} &= -2 \\sum_{i=1}^n\\left(y_i - b_0 - \\sum_{j=1}^k b_j x_{ij}\\right)x_{ij} = 0, \\quad j = 1, 2, \\dots, k\n\\end{align*}\\]\n\n\\(k + 1\\) equations with \\(k + 1\\) unknown parameters.\nThe ordinary least squares estimators are the solutions to the normal equations.\n\n\n\nüçπ üç∫ üç∏ ü•Ç I buy you a drink if you solve the equations by hand without using matrix notations or operations!"
  },
  {
    "objectID": "slides/04-linear-reg.html#interpreting-coefficients",
    "href": "slides/04-linear-reg.html#interpreting-coefficients",
    "title": "Linear Regression üìà",
    "section": "Interpreting Coefficients",
    "text": "Interpreting Coefficients\n\n\n\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)    2.939      0.312   9.422     0.00\nyoutube        0.046      0.001  32.809     0.00\nfacebook       0.189      0.009  21.893     0.00\ninstagram     -0.001      0.006  -0.177     0.86\n\n\n\n\\[\\hat{y} = b_0 + b_1 x_1 + \\cdots + b_kx_k\\]\n\\[\\widehat{\\texttt{sales}} = 2.939 + 0.046 \\times \\texttt{YouTube} + 0.189 \\times  \\texttt{Facebook} - 0.001 \\times \\texttt{Instagram}\\]\n\n\n\\(b_1\\): Holding all other predictors fixed, for one unit increase of Youtube, the sales is expected to be increased, on average, by 0.046 units.\n\n\\(b_2\\): All else held constant, one unit increase of Facebook leads to, on average, 0.189 unit increase of sales.\n\n\\(b_0\\): The sales with no expenditures on Youtube, Facebook, and Instagram is expected to be 2.939. (Make sense?!)"
  },
  {
    "objectID": "slides/04-linear-reg.html#inference-on-coefficients",
    "href": "slides/04-linear-reg.html#inference-on-coefficients",
    "title": "Linear Regression üìà",
    "section": "Inference on Coefficients",
    "text": "Inference on Coefficients\n\n\n\n\n\nThe \\((1-\\alpha)100\\%\\) Wald confidence interval (CI) for \\(\\beta_j\\), \\(j = 0, 1, \\dots, k\\) is\n\n\\[\\left(b_j- t_{\\alpha/2, n-p}~se(b_j), \\quad b_j + t_{\\alpha/2, n-p}~ se(b_j)\\right)\\]\n\n\n\n            2.5 % 97.5 %\n(Intercept)  2.32   3.55\nyoutube      0.04   0.05\nfacebook     0.17   0.21\ninstagram   -0.01   0.01\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nThese are marginal CIs seperately for each \\(b_j\\).\nIndividual CI ignores the correlation between \\(b_j\\)s.\nBetter to consider elliptically-shaped regions."
  },
  {
    "objectID": "slides/04-linear-reg.html#collinearity",
    "href": "slides/04-linear-reg.html#collinearity",
    "title": "Linear Regression üìà",
    "section": "Collinearity",
    "text": "Collinearity\n\nInterpretation makes sense when predictors are not highly correlated.\n\nCorrelations amongst predictors cause problems (Collinearity).\n\n\nLarge variance of coefficients.\n\nLarge magnitude of coefficients.\n\nInstable and wrong signed coefficients.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThink about the standard error size. Do we tend to conclude \\(\\beta_j = 0\\) or not?"
  },
  {
    "objectID": "slides/04-linear-reg.html#extrapolation",
    "href": "slides/04-linear-reg.html#extrapolation",
    "title": "Linear Regression üìà",
    "section": "Extrapolation",
    "text": "Extrapolation\n\nRegression models are intended as interpolation equations over the range of the regressors used to fit the model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDo not use regression for time series forecasting!"
  },
  {
    "objectID": "slides/04-linear-reg.html#regression-is-not-for-causal-inference",
    "href": "slides/04-linear-reg.html#regression-is-not-for-causal-inference",
    "title": "Linear Regression üìà",
    "section": "Regression is Not for Causal Inference",
    "text": "Regression is Not for Causal Inference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClaims of causality should be avoided for observational data."
  },
  {
    "objectID": "slides/04-linear-reg.html#practical-significance-vs.-statisical-significance",
    "href": "slides/04-linear-reg.html#practical-significance-vs.-statisical-significance",
    "title": "Linear Regression üìà",
    "section": "Practical Significance vs.¬†Statisical Significance",
    "text": "Practical Significance vs.¬†Statisical Significance\n\n\n\n\n\n\nImportant\n\n\n\n\n\n\\(H_0:\\beta_j = 0\\) will always be rejected as long as the sample size is large enough, even \\(x_j\\) has a very small effect on \\(y\\).\n\nConsider the practical significance of the result, not just the statistical significance.\nUse confidence intervals to draw conclusions instead of relying only on p-values.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nIf the sample size is small, there may not be enough evidence to reject \\(H_0:\\beta_j = 0\\).\n\n\nDo Not immediately conclude that the variable has no association with the response.\nThere may be a linear association that is just not strong enough to detect given your data, or there may be a non-linear association.\n\n\n\n\n\n\n\np-value is a dichotomous approach"
  },
  {
    "objectID": "slides/04-linear-reg.html#test-for-significance",
    "href": "slides/04-linear-reg.html#test-for-significance",
    "title": "Linear Regression üìà",
    "section": "Test for significance",
    "text": "Test for significance\n\nTest for significance: Determine if there is a linear relationship between the response and any of the regressor variables.\n \\(H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_k = 0 \\quad H_1: \\beta_j \\ne 0 \\text{ for at least one } j\\) \n\n\n\n\\(\\sum_{i=1}^n(y_i - \\overline{y})^2 = \\sum_{i=1}^n(\\hat{y}_i - \\overline{y})^2 + \\sum_{i=1}^n(y_i - \\hat{y}_i)^2\\)\nReject \\(H_0\\) if \\(F_{test} > F_{\\alpha, k, n - k - 1}\\).\n\n\n\nWhat is the sample statistic we use to accesses how well the model fits the data?"
  },
  {
    "objectID": "slides/04-linear-reg.html#reduced-model-vs.-full-model",
    "href": "slides/04-linear-reg.html#reduced-model-vs.-full-model",
    "title": "Linear Regression üìà",
    "section": "Reduced Model vs.¬†Full Model",
    "text": "Reduced Model vs.¬†Full Model\n\nOverall test of significance: all predictors vs.¬†Marginal \\(t\\)-test: one single predictor\nHow to test any subset of predictors?\n\n\n\n\nFull Model: \\(y = \\beta_0 + \\beta_1x_1+\\beta_2x_2 + \\beta_3x_3 + \\beta_4x_4 + \\epsilon\\)\n\n\\(H_0: \\beta_{2} = \\beta_{4} = 0\\)\n\n\n\n\n\nReduced Model (under \\(H_0\\)): \\(y = \\beta_0 + \\beta_1x_1 + \\beta_3x_3 + \\epsilon\\)\n\nLike to see if \\(x_2\\) and \\(x_4\\) can contribute significantly to the regression model when \\(x_1\\) and \\(x_3\\) are in the model.\n\nIf yes, \\(\\beta_{2} \\ne 0\\) and/or \\(\\beta_{4} \\ne 0\\). (Reject \\(H_0\\))\nOtherwise, \\(\\beta_{2} = \\beta_{4} = 0\\). (Do not reject \\(H_0\\))\n\n\n\n\nGiven \\(x_1\\) and \\(x_3\\) in the model, we examine how much extra \\(SS_R\\) is increased (\\(SS_{res}\\) is reduced) if \\(x_2\\) and \\(x_4\\) are added to the model."
  },
  {
    "objectID": "slides/04-linear-reg.html#extra-sum-of-squares-and-partial-f-test",
    "href": "slides/04-linear-reg.html#extra-sum-of-squares-and-partial-f-test",
    "title": "Linear Regression üìà",
    "section": "Extra-sum-of-squares and Partial \\(F\\)-test",
    "text": "Extra-sum-of-squares and Partial \\(F\\)-test\n\n\\(F_{test} = \\frac{SS_R(\\beta_2, \\beta_4 \\mid \\beta_1, \\beta_3)/r}{SS_{res}/(n-k-1)} = \\frac{MS_R(\\beta_2, \\beta_4 \\mid \\beta_1, \\beta_3)}{MS_{res}}\\) where \\(r\\) is the number of coefficients being tested.\nUnder \\(H_0\\) that \\(\\beta_{2} = \\beta_{4} = 0\\), \\(F_{test} \\sim F_{r, n-k-1}\\).\nReject \\(H_0\\) if \\(F_{test} > F_{\\alpha, r, n-k-1}\\).\n\n\n\nGiven the regressors of \\(x_1, x_3\\) are in the model,\n\nIf the regressors of \\(x_2\\) and \\(x_4\\) contribute much, \\(SS_R(\\beta_2, \\beta_4 \\mid \\beta_1, \\beta_3)\\) will be large.\nA large \\(SS_R(\\beta_2, \\beta_4 \\mid \\beta_1, \\beta_3)\\) implies a large \\(F_{test}\\).\nA large \\(F_{test}\\) tends to reject \\(H_0\\), and conclude that \\(\\beta_{2} \\ne 0\\) and/or \\(\\beta_{4} \\ne 0\\).\nThe regressors of \\((x_2, x_4)\\) provide additional explanatory and prediction power that \\((x_1, x_3)\\) cannot provide."
  },
  {
    "objectID": "slides/04-linear-reg.html#regression-model-in-matrix-form",
    "href": "slides/04-linear-reg.html#regression-model-in-matrix-form",
    "title": "Linear Regression üìà",
    "section": "Regression Model in Matrix Form",
    "text": "Regression Model in Matrix Form\n\\[y_i= \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_kx_{ik} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2), \\quad i = 1, 2, \\dots, n\\]\n\n\n\\[\\bf y = \\bf X \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\] where\n\\[\\begin{align}\n\\bf y = \\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n \\end{bmatrix},\\quad\n\\bf X = \\begin{bmatrix}\n  1 & x_{11} & x_{12} & \\cdots & x_{1k} \\\\\n  1 & x_{21} & x_{22} & \\cdots & x_{2k} \\\\\n  \\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  1 & x_{n1} & x_{n2} & \\cdots & x_{nk} \\end{bmatrix}, \\quad\n\\boldsymbol{\\beta} = \\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_k \\end{bmatrix} , \\quad\n\\boldsymbol{\\epsilon} = \\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\vdots \\\\\n\\epsilon_n\\end{bmatrix}\n\\end{align}\\]\n\n\n\\({\\bf X}_{n \\times p}\\): Design matrix\n\n\\(\\boldsymbol{\\epsilon} \\sim MVN_n({\\bf 0}, \\sigma^2 {\\bf I}_n)\\)1\n\n\n\nFor simplicity and convenience, \\(N({\\bf a}, {\\bf B})\\) represents a multivariate normal distribution with mean vector \\({\\bf a}\\) and covariance matrix \\({\\bf B}\\)."
  },
  {
    "objectID": "slides/04-linear-reg.html#least-squares-estimation-in-matrix-form",
    "href": "slides/04-linear-reg.html#least-squares-estimation-in-matrix-form",
    "title": "Linear Regression üìà",
    "section": "Least Squares Estimation in Matrix Form",
    "text": "Least Squares Estimation in Matrix Form\n\\[\\begin{align}\nS(\\boldsymbol{\\beta}) = \\sum_{i=1}^n\\epsilon_i^2 = \\boldsymbol{\\epsilon}'\\boldsymbol{\\epsilon} &= (\\bf y - \\bf X \\boldsymbol{\\beta})'(\\bf y - \\bf X \\boldsymbol{\\beta}) \\\\\n&= \\bf y'\\bf y - \\boldsymbol{\\beta}'\\bf X'\\bf y - \\bf y'\\bf X \\boldsymbol{\\beta} + \\boldsymbol{\\beta}' \\bf X' \\bf X \\boldsymbol{\\beta} \\\\\n&={\\bf y}'{\\bf y} - 2\\boldsymbol{\\beta}'{\\bf X}'{\\bf y} + \\boldsymbol{\\beta}' {\\bf X}' {\\bf X} \\boldsymbol{\\beta}\n\\end{align}\\]\n\nLet \\({\\bf t}\\) and \\({\\bf a}\\) be \\(n \\times 1\\) column vectors, and \\({\\bf A}_{n \\times n}\\) is a symmetric matrix.\n\n\\(\\frac{\\partial {\\bf t}'{\\bf a} }{\\partial {\\bf t}} = \\frac{\\partial {\\bf a}'{\\bf t} }{\\partial {\\bf t}} = {\\bf a}\\)\n\\(\\frac{\\partial {\\bf t}'{\\bf A} {\\bf t}}{\\partial {\\bf t}} = 2{\\bf A} {\\bf t}\\)\n\n\n\nNormal equations: \\(\\begin{align} \\left.\\frac{\\partial S}{\\partial \\boldsymbol{\\beta}}\\right \\vert_{\\bf b} = -2 {\\bf X}' {\\bf y} + 2 {\\bf X}' {\\bf X} {\\bf b} = \\boldsymbol{0} \\end{align}\\)\n\n\n\n\nLSE for \\(\\boldsymbol{\\beta}\\):  \\(\\begin{align} \\boxed{{\\bf b} = \\arg \\min _{\\boldsymbol{\\beta}} (\\bf y - \\bf X \\boldsymbol{\\beta})'(\\bf y - \\bf X \\boldsymbol{\\beta}) = ({\\bf X}' {\\bf X}) ^{-1} {\\bf X}' \\bf y} \\end{align}\\)  provided that \\({\\bf X}' {\\bf X}\\) is full rank."
  },
  {
    "objectID": "slides/04-linear-reg.html#normal-equations",
    "href": "slides/04-linear-reg.html#normal-equations",
    "title": "Linear Regression üìà",
    "section": "Normal Equations",
    "text": "Normal Equations\n\\((\\bf X' \\bf X) \\bf b = \\bf X' \\bf y\\)"
  },
  {
    "objectID": "slides/04-linear-reg.html#hat-matrix",
    "href": "slides/04-linear-reg.html#hat-matrix",
    "title": "Linear Regression üìà",
    "section": "Hat Matrix",
    "text": "Hat Matrix\n\\[\\hat{\\bf y} = {\\bf X} {\\bf b} = {\\bf X} ({\\bf X}' {\\bf X}) ^{-1} \\bf X' \\bf y = \\bf H \\bf y\\]\n\nThe hat matrix \\({\\bf H}_{n \\times n} = {\\bf X} ({\\bf X}' {\\bf X}) ^{-1} \\bf X'\\)\nThe vector of residuals \\(e_i = y_i - \\hat{y}_i\\) is \\[\\bf e = \\bf y - \\hat{\\bf y} = \\bf y - \\bf X \\bf b = \\bf y -\\bf H \\bf y = (\\bf I - \\bf H) \\bf y\\]\n\n\n\n\nBoth \\(\\bf H\\) and \\(\\bf I-H\\) are symmetric and idempotent. They are projection matrices.\n\\(\\bf H\\) projects \\(\\bf y\\) to \\(\\hat{\\bf y}\\) on the \\((k+1)\\)-dimensional space spanned by columns of \\(\\bf X\\), or the column space of \\(\\bf X\\), \\(Col(\\bf X)\\).\n\\(\\bf I - H\\) projects \\(\\bf y\\) to \\(\\bf e\\) on the space perpendicular to \\(Col(\\bf X)\\), or \\(Col(\\bf X)^{\\bot}\\)."
  },
  {
    "objectID": "slides/04-linear-reg.html#geometrical-interpretation-of-least-squares",
    "href": "slides/04-linear-reg.html#geometrical-interpretation-of-least-squares",
    "title": "Linear Regression üìà",
    "section": "Geometrical Interpretation of Least Squares",
    "text": "Geometrical Interpretation of Least Squares\n\n\n\n\\(Col({\\bf X}) = \\{ {\\bf Xb}: {\\bf b} \\in {\\bf R}^{k+1} \\}\\), \\({\\bf y} \\notin Col({\\bf X})\\)\n\\(\\hat{{\\bf y}} = {\\bf Xb} = {\\bf H} {\\bf y} \\in Col({\\bf X})\\)\n\\(\\small {\\bf e} = ({\\bf y - \\hat{\\bf y}}) = ({\\bf y - X b}) = ({\\bf I} - {\\bf H}) {\\bf y} \\perp Col({\\bf X})\\)\n\\({\\bf X}'{\\bf e} = 0\\)\n\n\nSearching for \\(\\bf b\\) that minimizes \\(SS_{res}\\) is equivalent to locating the point \\({\\bf Xb} \\in Col({\\bf X})\\) \\((C)\\) that is as close to \\(\\bf y\\) \\((A)\\) as possible!\n\n\n\n\n\n\nhttps://commons.wikimedia.org/wiki/File:OLS_geometric_interpretation.svg\n\n\n\n\n\n\n\nMinimize the distance of \\(\\color{red}{A}\\) to \\(Col(\\bf X)\\): Find the point in \\(Col(\\bf X)\\) that is closest to \\(A\\). The distance is minimized when the point in the space is the foot of the line from \\(A\\) normal to the space. This is point \\(C\\)."
  },
  {
    "objectID": "slides/04-linear-reg.html#multivariate-gaussiannormal-distribution",
    "href": "slides/04-linear-reg.html#multivariate-gaussiannormal-distribution",
    "title": "Linear Regression üìà",
    "section": "Multivariate Gaussian/Normal Distribution",
    "text": "Multivariate Gaussian/Normal Distribution\n\n\n\\({\\bf y} \\sim N_n(\\boldsymbol \\mu, {\\bf \\Sigma})\\)\n\n\n\\(\\boldsymbol \\mu\\): \\(n \\times 1\\) mean vector\n\n\\({\\bf \\Sigma}\\): \\(n \\times n\\) covariance matrix\n\n\n\n\\(\\boldsymbol \\mu= (2, 1)'\\); \\({\\bf \\Sigma} = \\begin{pmatrix} 2 & -1 \\\\ -1 & 2\\end{pmatrix}\\)"
  },
  {
    "objectID": "slides/04-linear-reg.html#sampling-distribution-of-bf-b",
    "href": "slides/04-linear-reg.html#sampling-distribution-of-bf-b",
    "title": "Linear Regression üìà",
    "section": "Sampling Distribution of \\({\\bf b}\\)\n",
    "text": "Sampling Distribution of \\({\\bf b}\\)\n\n\n\\({\\bf y} \\sim N(\\boldsymbol \\mu, {\\bf \\Sigma})\\), and \\({\\bf Z = By + c}\\) with a constant matrix \\({\\bf B}\\) and vector \\(\\bf c\\), then \\[{\\bf Z} \\sim N({\\bf B\\boldsymbol \\mu}, {\\bf B \\Sigma B}')\\]\n\n\\({\\bf b} = ({\\bf X}' {\\bf X}) ^{-1} \\bf X' \\bf y\\)\n\\[\\textbf{b} \\sim N\\left(\\boldsymbol \\beta, \\sigma^2 {\\bf (X'X)}^{-1} \\right)\\] \\[E(\\textbf{b}) = E\\left[ {\\bf (X'X)}^{-1} {\\bf X}' {\\bf y}\\right] = \\boldsymbol \\beta\\] \\[\\mathrm{Var}(\\textbf{b}) = \\mathrm{Var}\\left[{\\bf (X'X)}^{-1} {\\bf X}' {\\bf y} \\right] = \\sigma^2 {\\bf (X'X)}^{-1}\\]\n\nThe unknown \\(\\sigma^2\\) is estimated by \\(\\hat{\\sigma}^2 = MS_{res} = \\frac{SS_{res}}{n - k - 1}\\).\n\\(\\textbf{b}\\) is Best Linear Unbiased Estimator (BLUE) (Gauss-Markov Theorem).\n\nThe standard error of \\(b_j\\) is \\({\\sqrt{s^2C_{jj}}}\\), where \\(C_{jj}\\) is the diagonal element of \\(({\\bf X'X})^{-1}\\) corresponding to \\(b_j\\)."
  },
  {
    "objectID": "slides/04-linear-reg.html#what-is-not-covered",
    "href": "slides/04-linear-reg.html#what-is-not-covered",
    "title": "Linear Regression üìà",
    "section": "What is Not Covered",
    "text": "What is Not Covered\n\nDetailed statistical inference\nRegression Diagnostics (Usual Data: Outliers, leverage points, influential points; Non-normality; Non-constant variance; Non-linearity)\nCategorical variables\nModel/Variable Selection\nCode for doing regression (lm() in R and linear_model.LinearRegression() in sklearn of Python)\nMaximum likelihood estimation\n\n\nWhere to learn these stuff?\n\nDr.¬†Yu‚Äôs MSSC 5780 slides https://math4780-f23.github.io/website/\nISL Ch 3, 6.1."
  },
  {
    "objectID": "slides/04-linear-reg.html#generalizations-of-the-linear-model",
    "href": "slides/04-linear-reg.html#generalizations-of-the-linear-model",
    "title": "Linear Regression üìà",
    "section": "Generalizations of the Linear Model",
    "text": "Generalizations of the Linear Model\n\nClassification: logistic regression, support vector machines\nNon-linearity: kernel smoothing, splines and generalized additive models, nearest neighbor methods.\nInteractions: Tree-based methods, bagging, random forests and boosting\nShrinkage and Regularization: Ridge regression and LASSO"
  },
  {
    "objectID": "slides/04-linear-reg.html#basic-concepts",
    "href": "slides/04-linear-reg.html#basic-concepts",
    "title": "Linear Regression üìà",
    "section": "Basic Concepts",
    "text": "Basic Concepts\n\nAlthough we have the closed form for \\({\\bf b} = ({\\bf X}' {\\bf X}) ^ {-1} \\bf X' \\bf y\\), we can solve for \\(\\bf b\\) numerically.\n\n\\[\\begin{align}\n\\underset{\\boldsymbol \\beta}{\\text{min}} \\quad \\ell(\\boldsymbol \\beta) = \\frac{1}{n} \\sum_i (y_i - \\beta_0 - x_i\\beta_1)^2 \\\\\n\\end{align}\\]\n\n\n\\(\\ell(\\boldsymbol \\beta)\\), in machine learning, is called the (squared) loss function.\n\n\n# generate data from a simple linear regression beta0 = 0.5, beta1 = 1\nset.seed(2024)\nn <- 1000\nx <- rnorm(n)\ny <- 0.5 + x + rnorm(n)\n\n\nStart with an initial value of \\(\\boldsymbol \\beta\\), say \\(\\widehat{\\boldsymbol \\beta} = (0.3, 1.5)\\), we compute the \\(SS_{res}\\)\n\n\n\\[\\begin{align}\n\\sum_i \\left( y_i - 0.3 - 1.5 x_i \\right)^2 \\\\\n\\end{align}\\]\nLet‚Äôs first generate a set of data. We have two parameters, an intercept \\(\\beta_= 0.5\\) and a slope \\(\\beta_1 = 1\\)."
  },
  {
    "objectID": "slides/04-linear-reg.html#basic-concepts-1",
    "href": "slides/04-linear-reg.html#basic-concepts-1",
    "title": "Linear Regression üìà",
    "section": "Basic Concepts",
    "text": "Basic Concepts\n\n# calculate the residual sum of squares for a grid of beta values\nrss <- function(b, trainx, trainy) mean((trainy - b[1] - trainx * b[2]) ^ 2)\nrss(b = c(0.3, 1.5), trainx = x, trainy = y)\n\n[1] 1.250039\n\n\n\nThe initial point \\((0.3, 1.5)\\) is not at the bottom of the surface.\n\n\n\n\n\n\n\nDoing this on all such Œ≤ values would allow us to create a surface of the RSS, as a function of the parameters. - Our goal is to minimize the \\(RR_{res}\\), knowing the corresponding \\(\\boldsymbol \\beta\\) values. Numerical optimization is a research field that investigates such problems and their properties."
  },
  {
    "objectID": "slides/04-linear-reg.html#optim",
    "href": "slides/04-linear-reg.html#optim",
    "title": "Linear Regression üìà",
    "section": "\noptim()1\n",
    "text": "optim()1\n\n\n(lm_optim <- optim(par = c(0.3, 1.5), fn = rss, trainx = x, trainy = y))\n\n$par\n[1] 0.5258467 0.9857827\n\n$value\n[1] 0.9449767\n\n$counts\nfunction gradient \n      55       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\nlm(y ~ x)$coef\n\n(Intercept)           x \n  0.5257906   0.9857546 \n\n\n\nThe par argument specifies an initial value. In this case, it is \\(\\beta_0 = \\beta_1 = 2\\).\nThe fn argument specifies the name of a function (rss in this case) that can calculate the objective function. This function may have multiple arguments. However, the first argument has to be the parameter(s) that is being optimized. In addition, the parameters need to be supplied to the function as a vector, but not matrix, list or other formats.\nThe arguments trainx = x, trainy = y specifies any additional arguments that the objective function fn (rss) needs. It behaves the same as if you are supplying this to the function rss it self.\nscipy.optimize for Python."
  },
  {
    "objectID": "slides/04-linear-reg.html#basic-principles",
    "href": "slides/04-linear-reg.html#basic-principles",
    "title": "Linear Regression üìà",
    "section": "Basic Principles",
    "text": "Basic Principles\nFor a general function \\(f(\\mathbf{x})\\) to be minimized with respect to (w.r.t.) \\(\\mathbf{x}\\in \\mathbf{R}^{p}\\), we have\n\n\nFirst-Order Necessary Condition:\n\n\nIf \\(f\\) is continuously differentiable in an open neighborhood of local minimum \\(\\mathbf{x}^\\ast\\), then \\(\\nabla f(\\mathbf{x}^\\ast) = \\mathbf{0}\\)."
  },
  {
    "objectID": "slides/04-linear-reg.html#taylor-expansion",
    "href": "slides/04-linear-reg.html#taylor-expansion",
    "title": "Linear Regression üìà",
    "section": "Taylor Expansion",
    "text": "Taylor Expansion\n\\[f(\\mathbf{x}^\\text{new}) \\approx f(\\mathbf{x}) + \\nabla f(\\mathbf{x}) (\\mathbf{x}^\\text{new} - \\mathbf{x})\\]\n\nWhether \\(\\nabla f(\\mathbf{x})\\) is positive or negative, we can always find a new point \\(\\mathbf{x}^\\text{new}\\) that makes \\(\\nabla f(\\mathbf{x}) (\\mathbf{x}^\\text{new} - \\mathbf{x})\\) less than 0, so that \\(\\mathbf{x}^\\text{new}\\) has a smaller functional value than \\(\\mathbf{x}\\).\n\n\n\n\n\\(\\nabla f(\\mathbf{x}^\\ast) = \\mathbf{0}\\) is only a necessary condition, not a sufficient condition.\n\\(x = 1\\) is a local minimizer, not a global one.\n\nOn the left hand side, we have a convex function, which looks like a bowl. The intuition is that, if \\(f(\\mathbf{x})\\) is a function that is smooth enough, and \\(\\mathbf{x}\\) is a point with \\(\\nabla f(\\mathbf{x}^\\ast) \\neq 0\\), then by the Taylor expansion, we have, for any new point \\(\\mathbf{x}^{new}\\) in the neighborhood of \\(\\mathbf{x}\\), we can approximate its function value - Since we only checked if the slope if ‚Äúflat‚Äù but didn‚Äôt care if its facing upward or downward, our condition cannot tell the difference."
  },
  {
    "objectID": "slides/04-linear-reg.html#second-order-property",
    "href": "slides/04-linear-reg.html#second-order-property",
    "title": "Linear Regression üìà",
    "section": "Second-order Property",
    "text": "Second-order Property\nSecond-order Sufficient Condition:\n\n\\(f\\) is twice continuously differentiable in an open neighborhood of \\(\\mathbf{x}^\\ast\\). If \\(\\nabla f(\\mathbf{x}^\\ast) = \\mathbf{0}\\) and \\(\\nabla^2 f(\\mathbf{x}^\\ast)\\) , the Hessian matrix, is positive definite, i.e., \\[\n\\nabla^2 f(\\mathbf{x}) = \\left(\\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_i \\partial x_j}\\right) = \\mathbf{H}(\\mathbf{x}) \\succeq 0\n\\] then \\(\\mathbf{x}^\\ast\\) is a strict local minimizer of \\(f\\).\n\n\n\\[\\begin{align}\n\\text{Left:} \\qquad \\nabla^2 f_1(x) &= 2 \\\\\n\\text{Right:} \\qquad \\nabla^2 f_2(x) &= 12x^2 + 12 x - 10\\\\\n\\end{align}\\]\n\nFor \\(f_1\\), \\(\\mathbf{H}(\\mathbf{x}) \\succeq 0\\), and the solution is a minimizer.\nFor \\(f_2\\), \\(\\nabla^2 f_2(-2.5) = 35\\), \\(\\nabla^2 f_2(0) = -10\\) and \\(\\nabla^2 f_2(1) = 14\\). So \\(-2.5\\) and \\(1\\) are local minimizers and \\(0\\) is a local maximizer.\n\n\n\n\\(\\mathbf{H}(\\mathbf{x})\\) is called the Hessian matrix, which will be frequently used in second-order methods. We can easily check this property for our examples: These conditions are sufficient, but again, they only discuss local properties, not global properties."
  },
  {
    "objectID": "slides/04-linear-reg.html#optimization-algorithm",
    "href": "slides/04-linear-reg.html#optimization-algorithm",
    "title": "Linear Regression üìà",
    "section": "Optimization Algorithm",
    "text": "Optimization Algorithm\n\n 1. Start with \\(\\mathbf{x}^{(0)}\\)\n\nFor \\(i = 1, 2, \\dots\\) until convergence, find \\(\\mathbf{x}^{(i)}\\) s.t. \\(f(\\mathbf{x}^{(i)}) < f(\\mathbf{x}^{(i-1)})\\)\n\n\n\n\nStopping criterion:\n\nGradient of the objective function: \\(\\lVert \\nabla f(\\mathbf{x}^{(i)}) \\rVert < \\epsilon\\)\n\n(Relative) change of distance: \\(\\lVert \\mathbf{x}^{(i)} - \\mathbf{x}^{(i-1)} \\rVert / \\lVert \\mathbf{x}^{(i-1)}\\rVert< \\epsilon\\) or \\(\\lVert \\mathbf{x}^{(i)} - \\mathbf{x}^{(i-1)} \\rVert < \\epsilon\\)\n\n(Relative) change of functional value: \\(| f(\\mathbf{x}^{(i)}) - f(\\mathbf{x}^{(i-1)})| / |f(\\mathbf{x}^{(i)})| < \\epsilon\\) or \\(| f(\\mathbf{x}^{(i)}) - f(\\mathbf{x}^{(i-1)})| < \\epsilon\\)\n\nStop at a pre-specified number of iterations\n\n\n\n\noptim(par, fn, gr = NULL, ...,\n      method = c(\"Nelder-Mead\", \"BFGS\", \"CG\", \"L-BFGS-B\", \"SANN\",\n                 \"Brent\"),\n      lower = -Inf, upper = Inf,\n      control = list(), hessian = FALSE)\nMost optimization algorithms follow the same idea: starting from a point x(0) (which is usually specified by the user) and move to a new point x(1) that improves the objective function value. Repeatedly performing this to get a sequence of points x(0),x(1),x(2),x(3),‚Ä¶ until the certain stopping criterion is reached. Most algorithms differ in terms of how to move from the current point x(k) to the next, better target point x(k+1). This may depend on the smoothness or structure of f, constrains on the domain, computational complexity, memory limitation, and many others."
  },
  {
    "objectID": "slides/04-linear-reg.html#bfgs-demo",
    "href": "slides/04-linear-reg.html#bfgs-demo",
    "title": "Linear Regression üìà",
    "section": "\nBFGS Demo",
    "text": "BFGS Demo\n\n\n\nf1 <- function(x) x ^ 2\nf2 <- function(x) {\n    x ^ 4 + 2 * x ^ 3 - 5 * x ^ 2\n}\noptim(par = 3, fn = f1, method = \"BFGS\")\n\n$par\n[1] -8.384004e-16\n\n$value\n[1] 1.75742e-29\n\n$counts\nfunction gradient \n       8        3 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\n\noptim(par = 10, fn = f2, method = \"BFGS\")\n\n$par\n[1] 0.9999996\n\n$value\n[1] -2\n\n$counts\nfunction gradient \n      37       12 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\noptim(par = 3, fn = f2, method = \"BFGS\")$par\n\n[1] -2.5"
  },
  {
    "objectID": "slides/04-linear-reg.html#second-order-newtons-method",
    "href": "slides/04-linear-reg.html#second-order-newtons-method",
    "title": "Linear Regression üìà",
    "section": "Second-order Newton‚Äôs Method\n",
    "text": "Second-order Newton‚Äôs Method\n\n\nSecond order Taylor expansion at a current point \\(\\mathbf{x}\\):\n\n\\[f(\\mathbf{x}^\\ast) \\approx f(\\mathbf{x}) + \\nabla f(\\mathbf{x})' (\\mathbf{x}^\\ast - \\mathbf{x}) + \\frac{1}{2} (\\mathbf{x}^\\ast - \\mathbf{x})' \\mathbf{H}(\\mathbf{x}) (\\mathbf{x}^\\ast - \\mathbf{x})\\]\n\nTake derivative w.r.t \\(\\mathbf{x}^\\ast\\) on both sides: \\[0 = \\nabla f(\\mathbf{x}^\\ast) = 0 + \\nabla f(\\mathbf{x}) + \\mathbf{H}(\\mathbf{x}) (\\mathbf{x}^\\ast - \\mathbf{x})\\] \\[\\boxed{\\mathbf{x}^{(i+1)} = \\mathbf{x}^{(i)} -  \\mathbf{H}(\\mathbf{x}^{(i)})^{-1} \\nabla f(\\mathbf{x}^{(i)})}\\]\nFor numerical stability, introduce a step size \\(\\delta \\in (0, 1)\\): \\[\\mathbf{x}^{(i+1)} = \\mathbf{x}^{(i)} -  {\\color{red}{\\delta}} \\, \\mathbf{H}(\\mathbf{x}^{(i)})^{-1} \\nabla f(\\mathbf{x}^{(i)})\\]\n\nIf \\(\\mathbf{H}\\) is difficult to compute, we may use some matrix to substitute it. For example, if we simplify use the identity matrix \\(\\mathbf{I}\\), then this reduces to a first-order method to be introduced later. However, if we can get a good approximation, we can still solve this linear system and get to a better point. Then the question is how to obtain such matrix in a computationally inexpensive way. The Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShanno (BFSG) algorithm is such an approach by iteratively updating its (inverse) estimation. For details, please see the SMLR book. We have already used the BFGS method previously in the optim() example."
  },
  {
    "objectID": "slides/04-linear-reg.html#first-order-gradient-descent",
    "href": "slides/04-linear-reg.html#first-order-gradient-descent",
    "title": "Linear Regression üìà",
    "section": "First-Order Gradient Descent\n",
    "text": "First-Order Gradient Descent\n\n\n\n\n\n\n\nNote\n\n\n\nWhen \\(\\mathbf{H}\\) or \\(\\mathbf{H}^{-1}\\) is difficult to compute\n\nGet an approximate one in a computationally inexpensive way. The BFGS algorithm is such an approach by iteratively updating its (inverse) estimation.\nUse first-order methods\n\n\n\n\n\n\nWhen using \\(\\mathbf{H}= \\mathbf{I}\\), we update \\[\\mathbf{x}^{(i+1)} = \\mathbf{x}^{(i)} - \\delta \\nabla f(\\mathbf{x}^{(i)}).\\]\n\nIt is crucial to figure out the step size \\(\\delta\\).\n\nA too large \\(\\delta\\) may not converge.\nA too small \\(\\delta\\) takes too many iterations.\n\n\n\nAlternatively, line search could be used."
  },
  {
    "objectID": "slides/04-linear-reg.html#demo-ss_res-contour",
    "href": "slides/04-linear-reg.html#demo-ss_res-contour",
    "title": "Linear Regression üìà",
    "section": "\nDemo \\(SS_{res}\\) Contour",
    "text": "Demo \\(SS_{res}\\) Contour\nThe objective function is \\(\\ell(\\boldsymbol \\beta) = \\frac{1}{2n}||\\mathbf{y} - \\mathbf{X} \\boldsymbol \\beta ||^2\\) with solution \\({\\bf b} = \\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1} \\mathbf{X}'\\mathbf{y}\\).\n\nCodeset.seed(2024)\nn <- 200\n\n# create some data with linear model\nX <- MASS::mvrnorm(n, c(0, 0), matrix(c(1, 0.7, 0.7, 1), 2, 2))\ny <- rnorm(n, mean = 2 * X[, 1] + X[, 2])\n  \nbeta1 <- seq(-1, 4, 0.005)\nbeta2 <- seq(-1, 4, 0.005)\nallbeta <- data.matrix(expand.grid(beta1, beta2))\nrss <- matrix(apply(allbeta, 1, \n                    function(b, X, y) sum((y - X %*% b) ^ 2), X, y),\n              length(beta1), length(beta2))\n  \n# quantile levels for drawing contour\nquanlvl = c(0.01, 0.025, 0.05, 0.2, 0.5, 0.75)\n  \n# plot the contour\ncontour(beta1, beta2, rss, levels = quantile(rss, quanlvl), las = 1)\nbox()\n  \n# the truth\nb <- solve(t(X) %*% X) %*% t(X) %*% y\npoints(b[1], b[2], pch = 19, col = \"blue\", cex = 2)"
  },
  {
    "objectID": "slides/04-linear-reg.html#demo-gradient-descent",
    "href": "slides/04-linear-reg.html#demo-gradient-descent",
    "title": "Linear Regression üìà",
    "section": "\nDemo Gradient Descent",
    "text": "Demo Gradient Descent\n\\[\n\\begin{align}\n\\frac{\\partial \\ell(\\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta} = -\\frac{1}{n} \\sum_{i=1}^n (y_i - x_i' \\boldsymbol \\beta) x_i.\n\\end{align}\n\\] First set an initial beta value, say \\(\\boldsymbol \\beta = \\mathbf{0}\\) for all entries, then proceed with the update\n\\[\\begin{align}\n\\boldsymbol \\beta^\\text{new} =& \\boldsymbol \\beta^\\text{old} - \\delta \\frac{\\partial \\ell(\\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta}\\\\\n=&\\boldsymbol \\beta^\\text{old} + \\delta \\frac{1}{n} \\sum_{i=1}^n (y_i - x_i' \\boldsymbol \\beta) x_i.\\\\\n\\end{align}\\]"
  },
  {
    "objectID": "slides/04-linear-reg.html#demo-gradient-descent-1",
    "href": "slides/04-linear-reg.html#demo-gradient-descent-1",
    "title": "Linear Regression üìà",
    "section": "\nDemo Gradient Descent",
    "text": "Demo Gradient Descent\n\nSet \\(\\boldsymbol{\\beta}^{(0)} = \\mathbf{0}\\), \\(\\delta = 0.2\\)"
  },
  {
    "objectID": "slides/04-linear-reg.html#demo-effect-of-delta",
    "href": "slides/04-linear-reg.html#demo-effect-of-delta",
    "title": "Linear Regression üìà",
    "section": "\nDemo Effect of \\(\\delta\\)\n",
    "text": "Demo Effect of \\(\\delta\\)\n\n\nThe descent path is very smooth because we choose a very small step size. However, if the step size is too large, we may observe unstable results or even unable to converge. For example, if We set \\(\\delta = 1\\) or \\(\\delta = 1.5\\)."
  },
  {
    "objectID": "slides/04-linear-reg.html#revisit-optim",
    "href": "slides/04-linear-reg.html#revisit-optim",
    "title": "Linear Regression üìà",
    "section": "Revisit optim()\n",
    "text": "Revisit optim()\n\noptim(par, fn, gr = NULL, ...,\n      method = c(\"Nelder-Mead\", \"BFGS\", \"CG\", \"L-BFGS-B\", \"SANN\",\n                 \"Brent\"),\n      lower = -Inf, upper = Inf,\n      control = list(), hessian = FALSE)\n\nIf gradient is known, we provide this information in the argument gr = to avoid numerically approximating the gradient.\n\nfaster convergence\nmore precise\n\n\n\n\n\n\nWhen we only supply the optim() function the objective function definition fn =, it will internally numerically approximate the gradient. Sometimes this is not preferred due to computational reasons. And when you do have the theoretical gradient, it is likely to be more accurate than numerically approximated values. Hence, as long as the gradient itself does not cost too much compute, we may take advantage of it. We could supply the optim() function using the gradient gr =. This is the example we used previously:\n\n\n\nmssc6250-s24.github.io/website"
  },
  {
    "objectID": "slides/01-syllabus.html#my-journey",
    "href": "slides/01-syllabus.html#my-journey",
    "title": "Welcome Aboard üôå",
    "section": "My Journey",
    "text": "My Journey\n\nAssistant Professor (2020/08 - )\n\n\n\n\n\n\n\n\n\n\nPostdoctoral Fellow\n\n\n\n\n\n\n\n\n\n\nPhD in Statistics\n\n\n\n\n\n\n\n\n\n\nMA in Economics/PhD program in Statistics"
  },
  {
    "objectID": "slides/01-syllabus.html#my-research",
    "href": "slides/01-syllabus.html#my-research",
    "title": "Welcome Aboard üôå",
    "section": "My Research",
    "text": "My Research\n\nBayesian spatio-temporal modeling and machine learning algorithms in neuroimaging\nBayesian Deep Learning for image classification\nEfficient MCMC for high dimensional regression\nGame-based learning for STEM and data science education\n\n\n\nfMRI\n\n\n\n\n\n\n\n\n\nEEG"
  },
  {
    "objectID": "slides/01-syllabus.html#how-to-reach-me",
    "href": "slides/01-syllabus.html#how-to-reach-me",
    "title": "Welcome Aboard üôå",
    "section": "How to Reach Me",
    "text": "How to Reach Me\n\nOffice hours TuTh 4:50 - 5:50 PM and W 12 - 1 PM in Cudahy Hall 353.\nüìß cheng-han.yu@marquette.edu\n\nAnswer your question within 24 hours.\nExpect a reply on Monday if shoot me a message on weekends.\nStart your subject line with [mssc6250] followed by a clear description of your question.\n\n\n\n\n\nI will NOT reply your e-mail if ‚Ä¶ Check the email policy in the syllabus!"
  },
  {
    "objectID": "slides/01-syllabus.html#prerequisites",
    "href": "slides/01-syllabus.html#prerequisites",
    "title": "Welcome Aboard üôå",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nOn bulletin: MATH 4720 (Intro to Statistics), MATH 3100 (Linear Algebra) and MATH 4780 (Regression Analysis)\nProgramming experience (Who does machine learning without coding?)\n\n\n\nHaving taken MATH 5700 (Probability) and MATH 5710 (Stats Inference) or other math and statistics courses (Stats Computing, etc) is recommended."
  },
  {
    "objectID": "slides/01-syllabus.html#textbook",
    "href": "slides/01-syllabus.html#textbook",
    "title": "Welcome Aboard üôå",
    "section": "Textbook",
    "text": "Textbook\n\n\n\n\n(ISL) An Introduction to Statistical Learning, 2nd edition, by James et al.¬†Publisher: Springer. \n\nDiscuss all chapters except Chapter 11 (survival analysis) and 13 (multiple testing).\nR and Python code\n\nIn the Preface,\n\n‚Ä¶ for advanced undergraduates or master‚Äôs students in Statistics or related quantitative fields,\n\n\n‚Ä¶ concentrate more on the applications of the methods and less on the mathematical details."
  },
  {
    "objectID": "slides/01-syllabus.html#optional-references-1",
    "href": "slides/01-syllabus.html#optional-references-1",
    "title": "Welcome Aboard üôå",
    "section": "Optional References",
    "text": "Optional References\n\n\n\n(PMLI) Probabilistic Machine Learning: An Introduction, by Kevin Murphy. Publisher: MIT Press.\nSelf-contained with lots of mathematics foundations\nPython code"
  },
  {
    "objectID": "slides/01-syllabus.html#optional-references-2",
    "href": "slides/01-syllabus.html#optional-references-2",
    "title": "Welcome Aboard üôå",
    "section": "Optional References",
    "text": "Optional References\n\n\n\n(PMLA) Probabilistic Machine Learning: Advanced Topics, by Kevin Murphy. Publisher: MIT Press.\nPhD level\nProbabilistic or distributional-based"
  },
  {
    "objectID": "slides/01-syllabus.html#optional-references-3",
    "href": "slides/01-syllabus.html#optional-references-3",
    "title": "Welcome Aboard üôå",
    "section": "Optional References",
    "text": "Optional References\n\n\n\n(ESL) The Elements of Statistical Learning, 2nd edition, by Hastie et. al.¬†Publisher: Springer.\nFor PhD students or researchers in mathematical sciences\nFrequentist-based"
  },
  {
    "objectID": "slides/01-syllabus.html#course-website---httpsmssc6250-s24.github.iowebsite",
    "href": "slides/01-syllabus.html#course-website---httpsmssc6250-s24.github.iowebsite",
    "title": "Welcome Aboard üôå",
    "section": "Course Website - https://mssc6250-s24.github.io/website/\n",
    "text": "Course Website - https://mssc6250-s24.github.io/website/"
  },
  {
    "objectID": "slides/01-syllabus.html#learning-management-system-d2l",
    "href": "slides/01-syllabus.html#learning-management-system-d2l",
    "title": "Welcome Aboard üôå",
    "section": "Learning Management System (D2L)",
    "text": "Learning Management System (D2L)\n\n\nAny new announcement will be posted in News in Course Home.  \nSubmit your homework Assessments > Dropbox.\nGo to https://www.marquette.edu/remote-learning/d2l.php to learn more about D2L."
  },
  {
    "objectID": "slides/01-syllabus.html#grading-policy",
    "href": "slides/01-syllabus.html#grading-policy",
    "title": "Welcome Aboard üôå",
    "section": "Grading Policy ‚ú®",
    "text": "Grading Policy ‚ú®\n\nThe grade is earned out of 1000 total points distributed as follows:\n\nHomework: 500 pts\nClass activity: 200 pts\nFinal project presentation and/or written report: 300 pts\n\n\n\n‚ùå No extra credit projects/homework/exam to compensate for a poor grade."
  },
  {
    "objectID": "slides/01-syllabus.html#grade-percentage-conversion",
    "href": "slides/01-syllabus.html#grade-percentage-conversion",
    "title": "Welcome Aboard üôå",
    "section": "Grade-Percentage Conversion",
    "text": "Grade-Percentage Conversion\n\nYour final grade is based on your percentage of pts earned out of 1000 pts.\n\n[x, y) means greater than or equal to x and less than y.\n\n\n\n\n\n\n \n Grade \n    Percentage \n  \n\n\n A \n    [94, 100] \n  \n\n A- \n    [90, 94) \n  \n\n B+ \n    [87, 90) \n  \n\n B \n    [83, 87) \n  \n\n B- \n    [80, 83) \n  \n\n C+ \n    [77, 80) \n  \n\n C \n    [70, 77) \n  \n\n F \n    [0, 70)"
  },
  {
    "objectID": "slides/01-syllabus.html#homework-500-pts",
    "href": "slides/01-syllabus.html#homework-500-pts",
    "title": "Welcome Aboard üôå",
    "section": "Homework (500 pts)",
    "text": "Homework (500 pts)\n\n\nAssessments > Dropbox and upload your homework in PDF format.\n‚ùå No make-up homework.\nDue Friday 11:59 PM  (Hard deadline and no late submission).\nYou have at least one week to finish your homework."
  },
  {
    "objectID": "slides/01-syllabus.html#in-class-activity-200-pts",
    "href": "slides/01-syllabus.html#in-class-activity-200-pts",
    "title": "Welcome Aboard üôå",
    "section": "In-Class Activity (200 pts)",
    "text": "In-Class Activity (200 pts)\n\nThere will be 3 to 4 in-class activities.\nStudents will learn from each other by presenting and discussing the assigned topics.\nMore details about the in-class activities will be released later."
  },
  {
    "objectID": "slides/01-syllabus.html#final-project-300-pts",
    "href": "slides/01-syllabus.html#final-project-300-pts",
    "title": "Welcome Aboard üôå",
    "section": "Final Project (300 pts)",
    "text": "Final Project (300 pts)\n\nThe final project includes two parts: written report and oral presentation?.\nYou have to participate (in-person) in the final presentation in order to pass the course.\nMore details about the project will be released later."
  },
  {
    "objectID": "slides/01-syllabus.html#which-programming-language",
    "href": "slides/01-syllabus.html#which-programming-language",
    "title": "Welcome Aboard üôå",
    "section": "Which Programming Language?",
    "text": "Which Programming Language?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse any language you want!"
  },
  {
    "objectID": "slides/01-syllabus.html#academic-integrity",
    "href": "slides/01-syllabus.html#academic-integrity",
    "title": "Welcome Aboard üôå",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nThis course expects all students to follow University and College statements on academic integrity.\n\n\nHonor Pledge and Honor Code: I recognize the importance of personal integrity in all aspects of life and work. I commit myself to truthfulness, honor, and responsibility, by which I earn the respect of others. I support the development of good character, and commit myself to uphold the highest standards of academic integrity as an important aspect of personal integrity. My commitment obliges me to conduct myself according to the Marquette University Honor Code."
  },
  {
    "objectID": "slides/01-syllabus.html#attendance-and-covid-19",
    "href": "slides/01-syllabus.html#attendance-and-covid-19",
    "title": "Welcome Aboard üôå",
    "section": "Attendance and COVID-19",
    "text": "Attendance and COVID-19\n\nIt is your responsibility as a Marquette University student to protect the health and safety of our community in this course.\nVisit What to do if you are exposed to COVID-19 or test positive website for university guidelines on the best course of action.\nVisit guidance on Spring 2024 Class attendance, withdrawal, and grading\n\nStudents are responsible for contacting instructors prior to the missed class session to indicate absence and the need to make up classwork/assignments.\n\nStudents requesting make up classwork/assignments are required to provide the COVID Cheq ‚Äústop sign‚Äù to confirm inability to attend class.\n\n\n\n\n\n\nmssc6250-s24.github.io/website"
  },
  {
    "objectID": "slides/08-bayes.html#thinking-like-a-bayesian",
    "href": "slides/08-bayes.html#thinking-like-a-bayesian",
    "title": "Bayesian Linear Regression \n",
    "section": "Thinking like a Bayesian",
    "text": "Thinking like a Bayesian\n\n\nWhen flipping a fair coin, we say that ‚Äúthe probability of flipping Heads is 0.5.‚Äù How do you interpret this probability?\n\nIf I flip this coin over and over, roughly 50% will be Heads.\nHeads and Tails are equally plausible.\nBoth a. and b. make sense.\n\n\n\n\n\n\nAn election is coming up and a pollster claims that candidate A has a 0.9 probability of winning. How do you interpret this probability?\n\nIf we observe the election over and over, candidate A will win roughly 90% of the time.\nCandidate A is much more likely to win than to lose.\nThe pollster‚Äôs calculation is wrong. Candidate A will either win or lose, thus their probability of winning can only be 0 or 1.\n\n\n\n\n\n\n\n1. a = 1 pt, b = 3 pts, c = 2 pts\n\n2. a = 1 pt, b = 3 pts, c = 1 pt"
  },
  {
    "objectID": "slides/08-bayes.html#thinking-like-a-bayesian-1",
    "href": "slides/08-bayes.html#thinking-like-a-bayesian-1",
    "title": "Bayesian Linear Regression \n",
    "section": "Thinking like a Bayesian",
    "text": "Thinking like a Bayesian\n\n\nTwo claims.\n(1) Ben claims he can predict the coin flip outcome. To test his claim, you flip a fair coin 5 times and he correctly predicts all.\n(2) Emily claims she can distinguish natural and artificial sweeteners. To test her claim, you give her 5 samples and she correctly identifies each.\nIn light of these experiments, what do you conclude?\n\nYou‚Äôre more confident in Emily‚Äôs claim than Ben‚Äôs claim.\nThe evidence supporting Ben‚Äôs claim is just as strong as the evidence supporting Emily‚Äôs claim.\n\n\n\n\n\n\nSuppose that during a doctor‚Äôs visit, you tested positive for COVID. If you only get to ask the doctor one question, which would it be?\n\nWhat‚Äôs the chance that I actually have COVID?\nIf in fact I don‚Äôt have COVID, what‚Äôs the chance that I would‚Äôve gotten this positive test result?\n\n\n\n\n\n\n\n3. a = 3 pts, b = 1 pt\n\n4. a = 3 pts, b = 1 pt\n\nTotals from 4‚Äì5 indicate that your current thinking is fairly frequentist, whereas totals from 9‚Äì12 indicate alignment with the Bayesian philosophy. In between these extremes, totals from 6‚Äì8 indicate that you see strengths in both philosophies."
  },
  {
    "objectID": "slides/08-bayes.html#frequentist-or-bayesian",
    "href": "slides/08-bayes.html#frequentist-or-bayesian",
    "title": "Bayesian Linear Regression \n",
    "section": "Frequentist or Bayesian?",
    "text": "Frequentist or Bayesian?\n\n\n\nTotals 4-5: your thinking is frequentist\n\n\n\n\n\n\n\n\n\n\n\n\nTotals 9-12: your thinking is Bayesian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTotals 6-8: you see strengths in both philosophies"
  },
  {
    "objectID": "slides/08-bayes.html#the-meaning-of-probability-relative-frequency",
    "href": "slides/08-bayes.html#the-meaning-of-probability-relative-frequency",
    "title": "Bayesian Linear Regression \n",
    "section": "The Meaning of Probability: Relative Frequency\n",
    "text": "The Meaning of Probability: Relative Frequency\n\n\nThe frequentist philosophy interprets probability as the long-run relative frequency of a repeatable experiment.\n\n\n\nThe probability that some outcome of a process will be obtained is defined as the relative frequency with which that outcome would be obtained if the process were repeated a large number of times independently under similar conditions.\n\n\n\n\n\n\n\n\n      Frequency Relative Frequency\nHeads         4                0.4\nTails         6                0.6\nTotal        10                1.0\n---------------------\n      Frequency Relative Frequency\nHeads       512              0.512\nTails       488              0.488\nTotal      1000              1.000\n---------------------\n\n\n\nIf we repeat tossing the coin 10 times, the probability of obtaining heads is 40%.\nIf 1000 times, the probability is 51.2%.\n\n\n\n\n\n\n\n\n\n\n\n\nsource: usplash"
  },
  {
    "objectID": "slides/08-bayes.html#issues-of-relative-frequency",
    "href": "slides/08-bayes.html#issues-of-relative-frequency",
    "title": "Bayesian Linear Regression \n",
    "section": "Issues of Relative Frequency\n",
    "text": "Issues of Relative Frequency\n\n\nüòï How large of a number is large enough?\n\n\n\nüòï Meaning of ‚Äúunder similar conditions‚Äù\n\n\n\n\nüòï The relative frequency is reliable under identical conditions?\n\n\n\n\nüëâ We only obtain an approximation instead of exact value.\n\n\n\n\nüòÇ How do you compute the probability that Chicago Cubs wins the World Series next year?"
  },
  {
    "objectID": "slides/08-bayes.html#the-meaning-of-probability-relative-plausibility",
    "href": "slides/08-bayes.html#the-meaning-of-probability-relative-plausibility",
    "title": "Bayesian Linear Regression \n",
    "section": "The Meaning of Probability: Relative Plausibility\n",
    "text": "The Meaning of Probability: Relative Plausibility\n\n\nIn the Bayesian philosophy, a probability measures the relative plausibility of an event.\n\n\n\n\n\nFor the statement ‚Äúcandidate A has a 0.9 probability of winning‚Äù\n\nA frequentist might\n\nsay the conclusion is wrong\nweirdly say in long-run hypothetical repetitions of the election, candidate A would win roughly 90% of the time.\n\n\nA Bayesian would say based on analysis the candidate A is 9 times more likely to win than to lose.\n\n\n\n\n\n\n\n\n\nSource: https://twitter.com/nytgraphics/status/796195155158171648/photo/1\n\n\n\n\n\n\n\n\nFor the statement ‚Äúthe probability of flipping Heads is 0.5‚Äù\n\nA frequentist would conclude that if we flip the coin over and over, roughly 1/2 of these flips will be Heads.\nA Bayesian would conclude that Heads and Tails are equally likely."
  },
  {
    "objectID": "slides/08-bayes.html#everybody-changes-their-mind",
    "href": "slides/08-bayes.html#everybody-changes-their-mind",
    "title": "Bayesian Linear Regression \n",
    "section": "Everybody Changes Their Mind",
    "text": "Everybody Changes Their Mind\n\nWe continuously update our knowledge about the world as we accumulate lived experiences, or collect data.\n\n\nFig 1.1 of https://www.bayesrulesbook.com/. The figures not being sourced come from this book too."
  },
  {
    "objectID": "slides/08-bayes.html#bayesian-knowledge-building-process",
    "href": "slides/08-bayes.html#bayesian-knowledge-building-process",
    "title": "Bayesian Linear Regression \n",
    "section": "Bayesian Knowledge-building Process",
    "text": "Bayesian Knowledge-building Process\n\nUsing data and prior beliefs to update our knowledge (Posterior), and repeating."
  },
  {
    "objectID": "slides/08-bayes.html#frequentist-relys-on-limited-data-only",
    "href": "slides/08-bayes.html#frequentist-relys-on-limited-data-only",
    "title": "Bayesian Linear Regression \n",
    "section": "Frequentist Relys on (Limited) Data Only",
    "text": "Frequentist Relys on (Limited) Data Only\n\nIn Question 3, in a frequentist analysis, ‚Äú5 out of 5‚Äù is ‚Äú5 out of 5‚Äù no matter if it‚Äôs in the context of Ben‚Äôs coins or Emily‚Äôs sweeteners.\n\n\nEqually confident conclusions that Ben can predict coin flips and Emily can distinguish between natural and artificial sweeteners.\n\n\n\n\n\n\nBut do you really believe Ben‚Äôs claim 100%? ü§î üòï\n\n\n\n\nIn fact, we judge their claim before evidence are collected, don‚Äôt we? ü§î\n\n\n\n\nFrequentist throws out all prior knowledge in favor of a mere 5 data points."
  },
  {
    "objectID": "slides/08-bayes.html#the-bayesian-balancing",
    "href": "slides/08-bayes.html#the-bayesian-balancing",
    "title": "Bayesian Linear Regression \n",
    "section": "The Bayesian Balancing",
    "text": "The Bayesian Balancing\n\nBen probably overstates his ability but Emily‚Äôs claim is reasonable, right?\n\n\n\nBayesian analyses balance and weight our prior experience/knowledge/belief and new data/evidence to judge a claim or make a conclusion.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe are not stubborn! If Ben had correctly predicted the outcome of 1 million coin flips, the strength of this data would far surpass that of our prior, leading to a posterior conclusion that perhaps Ben is psychic!"
  },
  {
    "objectID": "slides/08-bayes.html#asking-different-questions",
    "href": "slides/08-bayes.html#asking-different-questions",
    "title": "Bayesian Linear Regression \n",
    "section": "Asking Different Questions",
    "text": "Asking Different Questions\n\n\nIn Question 4,\n\nA Bayesian answer (a) what‚Äôs the chance that I actually have COVID?\n\nA frequentist answer (b) if in fact I do not have COVID, what‚Äôs the chance that I would‚Äôve gotten this positive test result?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(H_0\\): Do not have COVID vs.¬†\\(H_1\\): Have COVID\nA frequestist assesses the uncertainty of the observed data in light of an assumed hypothesis \\(P(Data \\mid H_0)\\)\nA Bayesian assesses the uncertainty of the hypothesis in light of the observed data \\(P(H_0 \\mid Data)\\)\n\n\n\n\n\n\n\n\n\n\n\n\nTest Positive\nTest Negative\nTotal\n\n\n\ndisease\n3\n1\n4\n\n\nno disease\n9\n87\n96\n\n\nTotal\n12\n88\n100\n\n\n\na Bayesian analysis would ask: Given my positive test result, what‚Äôs the chance that I actually have the disease? Since only 3 of the 12 people that tested positive have the disease (Table 1.1), there‚Äôs only a 25% chance that you have the disease. Thus, when we take into account the disease‚Äôs rarity and the relatively high false positive rate, it‚Äôs relatively unlikely that you actually have the disease. What a relief.\nsince disease status isn‚Äôt repeatable, the probability you have the disease is either 1 or 0 ‚Äì you have it or you don‚Äôt. To the contrary, medical testing (and data collection in general) is repeatable. You can get tested for the disease over and over and over. Thus, a frequentist analysis would ask: If I don‚Äôt actually have the disease, what‚Äôs the chance that I would‚Äôve tested positive? Since only 9 of the 96 people without the disease tested positive, there‚Äôs a roughly 10% (9/96) chance that you would‚Äôve tested positive even if you didn‚Äôt have the disease."
  },
  {
    "objectID": "slides/08-bayes.html#fake-news",
    "href": "slides/08-bayes.html#fake-news",
    "title": "Bayesian Linear Regression \n",
    "section": "Fake News",
    "text": "Fake News\n\n\n\nTell if an incoming article is fake.\n\n\n\n\n\n\nPrior info: Our prior information suggested that 40% of the articles are fake,\n\n\n\n  type   n percent\n  fake  60     0.4\n  real  90     0.6\n Total 150     1.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData come in: The usage of an ! might seem odd for a real article. The exclamation point data is more consistent with fake news.\n\n\n\n title_has_excl fake real\n          FALSE   44   88\n           TRUE   16    2\n          Total   60   90"
  },
  {
    "objectID": "slides/08-bayes.html#bayesian-updating-rule",
    "href": "slides/08-bayes.html#bayesian-updating-rule",
    "title": "Bayesian Linear Regression \n",
    "section": "Bayesian Updating Rule",
    "text": "Bayesian Updating Rule\n\n\nLet \\(F\\) denote the event that an article is fake.\nThe prior probability model\n\n\n\n\n\n\n\n\n\nEvent\n\\(F\\)\n\\(F^c\\)\nTotal\n\n\nProbability \\(P(\\cdot)\\)\n\n0.4\n0.6\n1"
  },
  {
    "objectID": "slides/08-bayes.html#bayesian-model-for-events",
    "href": "slides/08-bayes.html#bayesian-model-for-events",
    "title": "Bayesian Linear Regression \n",
    "section": "Bayesian Model for Events",
    "text": "Bayesian Model for Events\n\n\n\n title_has_excl fake real\n          FALSE   44   88\n           TRUE   16    2\n          Total   60   90\n\n\n\nLet \\(D\\) be the event that an article title has exclamation mark.\nConditional probability: \\(P(D \\mid F) = 0.27\\); \\(P(D \\mid F^c) = 0.02\\).\n\n\n\n\nOpposite position: We know the incoming article used ! (data); we don‚Äôt know whether or not the article is fake (what we want to decide).\n\n\n\n\nCompare \\(P(D \\mid F)\\) and \\(P(D \\mid F^c)\\) to ascertain the relative likelihoods of observed data \\(D\\) under different scenarios of the uncertain article status."
  },
  {
    "objectID": "slides/08-bayes.html#likelihood-function",
    "href": "slides/08-bayes.html#likelihood-function",
    "title": "Bayesian Linear Regression \n",
    "section": "Likelihood Function",
    "text": "Likelihood Function\n\n\n\nLikelihood function notation \\(L(\\cdot\\mid D)\\):\n\n\\[L(F \\mid D) = P(D \\mid F) \\text{ and } L(F^c \\mid D) = P(D \\mid F^c)\\]\n\n\n\n\nWhen \\(F\\) is known, the conditional probability function \\(P(\\cdot \\mid F)\\) allows us to compare the probabilities of an unknown event \\(D\\), \\(D^c\\), occurring with \\(F\\):\n\n\\[P(D \\mid F) \\text{  vs. } P(D^c \\mid F)\\]\n\n\n\n\n\nWhen \\(D\\) is known, the likelihood function \\(L(\\cdot \\mid D) = P(D \\mid \\cdot)\\) allows us to evaluate the relative compatibility of data \\(D\\) with \\(F\\) or \\(F^c\\):\n\n\\[L(F \\mid D) \\text{  vs. } L(F^c \\mid D)\\]\n\n\n\n\n\n\n\n\n\n\n\n\nEvent\n\\(F\\)\n\\(F^c\\)\nTotal\n\n\n\nProbability \\(P(\\cdot)\\)\n\n0.4\n0.6\n1\n\n\nLikelihood \\(L(\\cdot \\mid D)\\)\n\n0.27\n0.02\n0.29\n\n\n\n\n\n\n\nThe likelihood function is not a probability function!"
  },
  {
    "objectID": "slides/08-bayes.html#bayes-rule",
    "href": "slides/08-bayes.html#bayes-rule",
    "title": "Bayesian Linear Regression \n",
    "section": "Bayes‚Äô Rule",
    "text": "Bayes‚Äô Rule\n\\[\\begin{align*} P(F \\mid D) &= \\frac{P(F \\cap D)}{P(D)}\\\\ &= \\frac{P(D \\mid F)P(F)}{P(D)} \\\\ &= \\frac{P(D \\mid F)P(F)}{P(D \\mid F)P(F) + P(D \\mid F^c)P(F^c)}\\\\ &= \\frac{L(F \\mid D)P(F)}{L(F \\mid D)P(F) + L(F^c \\mid D)P(F^c)}\\end{align*}\\]\n\n\\[\\text{posterior = } \\frac{\\text{likelihood} \\cdot \\text{prior }}{ \\text{normalizing constant}} \\]\n\nThe normalizing constant \\(P(D)\\) is known as marginal likelihood or evidence."
  },
  {
    "objectID": "slides/08-bayes.html#posterior",
    "href": "slides/08-bayes.html#posterior",
    "title": "Bayesian Linear Regression \n",
    "section": "Posterior",
    "text": "Posterior\n\nStarted with a prior understanding that there‚Äôs a 40% chance that the incoming article would be fake.\nYet upon observing the use of an exclamation point in the title\n\n\n\n‚ÄúThe president has a funny secret!‚Äù\n\n\na feature that‚Äôs more common to fake news\n\nOur posterior understanding evolved quite a bit ‚Äì the chance that the article is fake jumped to 88.9%.\n\n\n\n\n\n\n\n\n\nEvent\n\\(F\\)\n\\(F^c\\)\nTotal\n\n\n\nPrior prob \\(P(\\cdot)\\)\n\n0.4\n0.6\n1\n\n\nPosterior prob \\(P(\\cdot \\mid D)\\)\n\n0.89\n0.11\n1"
  },
  {
    "objectID": "slides/08-bayes.html#bayesian-inference-for-random-variables",
    "href": "slides/08-bayes.html#bayesian-inference-for-random-variables",
    "title": "Bayesian Linear Regression \n",
    "section": "Bayesian Inference for Random Variables",
    "text": "Bayesian Inference for Random Variables\nFor any random variables parameter \\(\\theta\\) and sample data \\({\\bf Y} = (Y_1, \\dots, Y_n)\\),\n\n\\(\\pi(\\theta)\\): the prior pmf/pdf of \\(\\theta\\)\n\\(L(\\theta \\mid y_1,\\dots, y_n)\\): the likelihood function of \\(\\theta\\) given observed data \\(\\mathbf{y}= \\{y_i \\}_{i = 1}^n\\).\nThe posterior distribution of \\(\\theta\\) given \\(\\mathbf{y}\\) is\n\n\\[\\pi(\\theta \\mid \\mathbf{y}) = \\frac{L(\\theta \\mid \\mathbf{y})\\pi(\\theta)}{p(\\mathbf{y})}\\] where \\[p(\\mathbf{y}) = \\begin{cases} \\int_{\\Theta} L(\\theta \\mid \\mathbf{y})\\pi(\\theta) ~ d\\theta & \\text{if } \\theta \\text{ is continuous }\\\\\n\\sum_{\\theta \\in \\Theta} L(\\theta \\mid \\mathbf{y})\\pi(\\theta) & \\text{if } \\theta \\text{ is discrete }\n\\end{cases}\\]"
  },
  {
    "objectID": "slides/08-bayes.html#proportionality",
    "href": "slides/08-bayes.html#proportionality",
    "title": "Bayesian Linear Regression \n",
    "section": "Proportionality",
    "text": "Proportionality\n\\[\\pi(\\theta \\mid \\mathbf{y}) = \\frac{L(\\theta \\mid \\mathbf{y})\\pi(\\theta)}{p(\\mathbf{y})} \\propto_{\\theta} L(\\theta \\mid \\mathbf{y})\\pi(\\theta)\\]\n\\[\\text{posterior } \\propto \\text{ likelihood } \\cdot \\text{ prior } \\]"
  },
  {
    "objectID": "slides/08-bayes.html#motivation-example",
    "href": "slides/08-bayes.html#motivation-example",
    "title": "Bayesian Linear Regression \n",
    "section": "Motivation Example",
    "text": "Motivation Example\n\n\n\nMichelle has decided to run for governor of Wisconsin.\n\n\nAccording to previous 30 polls, Michelle‚Äôs support is centered round 45%, and she polled at around 35% in the dreariest days and around 55% in the best days.\nWith this prior information, we‚Äôd like to estimate/update Michelle‚Äôs support by conducting a new poll.\n\n\n\n\n\n\n\n\n\n\n\n\nKey: Describe prior and data information using probabilistic models.\n\n\nThe parameter to be estimated is \\(\\theta\\), the Michelle‚Äôs support, which is between 0 and 1."
  },
  {
    "objectID": "slides/08-bayes.html#prior-distribution",
    "href": "slides/08-bayes.html#prior-distribution",
    "title": "Bayesian Linear Regression \n",
    "section": "Prior Distribution",
    "text": "Prior Distribution\n\nA popular probability distribution for probability is beta distribution, denoted as \\(\\text{Beta}(\\alpha, \\beta)\\), where \\(\\alpha > 0\\) and \\(\\beta > 0\\) are shape parameters.\n\n\\[\\pi(\\theta \\mid \\alpha, \\beta) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\theta^{\\alpha - 1}(1-\\theta)^{\\beta-1}\\]"
  },
  {
    "objectID": "slides/08-bayes.html#prior-distribution-1",
    "href": "slides/08-bayes.html#prior-distribution-1",
    "title": "Bayesian Linear Regression \n",
    "section": "Prior Distribution",
    "text": "Prior Distribution\n\n\\(\\theta \\sim \\text{beta}(\\alpha, \\beta)\\)\nIn the prior model, \\(\\alpha\\) and \\(\\beta\\) are hyperparameters to be chosen to reflect our prior information.\n\n\n\nMichelle‚Äôs support is centered round 45%, and she polled at around 35% in the dreariest days and around 55% in the best days.\n\n\nChoose \\(\\alpha\\) and \\(\\beta\\) so that the prior mean is about 0.45 and the range is from 0.35 to 0.55.\n\\(\\mathrm{E}(\\theta) = \\frac{\\alpha}{\\alpha + \\beta}\\)\n\\(\\mathrm{Var}(\\theta) = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\)"
  },
  {
    "objectID": "slides/08-bayes.html#prior-distribution-2",
    "href": "slides/08-bayes.html#prior-distribution-2",
    "title": "Bayesian Linear Regression \n",
    "section": "Prior Distribution",
    "text": "Prior Distribution"
  },
  {
    "objectID": "slides/08-bayes.html#likelihood",
    "href": "slides/08-bayes.html#likelihood",
    "title": "Bayesian Linear Regression \n",
    "section": "Likelihood",
    "text": "Likelihood\n\nYou plan to conduct a new poll of \\(n = 50\\) Cheeseheads and record \\(Y\\), the number that support Michelle.\n\n\nWhat distribution can be used for modeling likelihood connecting the data \\(y\\) and the parameter we are interested, \\(\\theta\\)?\n\n\n\nIf voters answer the poll independently, and the probability that any polled voter supports Michelle is \\(\\theta\\), we could consider\n\n\\[Y \\mid \\theta \\sim \\text{binomial}(n=50, \\theta)\\]\n\n\n\nThe poll result is \\(y = 30\\), the likelihood is\n\n\\[L(\\theta \\mid y = 30) = {50 \\choose 30}\\theta^{30}(1-\\theta)^{20}, \\quad \\theta \\in (0, 1)\\]"
  },
  {
    "objectID": "slides/08-bayes.html#likelihood-1",
    "href": "slides/08-bayes.html#likelihood-1",
    "title": "Bayesian Linear Regression \n",
    "section": "Likelihood",
    "text": "Likelihood"
  },
  {
    "objectID": "slides/08-bayes.html#bayesian-model",
    "href": "slides/08-bayes.html#bayesian-model",
    "title": "Bayesian Linear Regression \n",
    "section": "Bayesian Model",
    "text": "Bayesian Model\n\\[\\begin{align}Y \\mid \\theta &\\sim \\text{binomial}(n=50, \\theta)\\\\ \\theta &\\sim \\text{beta}(45, 55)\n\\end{align}\\]\nGoal: Obtain the posterior distribution \\(\\pi(\\theta \\mid y)\\).\n\n\\[\n\\begin{align} \\pi(\\theta \\mid y) &\\propto_{\\theta} L(\\theta \\mid y)\\pi(\\theta) \\\\\n&= {50 \\choose 30}\\theta^{30}(1-\\theta)^{20} \\times \\frac{\\Gamma(100)}{\\Gamma(45)\\Gamma(55)}\\theta^{44}(1-\\theta)^{54}\\\\\n&\\propto_{\\theta} \\theta^{74}(1-\\theta)^{74}\\\\\n&= \\frac{\\Gamma(150)}{\\Gamma(75)\\Gamma(75)} \\theta^{74}(1-\\theta)^{74} \\\\\n&= \\text{beta}(75, 75)\\end{align}\n\\]\nusing the fact that \\(\\int_{\\mathcal{X}} f(x) dx = 1\\) for any pdf \\(f(x)\\)."
  },
  {
    "objectID": "slides/08-bayes.html#posterior-distribution",
    "href": "slides/08-bayes.html#posterior-distribution",
    "title": "Bayesian Linear Regression \n",
    "section": "Posterior Distribution",
    "text": "Posterior Distribution"
  },
  {
    "objectID": "slides/08-bayes.html#take-home-message",
    "href": "slides/08-bayes.html#take-home-message",
    "title": "Bayesian Linear Regression \n",
    "section": "Take-home Message",
    "text": "Take-home Message\n\nThe parameter is a random variable in the Bayesian world.\nIt is fixed and constant in the frequentist world."
  },
  {
    "objectID": "slides/08-bayes.html#normal-data-model",
    "href": "slides/08-bayes.html#normal-data-model",
    "title": "Bayesian Linear Regression \n",
    "section": "Normal Data Model",
    "text": "Normal Data Model\nIn simple linear regression, we have\n\\[Y_i \\mid \\beta_0, \\beta_1, \\sigma \\stackrel{ind}{\\sim}  N\\left(\\mu_i, \\sigma^2 \\right) \\quad \\text{with} \\quad \\mu_i = \\beta_0 + \\beta_1 X_i\\]\nThis normal data model is our likelihood \\(L(\\boldsymbol \\theta= (\\beta_0, \\beta_1, \\sigma) \\mid \\mathcal{D} = \\{\\mathbf{y}, \\mathbf{x}\\})\\), as it evaluates how the data are compatible with different possible values of parameters.\n\nTo be a Bayesian, what do we do next?\n\n\n\nWe just assign priors to the unknown parameters, then do the posterior inference using Bayes‚Äô rule!\n\n\n\n\nThe big question is how?"
  },
  {
    "objectID": "slides/08-bayes.html#prior-models",
    "href": "slides/08-bayes.html#prior-models",
    "title": "Bayesian Linear Regression \n",
    "section": "Prior Models",
    "text": "Prior Models\n\nThere are countless approaches to construct priors for \\(\\beta_0, \\beta_1\\), and \\(\\sigma\\).\n\n\n\nFor simplicity, assume \\(\\beta_0, \\beta_1\\), and \\(\\sigma\\) are independent, \\(\\pi(\\boldsymbol \\theta) = \\pi(\\beta_0, \\beta_1, \\sigma) = \\pi(\\beta_0)\\pi(\\beta_1)\\pi(\\sigma)\\)\n\n\n\n\n\n\n\\(\\beta_0\\) and \\(\\beta_1\\) can technically take any values in the real line.\n\n\\[\\begin{align} \\beta_0 \\sim N(m_0, s_0^2) \\\\\n\\beta_1 \\sim N(m_1, s_1^2) \\end{align}\\]\n\n\n\n\n\\(\\sigma\\) must be positive.\n\n\\[\\begin{align} \\sigma \\sim \\text{Exp}(\\lambda) \\end{align}\\] \\(\\pi(\\sigma) = \\lambda e^{-\\lambda \\sigma}\\), \\(\\lambda > 0\\) and \\(\\mathrm{E}(\\sigma) = 1/\\lambda\\), and \\(\\mathrm{Var}(\\sigma) = 1/\\lambda^2\\)\nhttps://mc-stan.org/rstanarm/articles/priors.html"
  },
  {
    "objectID": "slides/08-bayes.html#bayesian-simple-linear-regression-model",
    "href": "slides/08-bayes.html#bayesian-simple-linear-regression-model",
    "title": "Bayesian Linear Regression \n",
    "section": "Bayesian Simple Linear Regression Model",
    "text": "Bayesian Simple Linear Regression Model\n\\[\\begin{align} Y_i \\mid \\beta_0, \\beta_1, \\sigma &\\stackrel{ind}{\\sim}  N\\left(\\mu_i, \\sigma^2 \\right) \\quad \\text{with} \\quad \\mu_i = \\beta_0 + \\beta_1 X_i \\\\\n\\beta_0 &\\sim N(m_0, s_0^2) \\\\\n\\beta_1 &\\sim N(m_1, s_1^2) \\\\\n\\sigma &\\sim \\text{Exp}(\\lambda) \\end{align}\\]"
  },
  {
    "objectID": "slides/08-bayes.html#capital-bikeshare-bikes-data-in-the-washington-d.c.-area.",
    "href": "slides/08-bayes.html#capital-bikeshare-bikes-data-in-the-washington-d.c.-area.",
    "title": "Bayesian Linear Regression \n",
    "section": "Capital Bikeshare bikes Data in the Washington, D.C. Area.",
    "text": "Capital Bikeshare bikes Data in the Washington, D.C. Area.\n\n\nRows: 500\nColumns: 2\n$ rides     <int> 654, 1229, 1454, 1518, 1362, 891, 1280, 1220, 1137, 1368, 13‚Ä¶\n$ temp_feel <dbl> 64.7, 49.0, 51.1, 52.6, 50.8, 46.6, 45.6, 49.2, 46.4, 45.6, ‚Ä¶"
  },
  {
    "objectID": "slides/08-bayes.html#tuning-prior-models",
    "href": "slides/08-bayes.html#tuning-prior-models",
    "title": "Bayesian Linear Regression \n",
    "section": "Tuning Prior Models",
    "text": "Tuning Prior Models\n\n\nPrior understanding 1:\n\nOn an average temperature day, say 65 or 70 degrees, there are typically around 5000 riders, though this average could be somewhere between 3000 and 7000.\n\n\n\n\n\nThe prior information tells us something about \\(\\beta_0\\), but the information has been centered.\n\nThe centered intercept, \\(\\beta_{0c}\\), reflects the typical ridership at the typical temperature.\n\n\n\n\\(\\beta_{0c} \\sim N(5000, 1000^2)\\)"
  },
  {
    "objectID": "slides/08-bayes.html#tuning-prior-models-1",
    "href": "slides/08-bayes.html#tuning-prior-models-1",
    "title": "Bayesian Linear Regression \n",
    "section": "Tuning Prior Models",
    "text": "Tuning Prior Models\n\n\nPrior understanding 2:\n\nFor every one degree increase in temperature, ridership typically increases by 100 rides, though this average increase could be as low as 20 or as high as 180.\n\n\n\n\n\\(\\beta_{1} \\sim N(100, 40^2)\\)"
  },
  {
    "objectID": "slides/08-bayes.html#tuning-prior-models-2",
    "href": "slides/08-bayes.html#tuning-prior-models-2",
    "title": "Bayesian Linear Regression \n",
    "section": "Tuning Prior Models",
    "text": "Tuning Prior Models\n\n\nPrior understanding 3:\n\nAt any given temperature, daily ridership will tend to vary with a moderate standard deviation of 1250 rides.\n\n\n\n\n\\(\\sigma \\sim \\text{Exp}(0.0008)\\) because \\(\\mathrm{E}(\\sigma) = 1/\\lambda = 1/0.0008 = 1250\\)"
  },
  {
    "objectID": "slides/08-bayes.html#prior-model-simulation",
    "href": "slides/08-bayes.html#prior-model-simulation",
    "title": "Bayesian Linear Regression \n",
    "section": "Prior Model Simulation",
    "text": "Prior Model Simulation\n\n100 prior plausible model lines \\(\\mu_{Y|X} = \\beta_0 + \\beta_1 X\\)"
  },
  {
    "objectID": "slides/08-bayes.html#posterior-inference",
    "href": "slides/08-bayes.html#posterior-inference",
    "title": "Bayesian Linear Regression \n",
    "section": "Posterior Inference",
    "text": "Posterior Inference\n\nUpdate our prior understanding of the relationship between ridership and temperature using data information provided by likelihood.\nThe joint posterior distribution is\n\n\\[\\pi(\\beta_0, \\beta_1, \\sigma \\mid \\mathbf{y}) = \\frac{L(\\beta_0, \\beta_1, \\sigma \\mid \\mathbf{y})\\pi(\\beta_0, \\beta_1, \\sigma)}{p(\\mathbf{y})}\\] where\n\n\\(L(\\beta_0, \\beta_1, \\sigma \\mid \\mathbf{y}) = p(\\mathbf{y}\\mid \\beta_0, \\beta_1, \\sigma) = \\prod_{i=1}^np(y_i \\mid \\beta_0, \\beta_1, \\sigma)\\)\n\\(\\pi(\\beta_0, \\beta_1, \\sigma) = \\pi(\\beta_0)\\pi(\\beta_1)\\pi(\\sigma)\\)\n\\(p(\\mathbf{y}) = \\int \\int \\int \\left[\\prod_{i=1}^np(y_i \\mid \\beta_0, \\beta_1, \\sigma)\\right]\\pi(\\beta_0)\\pi(\\beta_1)\\pi(\\sigma) ~ d\\beta_0d\\beta_1d\\sigma\\)\nThere are lots of ways to generate/approximate the posterior distribution of parameters. One method is Markov chain Monte Carlo (MCMC)."
  },
  {
    "objectID": "slides/08-bayes.html#rstanarmstan_glm",
    "href": "slides/08-bayes.html#rstanarmstan_glm",
    "title": "Bayesian Linear Regression \n",
    "section": "rstanarm::stan_glm()",
    "text": "rstanarm::stan_glm()\n\n\nrstanarm uses RStan syntax1 to do Bayesian inference for applied regression models (arm).\n\n\nbike_model <- rstanarm::stan_glm(rides ~ temp_feel, data = bikes,\n                                 family = gaussian,\n                                 prior_intercept = normal(5000, 1000),\n                                 prior = normal(100, 40), \n                                 prior_aux = exponential(0.0008),\n                                 chains = 4, iter = 5000*2, seed = 84735)\n\n\nGenerate 4 Monte Carlo chains (chains = 4), each having 10000 iterations (iter = 5000*2).\nEach iteration draw a posterior sample of the parameters \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma\\).\nAfter tossing out the first half of Markov chain values from the warm-up or burn-in phase, stan_glm() simulation produces four parallel chains of length 5000 for each model parameter:\n\n\\(\\{ \\beta_0^{(1)}, \\beta_0^{(2)}, \\dots, \\beta_0^{(5000)} \\}\\), \\(\\{ \\beta_1^{(1)}, \\beta_1^{(2)}, \\dots, \\beta_1^{(5000)} \\}\\), \\(\\{ \\sigma^{(1)}, \\sigma^{(2)}, \\dots, \\sigma^{(5000)} \\}\\)\nRStan is the R interface to Stan."
  },
  {
    "objectID": "slides/08-bayes.html#convergence-diagnostics",
    "href": "slides/08-bayes.html#convergence-diagnostics",
    "title": "Bayesian Linear Regression \n",
    "section": "Convergence Diagnostics",
    "text": "Convergence Diagnostics\n\n# Trace plots of parallel chains\nbayesplot::mcmc_trace(bike_model, size = 0.1)"
  },
  {
    "objectID": "slides/08-bayes.html#convergence-diagnostics-1",
    "href": "slides/08-bayes.html#convergence-diagnostics-1",
    "title": "Bayesian Linear Regression \n",
    "section": "Convergence Diagnostics",
    "text": "Convergence Diagnostics\n\n# Density plots of parallel chains\nbayesplot::mcmc_dens_overlay(bike_model)\n\n\n\nquantifies the number of independent samples it would take to produce an equivalently accurate posterior approximation.\nwe observe that these four chains produce nearly indistinguishable posterior approximations. This provides evidence that our simulation is stable and sufficiently long ‚Äì running the chains for more iterations likely wouldn‚Äôt produce drastically different or improved posterior approximations."
  },
  {
    "objectID": "slides/08-bayes.html#convergence-diagnostics-2",
    "href": "slides/08-bayes.html#convergence-diagnostics-2",
    "title": "Bayesian Linear Regression \n",
    "section": "Convergence Diagnostics",
    "text": "Convergence Diagnostics\n\n# Effective sample size ratio\nbayesplot::neff_ratio(bike_model)\n\n(Intercept)   temp_feel       sigma \n      1.006       0.997       1.017 \n\n# Rhat\nbayesplot::rhat(bike_model)\n\n(Intercept)   temp_feel       sigma \n          1           1           1 \n\n\n\nBoth effective sample size and R-hat are close to 1, indicating that the chains are stable, mixing quickly, and behaving much like an independent sample.\n\n\nThere‚Äôs no magic rule for interpreting this ratio, and it should be utilized alongside other diagnostics such as the trace plot. That said, we might be suspicious of a Markov chain for which the effective sample size ratio is less than 0.1, i.e., the effective sample size is less than 10% of the actual sample size.\nan R-hat ratio greater than 1.05 raises some red flags about the stability of the simulation."
  },
  {
    "objectID": "slides/08-bayes.html#convergence-issues",
    "href": "slides/08-bayes.html#convergence-issues",
    "title": "Bayesian Linear Regression \n",
    "section": "Convergence Issues",
    "text": "Convergence Issues\n\n\n\n\nHighly Autocorrelated Chain: Effective size is small, not many independent samples that are representative of the true posterior distribution.\n\nRun longer and thinning the chain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlow Convergence: Need wait longer to have the chain reached a stable mixing zone that are representative of the true posterior distribution.\n\nSet a longer burn-in or warm-up period"
  },
  {
    "objectID": "slides/08-bayes.html#interpreting-the-posterior",
    "href": "slides/08-bayes.html#interpreting-the-posterior",
    "title": "Bayesian Linear Regression \n",
    "section": "Interpreting the Posterior",
    "text": "Interpreting the Posterior\n\n# Posterior summary statistics\ntidy(bike_model, effects = c(\"fixed\", \"aux\"),\n     conf.int = TRUE, conf.level = 0.80)\n\n# A tibble: 4 √ó 5\n  term        estimate std.error conf.low conf.high\n  <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)  -2196.     356.    -2643.    -1738. \n2 temp_feel       82.2      5.10     75.7      88.5\n3 sigma         1282.      40.9    1231.     1336. \n4 mean_PPD      3487.      81.2    3382.     3591. \n\n\n\nThe posterior relationship is\n\n\\[-2196 + 82.2 X\\]\n\nThe 80% credible interval for \\(\\beta_1\\) is \\((75.7, 88.5)\\).\nGiven the data, the probability that \\(\\beta_1\\) is between 75.7 and 88.5 is 80%., i.e., \\(P(\\beta_1 \\in(75.7, 88.5) \\mid \\mathbf{y}, \\mathbf{x}) = 80\\%\\)."
  },
  {
    "objectID": "slides/08-bayes.html#posterior-samples",
    "href": "slides/08-bayes.html#posterior-samples",
    "title": "Bayesian Linear Regression \n",
    "section": "Posterior Samples",
    "text": "Posterior Samples\n\n# Store the 4 chains for each parameter in 1 data frame\nbike_model_df <- as.data.frame(bike_model)\nnrow(bike_model_df)\n\n[1] 20000\n\nhead(bike_model_df)\n\n  (Intercept) temp_feel sigma\n1       -2316      84.2  1245\n2       -2489      87.1  1306\n3       -2491      86.3  1295\n4       -2853      91.6  1298\n5       -2379      85.9  1319\n6       -2277      82.7  1278\n\n\nHow to obtain \\(P(\\beta_1 > 0 \\mid \\mathbf{y}, \\mathbf{x})\\)?\n\n\\(P(\\beta_1 > 0 \\mid \\mathbf{y}, \\mathbf{x}) \\approx \\frac{1}{20000}\\sum_{t=1}^{20000} \\mathbf{1}\\left(\\beta_1^{(t)} > 0\\right)\\)\n\nsum(bike_model_df$temp_feel > 0) / nrow(bike_model_df)\n\n[1] 1"
  },
  {
    "objectID": "slides/08-bayes.html#posterior-regression-lines",
    "href": "slides/08-bayes.html#posterior-regression-lines",
    "title": "Bayesian Linear Regression \n",
    "section": "Posterior Regression Lines",
    "text": "Posterior Regression Lines"
  },
  {
    "objectID": "slides/08-bayes.html#posterior-predictive-draws",
    "href": "slides/08-bayes.html#posterior-predictive-draws",
    "title": "Bayesian Linear Regression \n",
    "section": "Posterior Predictive Draws",
    "text": "Posterior Predictive Draws\nFor each posterior draw \\(\\{\\beta_0^{(t)}, \\beta_1^{(t)}, \\sigma^{(t)} \\}_{t = 1}^{20000}\\), we have the posterior predictive distribution \\[Y_i^{(t)} \\sim N\\left(\\beta_0^{(t)} + \\beta_1^{(t)}X_i, \\, (\\sigma^{(t)})^2\\right)\\]"
  },
  {
    "objectID": "slides/08-bayes.html#bayesian-interpretation-for-ridge-and-lasso",
    "href": "slides/08-bayes.html#bayesian-interpretation-for-ridge-and-lasso",
    "title": "Bayesian Linear Regression \n",
    "section": "Bayesian Interpretation for Ridge and Lasso",
    "text": "Bayesian Interpretation for Ridge and Lasso\n\nLasso and Ridge regression can be interpreted as a Bayesian regression model.\nThe same data-level likelihood \\[Y_i \\mid \\boldsymbol \\beta, \\sigma \\stackrel{ind}{\\sim}  N\\left(\\mu_i, \\sigma^2 \\right) \\quad \\text{with} \\quad \\mu_i = \\mathbf{x}_i'\\boldsymbol \\beta\\]\nWe use prior distributions to regularize how parameters behave.\n\n\n\nThe two methods can assign the same prior distributions to \\(\\beta_0\\) and \\(\\sigma\\), but they use different priors on \\(\\{\\beta_j\\}_{j = 1}^p\\).1\n\nLasso: \\(\\beta_j \\stackrel{iid}{\\sim} \\text{Laplace}\\left(0, \\tau(\\lambda)\\right)\\)\nRidge: \\(\\beta_j \\stackrel{iid}{\\sim} N\\left(0, \\tau(\\lambda)\\right)\\)\n\n\n\n\nRemember we usually standardize coefficients before implementing Ridge and Lasso."
  },
  {
    "objectID": "slides/08-bayes.html#ridge-and-lasso-priors",
    "href": "slides/08-bayes.html#ridge-and-lasso-priors",
    "title": "Bayesian Linear Regression \n",
    "section": "Ridge and Lasso Priors",
    "text": "Ridge and Lasso Priors\n\n\nLasso\n\n\\[\\beta_j \\stackrel{iid}{\\sim} \\text{Laplace}\\left(0, \\tau(\\lambda)\\right)\\]\nLasso solution is the posterior mode of \\(\\boldsymbol \\beta\\)\n\\[\\boldsymbol \\beta^{(l)} = \\mathop{\\mathrm{arg\\,max}}_{\\boldsymbol \\beta} \\,\\,\\, \\pi(\\boldsymbol \\beta\\mid \\mathbf{y}, \\mathbf{x})\\]\n\n\n\n\n\n\n\n\n\n\nRidge\n\n\\[\\beta_j \\stackrel{iid}{\\sim} N\\left(0, \\tau(\\lambda)\\right)\\]\nRidge solution is the posterior mode/mean of \\(\\boldsymbol \\beta\\)\n\\[\\boldsymbol \\beta^{(r)} = \\mathop{\\mathrm{arg\\,max}}_{\\boldsymbol \\beta} \\,\\, \\, \\pi(\\boldsymbol \\beta\\mid \\mathbf{y}, \\mathbf{x})\\]"
  },
  {
    "objectID": "slides/08-bayes.html#resources",
    "href": "slides/08-bayes.html#resources",
    "title": "Bayesian Linear Regression \n",
    "section": "Resources",
    "text": "Resources\n\nBayes Rules! An Introduction to Applied Bayesian Modeling\nStatistical Rethinking\nA Student‚Äôs Guide to Bayesian Statistics\nBayesian Data Analysis\n\n\n\n\nmssc6250-s24.github.io/website"
  },
  {
    "objectID": "slides/05-ridge-cv.html#when-ordinary-least-squares-ols-doesnt-work-well-collinearity",
    "href": "slides/05-ridge-cv.html#when-ordinary-least-squares-ols-doesnt-work-well-collinearity",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "When Ordinary Least Squares (OLS) Doesn‚Äôt Work Well: Collinearity",
    "text": "When Ordinary Least Squares (OLS) Doesn‚Äôt Work Well: Collinearity\n\nWhen predictors are highly correlated, \\(\\mathrm{Var}(b_j)\\) is much inflated.1\n\nA tiny change in the training data causes a large change in \\(b_j\\), leading to unreliable estimation and prediction.\n\n\n\n\n\n\\({\\bf X'X} = \\begin{bmatrix} 1 & 0.99 \\\\ 0.99 & 1 \\end{bmatrix}\\) \\(\\quad ({\\bf X'X})^{-1} = \\begin{bmatrix} 50.3 & -49.7 \\\\ -49.7 & 50.3 \\end{bmatrix}\\)\n\n\\(\\mathrm{Var}(b_j) = 50.3\\sigma^2\\)\n\nAn increase in 50-fold over the ideal case when the two regressors are orthogonal.\n\n\n\n\nsolve(matrix(c(1, 0.99, 0.99, 1), 2, 2))\n\n      [,1]  [,2]\n[1,]  50.3 -49.7\n[2,] -49.7  50.3\n\n\nthe eigen vector direction with the smallest eigen value\n\n\nAlthough \\(\\mathrm{Var}(b_j)\\) is still the smallest among all linear unbiased estimators."
  },
  {
    "objectID": "slides/05-ridge-cv.html#when-ols-doesnt-work-well-optimization-perspective",
    "href": "slides/05-ridge-cv.html#when-ols-doesnt-work-well-optimization-perspective",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "When OLS Doesn‚Äôt Work Well: Optimization Perspective",
    "text": "When OLS Doesn‚Äôt Work Well: Optimization Perspective\n\n\\(\\beta_1 = \\beta_2 = 1\\) and \\(\\mathrm{Corr}(x_1, x_2) = 0.99\\)\nThe relatively ‚Äúflat‚Äù valley in the objective function walks along the eigen-vector of \\({\\bf X}'{\\bf X}\\) having the smallest eigen-value.\n\n\n\n\n\n\n\n\n\n\n\n\n\neigen(\n    matrix(c(1, 0.99, 0.99, 1), \n             2, 2)\n    )\n\neigen() decomposition\n$values\n[1] 1.99 0.01\n\n$vectors\n      [,1]   [,2]\n[1,] 0.707 -0.707\n[2,] 0.707  0.707\n\n\n\n\n\nFrom optimization point of view, the objective function (\\(\\ell_2\\) loss) is ‚Äúflat‚Äù along certain directions in the parameter domain.\na small eigen-value in XTX makes the corresponding eigen-value large in the inverse."
  },
  {
    "objectID": "slides/05-ridge-cv.html#when-ols-doesnt-work-well-optimization-perspective-1",
    "href": "slides/05-ridge-cv.html#when-ols-doesnt-work-well-optimization-perspective-1",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "When OLS Doesn‚Äôt Work Well: Optimization Perspective",
    "text": "When OLS Doesn‚Äôt Work Well: Optimization Perspective\n\nA little change in the training set perturbs the objective function. The LSEs lie on a valley centered around the truth."
  },
  {
    "objectID": "slides/05-ridge-cv.html#when-ols-doesnt-work-well-high-variance",
    "href": "slides/05-ridge-cv.html#when-ols-doesnt-work-well-high-variance",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "When OLS Doesn‚Äôt Work Well: High Variance",
    "text": "When OLS Doesn‚Äôt Work Well: High Variance\n\nThe optimizer could land anywhere along the valley, leading to large variance of the LSE.\nOver many simulation runs, the LSE lies around the line of \\(\\beta_1 + \\beta_2 = 2\\), the direction of the eigen-vector of the smallest eigen-value."
  },
  {
    "objectID": "slides/05-ridge-cv.html#when-ols-doesnt-work-well-large-p-small-n-high-dimensional-data",
    "href": "slides/05-ridge-cv.html#when-ols-doesnt-work-well-large-p-small-n-high-dimensional-data",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "When OLS Doesn‚Äôt Work Well: Large-\\(p\\)-small-\\(n\\) (High Dimensional Data)",
    "text": "When OLS Doesn‚Äôt Work Well: Large-\\(p\\)-small-\\(n\\) (High Dimensional Data)\n\nOLS stays well in the world of ‚Äúlarge-\\(n\\)-small-\\(p\\)‚Äù.\nWhen \\(p > n\\), \\({\\bf X}'{\\bf X}\\) is not invertible.\nThere is no unique \\(\\boldsymbol \\beta\\) estimate.\n\n\nIntuition: Too many degrees of freedom (\\(p\\)) to specify a model, but not enough information (\\(n\\)) to decide which one is the one.\n\nToo flexible and ends up with overfitting"
  },
  {
    "objectID": "slides/05-ridge-cv.html#remedy-for-large-variance-and-large-p-small-n",
    "href": "slides/05-ridge-cv.html#remedy-for-large-variance-and-large-p-small-n",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Remedy for Large Variance and Large-\\(p\\)-small-\\(n\\)\n",
    "text": "Remedy for Large Variance and Large-\\(p\\)-small-\\(n\\)\n\n\nMake \\({\\bf X}'{\\bf X}\\) invertible when \\(p > n\\) by regularizing coefficient behavior!\nA good estimator balances bias and variance well, or minimizes the mean square error \\[\\text{MSE}(\\hat{\\beta}) = E[(\\hat{\\beta} - \\beta)^2] = \\mathrm{Var}(\\hat{\\beta}) + \\text{Bias}(\\hat{\\beta})^2\\]\nFind a biased estimator \\(\\hat{\\boldsymbol \\beta}\\) that has smaller variance and MSE than the LSE \\({\\bf b}\\)."
  },
  {
    "objectID": "slides/05-ridge-cv.html#ridge-regression-1",
    "href": "slides/05-ridge-cv.html#ridge-regression-1",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\nIdea: Add a ridge (diagonal matrix) to \\({\\bf X} ' {\\bf X}\\).1\n\n\n\\[\\widehat{\\boldsymbol \\beta}^\\text{r} = (\\mathbf{X}' \\mathbf{X}+ n \\lambda \\mathbf{I})^{-1} \\mathbf{X}' \\mathbf{y},\\]\n\n\nTo regularize coefficient behavior, we add an \\(\\ell_2\\) penalty to the residual sum of squares, for some tuning parameter \\(\\lambda > 0\\).\n\n\\[\n\\begin{align}\n\\widehat{\\boldsymbol \\beta}^\\text{r} =& \\, \\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol \\beta} \\underbrace{\\lVert \\mathbf{y}- \\mathbf{X}\\boldsymbol \\beta\\rVert_2^2}_{SS_{res}} + n \\lambda \\lVert\\boldsymbol \\beta\\rVert_2^2\\\\\n=& \\, \\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol \\beta} \\underbrace{\\frac{1}{n} \\sum_{i=1}^n (y_i - x_i'\\boldsymbol \\beta)^2}_{\\text{MSE}_{\\texttt{Tr}}} + \\lambda \\sum_{j=1}^p \\beta_j^2\\\\\n=& \\, \\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol \\beta} \\color{green} - \\text{ goodness of fit } + \\text{ model complexity/flexibility}\n\\end{align}\n\\]\n\nThis is a special case of the Tikhonov regularization."
  },
  {
    "objectID": "slides/05-ridge-cv.html#properties-of-ridge-regression",
    "href": "slides/05-ridge-cv.html#properties-of-ridge-regression",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Properties of Ridge Regression",
    "text": "Properties of Ridge Regression\n\\[\n\\begin{align}\n\\widehat{\\boldsymbol \\beta}^\\text{r} =& \\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol \\beta} \\frac{1}{n} \\sum_{i=1}^n (y_i - x_i'\\boldsymbol \\beta)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2,\n\\end{align}\n\\]\nProperties of ridge regression:\n\nHas less degrees of freedom in the sense that the cost increases when large coefficients are used.\n\n\n\nFavors small-magnitude coefficient estimates to avoid cost penalty. (Shrinkage)\n\n\n\n\nThe shrinkage parameter \\(\\lambda\\) controls the degree of penalty.\n\n\n\\(\\lambda \\rightarrow 0\\): No penalty, and \\(\\widehat{\\boldsymbol \\beta}^\\text{r} = \\bf b\\).\n\n\\(\\lambda \\rightarrow \\infty\\): Unbearable penalty, and \\(\\widehat{\\boldsymbol \\beta}^\\text{r} \\rightarrow \\mathbf{0}\\)"
  },
  {
    "objectID": "slides/05-ridge-cv.html#ridge-penalty",
    "href": "slides/05-ridge-cv.html#ridge-penalty",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Ridge Penalty",
    "text": "Ridge Penalty\n\\[\\lambda \\lVert \\boldsymbol \\beta\\rVert^2_2 = \\lambda \\boldsymbol \\beta' \\mathbf{I}\\boldsymbol \\beta\\]\n\nThe penalty contour is circle-shaped, forcing the objective function to be more convex, leading to a more stable or less varying solution."
  },
  {
    "objectID": "slides/05-ridge-cv.html#more-convex-loss-function",
    "href": "slides/05-ridge-cv.html#more-convex-loss-function",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "More Convex Loss Function",
    "text": "More Convex Loss Function\n\nAdding a ridge penalty forces the objective to be more convex due to the added eigenvalues.\n\n\neigen(matrix(c(1, 0.99, 0.99, 1), 2, 2) + diag(2))[1]\n\n$values\n[1] 2.99 1.01\n\n\n\nHence, by adding this to the OLS objective function, the solution is more stable, in the sense that each time we observe a new set of data, this contour looks pretty much the same. This may be interpreted in several different ways such as: 1) the objective function is more convex and less affected by the random samples; 2) the variance of the estimator is smaller because the eigenvalues of \\(\\mathbf{X}^\\text{T}\\mathbf{X}+ n \\lambda \\mathbf{I}\\) are large."
  },
  {
    "objectID": "slides/05-ridge-cv.html#the-bias-variance-trade-off",
    "href": "slides/05-ridge-cv.html#the-bias-variance-trade-off",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "The Bias-variance Trade-off",
    "text": "The Bias-variance Trade-off\n\nAs \\(\\lambda \\downarrow\\), bias \\(\\downarrow\\) and variance \\(\\uparrow\\)\n\nAs \\(\\lambda \\uparrow\\), bias \\(\\uparrow\\) and variance \\(\\downarrow\\)\n\n\n\nThis effect is gradually changing as we increase the penalty level. The following simulation shows how the variation of \\(\\boldsymbol \\beta\\) changes. We show this with two penalty values, and see how the estimated parameters are away from the truth.\nNow, we may ask the question: is it worth it? In fact, this bias and variance will be then carried to the predicted values \\(x^\\text{T}\\widehat{\\boldsymbol \\beta}^\\text{ridge}\\). Hence, we can judge if this is beneficial from the prediction accuracy. And we need some procedure to do this.\nRemark: The bias-variance trade-off will appear frequently in this course. And the way to evaluate the benefit of this is to see if it eventually reduces the prediction error (\\(\\text{Bias}^2 + \\text{Variance}\\) plus a term called irreducible error, which will be introduced in later chapter)."
  },
  {
    "objectID": "slides/05-ridge-cv.html#lower-test-mse",
    "href": "slides/05-ridge-cv.html#lower-test-mse",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Lower Test MSE",
    "text": "Lower Test MSE\n\n\n\nWhen LS \\(b_j\\) are variable or \\(p > n\\), ridge regression could have lower test MSE and better predictive performance.\nSquared bias (black)\nVariance (green)\n\\(\\text{MSE}_{\\texttt{Tr}}\\) (purple)\n\n\n\n\n\n\n\nSource: ISL Figure 6.5"
  },
  {
    "objectID": "slides/05-ridge-cv.html#masslm.ridge",
    "href": "slides/05-ridge-cv.html#masslm.ridge",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "MASS::lm.ridge()",
    "text": "MASS::lm.ridge()\n\nThe lambda parameter lm.ridge() actually specifies the \\(n\\lambda\\) in our notation.\nOLS is scale equivalent: \\(X_jb_j\\) remains the same regardless of how the \\(j\\)-th predictor is scaled.\nRidge coefficient estimates can change substantially when multiplying a given predictor by a constant. \\(X_j\\hat{\\beta}^{r}_{j, \\lambda}\\) depends on \\(\\lambda\\), the scaling of \\(X_j\\), and even the scaling of other predictors.\nStandardize all predictors!1\n\n\n\nhead(mtcars, 3)\n\n               mpg cyl disp  hp drat   wt qsec vs am gear carb\nMazda RX4     21.0   6  160 110 3.90 2.62 16.5  0  1    4    4\nMazda RX4 Wag 21.0   6  160 110 3.90 2.88 17.0  0  1    4    4\nDatsun 710    22.8   4  108  93 3.85 2.32 18.6  1  1    4    1\n\n(fit <- MASS::lm.ridge(mpg ~ ., data = mtcars, lambda = 1)) ## ridge fit\n\n              cyl     disp       hp     drat       wt     qsec       vs \n16.53766 -0.16240  0.00233 -0.01493  0.92463 -2.46115  0.49259  0.37465 \n      am     gear     carb \n 2.30838  0.68572 -0.57579 \n\n\n\n\nIn practice, if the intercept is not our interest, we also standardize the response."
  },
  {
    "objectID": "slides/05-ridge-cv.html#scaling-issues-of-lm.ridge",
    "href": "slides/05-ridge-cv.html#scaling-issues-of-lm.ridge",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Scaling Issues of lm.ridge()\n",
    "text": "Scaling Issues of lm.ridge()\n\n\ncoef(fit) transforms these back to the original scale.\nfit$coef shows the coefficients of the standardized predictors.\n\n\ncoef(fit)\n\n              cyl     disp       hp     drat       wt     qsec       vs \n16.53766 -0.16240  0.00233 -0.01493  0.92463 -2.46115  0.49259  0.37465 \n      am     gear     carb \n 2.30838  0.68572 -0.57579 \n\nfit$coef\n\n   cyl   disp     hp   drat     wt   qsec     vs     am   gear   carb \n-0.285  0.285 -1.008  0.487 -2.370  0.866  0.186  1.134  0.498 -0.915"
  },
  {
    "objectID": "slides/05-ridge-cv.html#scaling-issues-of-lm.ridge-1",
    "href": "slides/05-ridge-cv.html#scaling-issues-of-lm.ridge-1",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Scaling Issues of lm.ridge()\n",
    "text": "Scaling Issues of lm.ridge()\n\n\nfit$coef\n\n   cyl   disp     hp   drat     wt   qsec     vs     am   gear   carb \n-0.285  0.285 -1.008  0.487 -2.370  0.866  0.186  1.134  0.498 -0.915 \n\n\n\n\nlm.ridge() uses \\(n\\) instead of \\(n-1\\) when calculating the standard deviation.\n\n\n# each column has mean 0 and var 1\nX <- scale(data.matrix(mtcars[, -1]), center = TRUE, scale = TRUE)\n\n# center y but not scaling\ny <- scale(mtcars$mpg, center = TRUE, scale = FALSE)\n\n\n\n# lambda = 1\nmy_ridge_coef <- solve(t(X) %*% X + diag(ncol(X))) %*% t(X) %*% y\nt(my_ridge_coef)\n\n        cyl disp    hp  drat    wt qsec   vs   am  gear   carb\n[1,] -0.294 0.27 -1.02 0.495 -2.39 0.87 0.19 1.15 0.505 -0.937"
  },
  {
    "objectID": "slides/05-ridge-cv.html#scaling-issues-of-lm.ridge-2",
    "href": "slides/05-ridge-cv.html#scaling-issues-of-lm.ridge-2",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Scaling Issues of lm.ridge()\n",
    "text": "Scaling Issues of lm.ridge()\n\n\nfit$coef\n\n   cyl   disp     hp   drat     wt   qsec     vs     am   gear   carb \n-0.285  0.285 -1.008  0.487 -2.370  0.866  0.186  1.134  0.498 -0.915 \n\n\n\n\n# use n instead of (n-1) for standardization\nn <- nrow(X); X <- X * sqrt(n / (n - 1))\n\n\n\n        cyl  disp    hp  drat    wt  qsec    vs   am  gear   carb\n[1,] -0.285 0.285 -1.01 0.487 -2.37 0.866 0.186 1.13 0.498 -0.915\n\n\nMore discussion at https://stats.stackexchange.com/questions/288754/lm-ridge-returns-different-results-that-are-from-manual-calculation"
  },
  {
    "objectID": "slides/05-ridge-cv.html#ridge-trace",
    "href": "slides/05-ridge-cv.html#ridge-trace",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Ridge Trace",
    "text": "Ridge Trace\n\nridge_fit <- lm.ridge(mpg ~ ., data = mtcars, lambda = 0:40)\nmatplot(coef(ridge_fit)[, -1], type = \"l\", xlab = \"Lambda\", ylab = \"Coefficients\")\ntext(rep(1, 10), coef(ridge_fit)[1,-1], colnames(mtcars)[2:11])\n\n\n\nSelect a value of \\(\\lambda\\) at which the ridge estimates are stable."
  },
  {
    "objectID": "slides/05-ridge-cv.html#methods-for-choosing-lambda",
    "href": "slides/05-ridge-cv.html#methods-for-choosing-lambda",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Methods for Choosing \\(\\lambda\\)\n",
    "text": "Methods for Choosing \\(\\lambda\\)\n\n\nMASS::select(ridge_fit)\n\nmodified HKB estimator is 2.59 \nmodified L-W estimator is 1.84 \nsmallest value of GCV  at 15 \n\n\n\nHoerl, Kennard, and Baldwin (1975): \\(\\lambda \\approx \\frac{p \\hat{\\sigma}^2}{{\\bf b}'{\\bf b}}\\)\nLawless and Wang (1976): \\(\\lambda \\approx \\frac{np \\hat{\\sigma}^2}{{\\bf b'X}'{\\bf Xb}}\\)\nGolub, Health, and Wahba (1979): Generalized Cross Validation"
  },
  {
    "objectID": "slides/05-ridge-cv.html#cross-validation",
    "href": "slides/05-ridge-cv.html#cross-validation",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Cross Validation",
    "text": "Cross Validation\n\nCross Validation (CV) is a resampling method.\nResampling methods refit a model of interest to data sampled from the training set.\n\nCV can be used to\n\nestimate the test error when there is no test data. (Model assessment)\n\nselect the tuning parameters that controls the model complexity/flexibility. (Model selection)\n\n\n\n\nHence, by adding this to the OLS objective function, the solution is more stable, in the sense that each time we observe a new set of data, this contour looks pretty much the same. This may be interpreted in several different ways such as: 1) the objective function is more convex and less affected by the random samples; 2) the variance of the estimator is smaller because the eigenvalues of \\(\\mathbf{X}^\\text{T}\\mathbf{X}+ n \\lambda \\mathbf{I}\\) are large."
  },
  {
    "objectID": "slides/05-ridge-cv.html#k-fold-cross-validation",
    "href": "slides/05-ridge-cv.html#k-fold-cross-validation",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "\n\\(k\\)-Fold Cross Validation",
    "text": "\\(k\\)-Fold Cross Validation\n\nRandomly divide the training data into \\(k\\) folds, of approximately equal size.\nUse 1 fold for validation to compute MSE, and the remaining \\(k - 1\\) partitions for training.\nRepeat \\(k\\) times. Each time a different fold is treated as a validation set.\nAverage \\(k\\) metrics, \\(\\text{MSE}_{CV} = \\frac{1}{k}\\sum_{i=1}^k\\text{MSE}_i\\).\nUse the CV estimate \\(\\text{MSE}_{CV}\\) to select the ‚Äúbest‚Äù tuning parameter.\n\n\nFive-fold cross validation. Source: Data science in a boxCan compute other performance measures."
  },
  {
    "objectID": "slides/05-ridge-cv.html#glmnet-package",
    "href": "slides/05-ridge-cv.html#glmnet-package",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "\nglmnet package \n",
    "text": "glmnet package \n\n\nThe parameter alpha controls the ridge (alpha = 0) and lasso (alpha = 1) penalties.\nSupply a decreasing sequence of lambda values.\nlm.ridge() use \\(SS_{res}\\) and \\(n\\lambda\\), while glmnet() use \\(\\text{MSE}_{\\texttt{Tr}}\\) and \\(\\lambda\\).\nArgument x should be a matrix.\n\n\n\n\nlm.ridge(formula = mpg ~ ., \n         data = mtcars, \n         lambda = 5:0)\n\n\n\nglmnet(x = data.matrix(mtcars[, -1]), \n       y = mtcars$mpg, \n       alpha = 0, \n       lambda = 5:0/nrow(mtcars))\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nlm.ridge() and glmnet() coefficients do not match exactly, specially when transforming back to original scale.\nNo need to worry too much as we focus on predictive performance.\n\n\n\n\n\nhttps://stats.stackexchange.com/questions/74206/ridge-regression-results-different-in-using-lm-ridge-and-glmnet"
  },
  {
    "objectID": "slides/05-ridge-cv.html#k-fold-cross-validation-using-cv.glmnet",
    "href": "slides/05-ridge-cv.html#k-fold-cross-validation-using-cv.glmnet",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "\n\\(k\\)-Fold Cross Validation using cv.glmnet()1\n",
    "text": "\\(k\\)-Fold Cross Validation using cv.glmnet()1\n\n\nThe \\(\\lambda\\) values are automatically selected, on the \\(\\log_{e}\\) scale.\n\n\nridge_cv_fit <- cv.glmnet(x = data.matrix(mtcars[, -1]), y = mtcars$mpg, alpha = 0,\n                          nfolds = 10, type.measure = \"mse\")\nplot(ridge_cv_fit$glmnet.fit, \"lambda\")\n\n\n\n\n\n\n\n\nWhy s and not lambda? In case we want to allow one to specify the model size in other ways in the future. s: Value(s) of the penalty parameter lambda at which predictions are required. Default is the entire sequence used to create the model.\nThere are other ways to do CV for ridge regression in R, for example, the caret (Classification And REgression Training) package and the rsample package in tidymodels ecosystem."
  },
  {
    "objectID": "slides/05-ridge-cv.html#determine-lambda",
    "href": "slides/05-ridge-cv.html#determine-lambda",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Determine \\(\\lambda\\)\n",
    "text": "Determine \\(\\lambda\\)\n\n\n\n\nplot(ridge_cv_fit)\n\n\n\n\n\n\n\n\n\nridge_cv_fit$lambda.min\n\n[1] 2.75\n\n# largest lambda s.t. error is within 1 s.e of the min\nridge_cv_fit$lambda.1se \n\n[1] 16.1\n\ncoef(ridge_cv_fit, s = \"lambda.min\")\n\n11 x 1 sparse Matrix of class \"dgCMatrix\"\n                  s1\n(Intercept) 21.11734\ncyl         -0.37134\ndisp        -0.00525\nhp          -0.01161\ndrat         1.05477\nwt          -1.23420\nqsec         0.16245\nvs           0.77196\nam           1.62381\ngear         0.54417\ncarb        -0.54742\n\n\n\n\ncoef(fit2, s = ‚Äúlambda.1se‚Äù)"
  },
  {
    "objectID": "slides/05-ridge-cv.html#generalized-cross-validation",
    "href": "slides/05-ridge-cv.html#generalized-cross-validation",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Generalized Cross-Validation",
    "text": "Generalized Cross-Validation\n\nThe generalized cross-validation (GCV) is a modified version of the leave-one-out CV (\\(n\\)-fold CV).\nThe GCV criterion is given by \\[\\text{GCV}(\\lambda) = \\frac{1}{n}\\sum_{i=1}^n \\left[ \\frac{y_i - x_i' \\widehat{\\boldsymbol \\beta}^\\text{r}_\\lambda}{1 - \\text{Trace}(\\mathbf{S}_\\lambda)/n} \\right]^2\\]\n\nwhere \\(\\mathbf{S}_\\lambda\\) is the hat matrix corresponding to the ridge regression:\n\\[\\mathbf{S}_\\lambda = \\mathbf{X}(\\mathbf{X}' \\mathbf{X}+ n\\lambda \\mathbf{I})^{-1} \\mathbf{X}'\\]\nThe interesting fact about leave-one-out CV in the linear regression setting is that we do not need to explicitly fit all leave-one-out models.\n\nESL p.¬†244\nlm.ridge code"
  },
  {
    "objectID": "slides/05-ridge-cv.html#generalized-cross-validation-1",
    "href": "slides/05-ridge-cv.html#generalized-cross-validation-1",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Generalized Cross-Validation",
    "text": "Generalized Cross-Validation\n\n\n\nplot(ridge_fit$lambda, \n     ridge_fit$GCV, \n     type = \"l\", col = \"darkgreen\", \n     ylab = \"GCV\", xlab = \"Lambda\", \n     lwd = 3)\n\n\n\n\n\n\n\n\n\nidx <- which.min(ridge_fit$GCV)\nridge_fit$lambda[idx]\n\n[1] 15\n\nround(coef(ridge_fit)[idx, ], 2)\n\n        cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb \n21.13 -0.37 -0.01 -0.01  1.05 -1.23  0.16  0.77  1.62  0.54 -0.55 \n\n\n\n\nSelect the best \\(\\lambda\\) that produces the smallest GCV.\nYou can clearly see that the GCV decreases initially, as \\(\\lambda\\) increases, this is because the reduced variance is more beneficial than the increased bias. However, as \\(\\lambda\\) increases further, the bias term will eventually dominate and causing the overall prediction error to increase. The fitted MSE under this model is\n\n\n\n\n\n\nmssc6250-s24.github.io/website"
  },
  {
    "objectID": "slides/02-overview.html#imageobject-recognition",
    "href": "slides/02-overview.html#imageobject-recognition",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Image/Object Recognition",
    "text": "Image/Object Recognition\n\n\n\n\n\n\n\n\n\nMachine learning is everywhere.\nCan we train a machine to have an ability to recognize dogs and cats?\nhundreds of thousands of objects?"
  },
  {
    "objectID": "slides/02-overview.html#recommender-system",
    "href": "slides/02-overview.html#recommender-system",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Recommender System",
    "text": "Recommender System\n\n\nwatch one video on YouTube, then lots of other videos that never show up pop up.\nCheck some items on Amazon, then it recommends some other items saying ‚Äúyou may also like XXX‚Äù\nThis kind of recommendation relies on machine learning.\nHow does the model know you may also like those items. Well the model learn this from other customer surfing and purchasing history."
  },
  {
    "objectID": "slides/02-overview.html#covid-detection",
    "href": "slides/02-overview.html#covid-detection",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "COVID Detection",
    "text": "COVID Detection"
  },
  {
    "objectID": "slides/02-overview.html#stock-price-forecasting",
    "href": "slides/02-overview.html#stock-price-forecasting",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Stock Price Forecasting",
    "text": "Stock Price Forecasting"
  },
  {
    "objectID": "slides/02-overview.html#risk-factors-for-cancer",
    "href": "slides/02-overview.html#risk-factors-for-cancer",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Risk Factors for Cancer",
    "text": "Risk Factors for Cancer"
  },
  {
    "objectID": "slides/02-overview.html#how-we-solve-real-life-problems",
    "href": "slides/02-overview.html#how-we-solve-real-life-problems",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "How We Solve Real-Life Problems?",
    "text": "How We Solve Real-Life Problems?\n\nWe describe and formulate our problems/questions by models.\n\n\n\nSolve problems/Answer questions by doing inference and/or predictions from the built model using the information from the data and computer algorithms.\n\n\n\n\nSo what is Machine Learning?\n\n\nMathematical/statistical or machine learning models.\nwe know the eating habits from a data of 100 american people. How do we know the the eating habits of all the people in the united states.\nIf we know someone‚Äôs GPA and major, how can we predict his salary after graduation?"
  },
  {
    "objectID": "slides/02-overview.html#machine-learning-vs.-statistical-learning",
    "href": "slides/02-overview.html#machine-learning-vs.-statistical-learning",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Machine Learning vs.¬†Statistical Learning",
    "text": "Machine Learning vs.¬†Statistical Learning\n\n\nMachine learning (ML) is a field in Computer Science (CS).\n\n\n‚ÄúA computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.‚Äù ‚Äì Tom Mitchell, Professor of ML at Carnegie Mellon University\n\n\ncomputer programs = software and algorithms\nexperience = data\ntasks = problems\n\n\n\nThe ML algorithms are mainly for predictive modeling problems, and many have been borrowed from Statistics, for example, linear regression.\nML is a CS perspective on modeling data with a focus on algorithmic methods.\n\n\n\n\nWait, I am a statistical modeler (computational statistician), and this is exactly what I am doing!"
  },
  {
    "objectID": "slides/02-overview.html#machine-learning-vs.-statistical-learning-1",
    "href": "slides/02-overview.html#machine-learning-vs.-statistical-learning-1",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Machine Learning vs.¬†Statistical Learning",
    "text": "Machine Learning vs.¬†Statistical Learning\n\n\nStatistical learning, used to be called Applied Statistics (what a ‚Äúfancy‚Äù name, ü§£), arose as a subfield of Statistics.\n\n\n\nStatistical learning refers to a set of tools for modeling and understanding complex datasets. ‚Äì An Introduction to Statistical Learning\n\n\n‚Ä¶to extract important patterns and trends, and understand ‚Äúwhat the data says.‚Äù We call this learning from data. ‚Äì The Elements of Statistical Learning\n\n\ntools = mathematics, computing hardware/software/architecture, programming languages, algorithms, etc.\nStatistical learning is a mathematical perspective on modeling data with a focus on data models and on goodness of fit."
  },
  {
    "objectID": "slides/02-overview.html#machine-learning-vs.-statistical-learning-2",
    "href": "slides/02-overview.html#machine-learning-vs.-statistical-learning-2",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Machine Learning vs.¬†Statistical Learning",
    "text": "Machine Learning vs.¬†Statistical Learning\n\nMachine learning emphasizes algorithms and automation.\nStatistical learning emphasizes modeling, interpretability, and uncertainty.\n\n\n\nThe distinction is blur:\n\nA machine learner needs a well-built statistical model that quantifies uncertainty about prediction\nA statistical learner needs a computationally efficient algorithm to deal with large complex data.\n\n\n\n\n\n\nBUT, one thing for sure. Machine learning has the upper hand in marketing!"
  },
  {
    "objectID": "slides/02-overview.html#fancy-terms-and-larger-grant",
    "href": "slides/02-overview.html#fancy-terms-and-larger-grant",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Fancy Terms and Larger Grant!",
    "text": "Fancy Terms and Larger Grant!\n\n\n\n\n\n\n\nSource: http://statweb.stanford.edu/~tibs/stat315a/glossary.pdf"
  },
  {
    "objectID": "slides/02-overview.html#section-1",
    "href": "slides/02-overview.html#section-1",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "",
    "text": "Source: All of Statistics"
  },
  {
    "objectID": "slides/02-overview.html#types-of-learning",
    "href": "slides/02-overview.html#types-of-learning",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Types of Learning",
    "text": "Types of Learning"
  },
  {
    "objectID": "slides/02-overview.html#supervised-learning",
    "href": "slides/02-overview.html#supervised-learning",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nResponse Y (output, outcome, target, label, dependent/endogenous variable)\nVector of p predictors X = (X_1, X_2, \\dots, X_p) (inputs, features, regressors, covariates, explanatory/exogenous/independent variable).\n\n\n\nRegression: Y is numeric (e.g price, blood pressure). (if p = 1, simple regression; if p > 1, multiple regression)\nClassification: Y is categorical (e.g survived/died, digit 0-9, cancer class of tissue sample).\n\n\n\n\nTraining data \\mathcal{D} = \\{(x_1, y_1), \\dots ,(x_n , y_n)\\} =\\{(x_i, y_i)\\}_{i=1}^n, x_i \\in \\mathbf{R}^p.\n\n\n\nGoal: Use training data (E) to train our model for better inference/prediction on the response (T)\n\nLearn a mapping from inputs to outputs."
  },
  {
    "objectID": "slides/02-overview.html#regression-example",
    "href": "slides/02-overview.html#regression-example",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Regression Example",
    "text": "Regression Example\n\nGoal: Establish the relationship between salary and demographic variables.\n\n\nSource: ISL Fig. 1.1"
  },
  {
    "objectID": "slides/02-overview.html#statistics-nah-machine-learning-neat",
    "href": "slides/02-overview.html#statistics-nah-machine-learning-neat",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Statistics Nah‚Ä¶ Machine Learning Neat!",
    "text": "Statistics Nah‚Ä¶ Machine Learning Neat!\n\nSource: https://towardsdatascience.com/the-actual-difference-between-statistics-and-machine-learning-64b49f07ea3"
  },
  {
    "objectID": "slides/02-overview.html#statistics-nah-machine-learning-neat-1",
    "href": "slides/02-overview.html#statistics-nah-machine-learning-neat-1",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Statistics Nah‚Ä¶ Machine Learning Neat!",
    "text": "Statistics Nah‚Ä¶ Machine Learning Neat!"
  },
  {
    "objectID": "slides/02-overview.html#classification-example",
    "href": "slides/02-overview.html#classification-example",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Classification Example",
    "text": "Classification Example\n\nGoal: Build a customized spam filtering system\n\n\nSource: http://penplusbytes.org/strategies-for-dealing-with-e-mail-spam/"
  },
  {
    "objectID": "slides/02-overview.html#classification-example-1",
    "href": "slides/02-overview.html#classification-example-1",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Classification Example",
    "text": "Classification Example\n\n\n\n\nSource: https://www.yesware.com/blog/email-spam/\n\n\n\n\n\nData: 4601 emails sent to George at HP, before 2000. Each is labeled as spam or email.\nInputs: relative frequencies of 57 of the commonly occurring words and punctuation marks.\n\n\n\n\n\nSource: The elements of statistical learning"
  },
  {
    "objectID": "slides/02-overview.html#objectives-of-supervised-learning",
    "href": "slides/02-overview.html#objectives-of-supervised-learning",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Objectives of Supervised Learning",
    "text": "Objectives of Supervised Learning\nOn the basis of the training data (E) we‚Äôd like to :\n\n(T) Prediction: Accurately predict unseen test cases.\n\nGiven a new test input (age, year, education), what is predicted salary level?\nGiven a new bunch of words in the email, does the model classify the email correctly?\n\n\n\n\n\n(T) Inference Understand which inputs affect the outcome, and how.\n\nIf education is up one level, how much salary will increase on average?\nIf ‚Äú!‚Äù increases one more time, how much percentage does the probability of being labeled as spam go up?\n\n\n\n\n\n\n(P) Assess the quality of our predictions and inferences.\n\nUse evalution metrics to assess the performance of a machine learning model/algorithm."
  },
  {
    "objectID": "slides/02-overview.html#unsupervised-learning",
    "href": "slides/02-overview.html#unsupervised-learning",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\nNo outcome variable, just a set of features measured on a set of samples.\nOur training sample is \\mathcal{D} = \\{x_1, \\dots, x_n\\}, x_i \\in \\mathbf{R}^p.\n\n\n\nObjective is more fuzzy\n\nfind groups of samples or features that behave similarly (Clustering)\n\nfind linear combinations of features with the most variation (Dimension Reduction).\n\n\n\n\n\n\nDifficult to know how well you are doing.\nCan be useful as a pre-processing step for supervised learning."
  },
  {
    "objectID": "slides/02-overview.html#clustering-example",
    "href": "slides/02-overview.html#clustering-example",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Clustering Example",
    "text": "Clustering Example\n\nCustomer Segmentation: dividing customers into groups or clusters on the basis of common characteristics.\n\n\nSource: https://www.datacamp.com/community/tutorials/introduction-customer-segmentation-python"
  },
  {
    "objectID": "slides/02-overview.html#deep-learning",
    "href": "slides/02-overview.html#deep-learning",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Deep Learning",
    "text": "Deep Learning\n\nAn (artifical) neural network is a machine learning model inspired by the biological neural networks that constitute animal brains.\n\n\n\n\n\nSource: Wiki"
  },
  {
    "objectID": "slides/02-overview.html#section-2",
    "href": "slides/02-overview.html#section-2",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "",
    "text": "A neural network with several hidden layers is called a deep neural network, or deep learning.\n\nSource: ISL Ch 10"
  },
  {
    "objectID": "slides/02-overview.html#accuracy-and-interpretability-trade-off",
    "href": "slides/02-overview.html#accuracy-and-interpretability-trade-off",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Accuracy and Interpretability Trade-Off",
    "text": "Accuracy and Interpretability Trade-Off\n\n\n\n\nSource: ISL Fig. 2.7\n\n\n\n\n\nmssc6250-s24.github.io/website"
  },
  {
    "objectID": "computing-pipelines.html",
    "href": "computing-pipelines.html",
    "title": "Pipelines",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.1 ‚îÄ‚îÄ\n\n\n‚úì ggplot2 3.3.5     ‚úì purrr   0.3.4\n‚úì tibble  3.1.6     ‚úì dplyr   1.0.7\n‚úì tidyr   1.1.4     ‚úì stringr 1.4.0\n‚úì readr   2.1.1     ‚úì forcats 0.5.1\n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(tidymodels)\n\nRegistered S3 method overwritten by 'tune':\n  method                   from   \n  required_pkgs.model_spec parsnip\n\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels 0.1.4 ‚îÄ‚îÄ\n\n\n‚úì broom        0.7.10         ‚úì rsample      0.1.1     \n‚úì dials        0.0.10         ‚úì tune         0.1.6     \n‚úì infer        1.0.1.9000     ‚úì workflows    0.2.4     \n‚úì modeldata    0.1.1          ‚úì workflowsets 0.1.0     \n‚úì parsnip      0.1.7          ‚úì yardstick    0.0.9     \n‚úì recipes      0.2.0          \n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels_conflicts() ‚îÄ‚îÄ\nx scales::discard() masks purrr::discard()\nx dplyr::filter()   masks stats::filter()\nx recipes::fixed()  masks stringr::fixed()\nx dplyr::lag()      masks stats::lag()\nx yardstick::spec() masks readr::spec()\nx recipes::step()   masks stats::step()\n‚Ä¢ Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(knitr)"
  },
  {
    "objectID": "computing-pipelines.html#simple-linear-regression",
    "href": "computing-pipelines.html#simple-linear-regression",
    "title": "Pipelines",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\nModel fitting\nFit model:\n\npenguins_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(body_mass_g ~ flipper_length_mm, data = penguins)\n\nTidy model output:\n\ntidy(penguins_fit)\n\n# A tibble: 2 √ó 5\n  term              estimate std.error statistic   p.value\n  <chr>                <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)        -5781.     306.       -18.9 5.59e- 55\n2 flipper_length_mm     49.7      1.52      32.7 4.37e-107\n\n\nFormat model output as table:\n\ntidy(penguins_fit) %>%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5780.831\n305.815\n-18.903\n0\n\n\nflipper_length_mm\n49.686\n1.518\n32.722\n0\n\n\n\n\n\nAugment data with model:\n\naugment(penguins_fit$fit)\n\n# A tibble: 342 √ó 9\n   .rownames body_mass_g flipper_length_‚Ä¶ .fitted  .resid    .hat .sigma .cooksd\n   <chr>           <int>            <int>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl>\n 1 1                3750              181   3212.  538.   0.00881   394. 8.34e-3\n 2 2                3800              186   3461.  339.   0.00622   394. 2.33e-3\n 3 3                3250              195   3908. -658.   0.00344   393. 4.83e-3\n 4 5                3450              193   3808. -358.   0.00385   394. 1.60e-3\n 5 6                3650              190   3659.   -9.43 0.00469   395. 1.35e-6\n 6 7                3625              181   3212.  413.   0.00881   394. 4.91e-3\n 7 8                4675              195   3908.  767.   0.00344   393. 6.56e-3\n 8 9                3475              193   3808. -333.   0.00385   394. 1.39e-3\n 9 10               4250              190   3659.  591.   0.00469   394. 5.31e-3\n10 11               3300              186   3461. -161.   0.00622   395. 5.23e-4\n# ‚Ä¶ with 332 more rows, and 1 more variable: .std.resid <dbl>\n\n\n\n\nStatistical inference"
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Go to your Files tab, check the box next to the file you want to download, then click on the blue gear icon on the Files tab to reveal the drop down menu, and select Export‚Ä¶ If you have selected multiple files to export, RStudio will zip them up into a single zip file for you. If you‚Äôve selected just a single file, it will only download that. The downloaded file will go to wherever files you download off the internet goes on your computer (usually your Downloads folder)."
  },
  {
    "objectID": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "href": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "title": "FAQ",
    "section": "How can I submit my assignment to Gradescope?",
    "text": "How can I submit my assignment to Gradescope?\nThe instructions for submitting your assignment to Gradescope can be found here. In a nutshell, you‚Äôll upload your PDF and them mark the page(s) where each question can be found. It‚Äôs OK if a question spans multiple pages, just mark them all. It‚Äôs also OK if a page includes multiple questions."
  },
  {
    "objectID": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "href": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "title": "FAQ",
    "section": "Can I use a local install of R and RStudio instead of using the RStudio containers?",
    "text": "Can I use a local install of R and RStudio instead of using the RStudio containers?\nThe short answer is, I‚Äôd rather you didn‚Äôt, to save yourself some headache. But, the long answer is, sure! But you will need to install a specific versions of R and RStudio for everything to work as expected. You will also need to install the R packages we‚Äôre using as well as have Git installed on your computer. These are not extremely challenging things to get right, but they are not trivial either, particularly on certain operating systems. Myself and the TAs are always happy to provide help with any computational questions when you‚Äôre working in the containers we have provided for you. If you‚Äôre working on your local setup, we can‚Äôt guarantee being able to resolve your issues, though we‚Äôre happy to try.\nIf you want to take this path, here is what you need to do:\n\nDownload and install R 4.1.2: https://cran.r-project.org/\nDownload and install a daily build of RStudio: https://dailies.rstudio.com/\nInstall Quarto CLI: https://quarto.org/docs/getting-started/installation.html\nInstall Git: https://happygitwithr.com/install-git.html\nInstall any necessary packages with install.packages(\"___\")\n\nAnd I‚Äôd like to reiterate again that successful installation of these software is not a learning goal of this course. So if any of this seems tedious or intimidating in any way, just use the computing environment we have set up for you. More on that here."
  },
  {
    "objectID": "project-description-4780.html",
    "href": "project-description-4780.html",
    "title": "MATH 4780 Project Description",
    "section": "",
    "text": "The project accounts for 220 points of the total 1000 points."
  },
  {
    "objectID": "project-description-4780.html#team-up",
    "href": "project-description-4780.html#team-up",
    "title": "MATH 4780 Project Description",
    "section": "Team up!",
    "text": "Team up!\n\nEach one of you loses 20 points of your project grade if you don‚Äôt meet the requirement or miss the deadline.\nYou will be randomly assigned to a group if you do not belong to any group before the deadline."
  },
  {
    "objectID": "project-description-4780.html#proposal",
    "href": "project-description-4780.html#proposal",
    "title": "MATH 4780 Project Description",
    "section": "Proposal",
    "text": "Proposal\n\nEach one of you loses 20 points of your project grade if you don‚Äôt meet the requirement or miss the deadline.\nAlthough it is risky, you can change your project topic after you submit your proposal if you decide to do something else."
  },
  {
    "objectID": "project-description-4780.html#presentation",
    "href": "project-description-4780.html#presentation",
    "title": "MATH 4780 Project Description",
    "section": "Presentation",
    "text": "Presentation\n\nYou need to participate (in-person) in the final presentation in order to pass the course.\nEach group presentation should be between 14 and 16 minute long, followed by 1 to 3 minute Q&A. If your presentation is too short or too long, every one of you loses 10 points of your project grade.\nEvery group member has to present some part of the group work. The one who does not present receives 0 point.\nThe order of presentation is determined by random sampling using the function .\nQuestions are encouraged during Q&A. Everyone is welcome to ask any questions about the projects.\nEach group is required to ask as least one question. After the 1st group presentation, the 2nd group should ask at least one question in Q&A. After the 2nd group presentation, the 3rd group should at least ask one question in Q&A, and so on and so forth. The 1st group will ask the last group questions about their project. If you, as a group, don‚Äôt ask a question when you should, every one of you loses 10 points of your project grade."
  },
  {
    "objectID": "project-description-4780.html#materials",
    "href": "project-description-4780.html#materials",
    "title": "MATH 4780 Project Description",
    "section": "Materials",
    "text": "Materials\n\nEach one of you loses 20 points of your project grade if you don‚Äôt meet the requirement or miss the deadline.\nYou need to share your entire work, including slides, code, and data if applicable.\nYour code should be able to reproduce all the numerical results, outputs, tables, and figures shown in the slides, including the source of the raw data (where you find and load the data) if the project is about data analysis."
  },
  {
    "objectID": "project-description-4780.html#group-performance-evaluation",
    "href": "project-description-4780.html#group-performance-evaluation",
    "title": "MATH 4780 Project Description",
    "section": "Group Performance Evaluation",
    "text": "Group Performance Evaluation\n\n\nMSSC 5780 students and Dr.¬†Yu evaluate your performance based on the four criteria:\n\nContent and Organization (8 pts)\nSlides Quality (4 pts)\nOral Presentation Skill and Delivery (4 pts)\nInteractions and Q&A (4 pts)\n\n\n\n\n\n\n\nContent and Organization (Data Analysis)\n\nBeautiful visualization helps find out relationship of variables and specification of models\nAll questions are answered accurately by the models\nDiscuss how and why the models are chosen\nApply sophisticated models and detailed analysis\nAll ideas are presented in logical order\n\nSlides Quality\n\nSlides show code and output beautifully\nSlides clearly aid the speaker in telling a coherent story\nAll tables and graphics are informative and related to the topic and make it easier to understand\nAttractive design, layout, and neatness.\n\nOral Presentation Skill\n\nGood volume and energy\nProper pace and diction\nAvoidance of distracting gestures\n\nInteractions and Q&A\n\nGood eye contact with audience\nExcellent listening skills\nAnswers audience questions with authority and accuracy"
  },
  {
    "objectID": "project-description-4780.html#best-contributor-nomination",
    "href": "project-description-4780.html#best-contributor-nomination",
    "title": "MATH 4780 Project Description",
    "section": "Best Contributor Nomination",
    "text": "Best Contributor Nomination\n\n\nYou nominate one single person who you think contributes the most to your group project.\nYou cannot nominate yourself, and you can only vote for one of your teammates.\nIf you don‚Äôt vote, you can‚Äôt be the best contributor even if you obtain the most votes. The person with the second highest votes wins the best contribution reward.\nIf there is no one single person who gets the most votes, every team member remains the same grade. For example, if your group finish in third place, and there is no best contributor, all members receive 200 points.\n\nDr.¬†Yu reserves the right to make changes to the project policy."
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course support",
    "section": "",
    "text": "Most of you will need help at some point and we want to make sure you can identify when that is without getting too frustrated and feel comfortable seeking help."
  },
  {
    "objectID": "course-support.html#lectures-and-labs",
    "href": "course-support.html#lectures-and-labs",
    "title": "Course support",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone."
  },
  {
    "objectID": "course-support.html#office-hours",
    "href": "course-support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage each and every one of you to take advantage of this resource! Make a pledge to stop by office hours at least once during the first three weeks of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself. You can find a list of everyone‚Äôs office hours here."
  },
  {
    "objectID": "course-support.html#discussion-forum",
    "href": "course-support.html#discussion-forum",
    "title": "Course support",
    "section": "Discussion forum",
    "text": "Discussion forum\nHave a question that can‚Äôt wait for office hours? Prefer to write out your question in detail rather than asking in person? The online discussion forum is the best venue for these! We will use Conversations as the online discussion forum. There is a chance another student has already asked a similar question, so please check the other posts on Ed Discussion before adding a new question. If you know the answer to a question that is posted, I encourage you to respond!"
  },
  {
    "objectID": "course-support.html#email",
    "href": "course-support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nPlease refrain from emailing any course content questions (those should go Conversations), and only use email for questions about personal matters that may not be appropriate for the public course forum (e.g., illness, accommodations, etc.). For such matters, you may email Dr.¬†Mine √áetinkaya-Rundel at mc301@duke.edu.\nIf there is a question that‚Äôs not appropriate for the public forum, you are welcome to email me directly. If you email me, please include ‚ÄúSTA 210‚Äù in the subject line. Barring extenuating circumstances, I will respond to STA 210 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday."
  },
  {
    "objectID": "course-support.html#academic-support",
    "href": "course-support.html#academic-support",
    "title": "Course support",
    "section": "Academic support",
    "text": "Academic support\nThere are times you may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Academic Resource Center. The Academic Resource Center (ARC) offers free services to all students during their undergraduate careers at Duke. Services include Learning Consultations, Peer Tutoring and Study Groups, ADHD/LD Coaching, Outreach Workshops, and more. Because learning is a process unique to every individual, they work with each student to discover and develop their own academic strategy for success at Duke. ARC services are available free to any Duke undergraduate student, in any year, studying in any discipline. Contact the ARC to schedule an appointment. Undergraduates in any year, studying any discipline can benefit! Contact theARC@duke.edu, 919-684-5917."
  },
  {
    "objectID": "course-support.html#mental-health-and-wellness",
    "href": "course-support.html#mental-health-and-wellness",
    "title": "Course support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\nStudent mental health and wellness is of primary importance at Duke, and the university offers resources to support students in managing daily stress and self-care. Duke offers several resources for students to seek assistance on coursework and to nurture daily habits that support overall well-being, some of which are listed below:\n\nThe Academic Resource Center: (919) 684-5917, theARC@duke.edu, or arc.duke.edu,\nDuWell: (919) 681-8421, duwell@studentaffairs.duke.edu, or studentaffairs.duke.edu/duwell\n\nIf your mental health concerns and/or stressful events negatively affect your daily emotional state, academic performance, or ability to participate in your daily activities, many resources are available to help you through difficult times. Duke encourages all students to access these resources.\n\nDukeReach. Provides comprehensive outreach services to identify and support students in managing all aspects of well-being. If you have concerns about a student‚Äôs behavior or health visit the website for resources and assistance. studentaffairs.duke.edu/dukereach\nCounseling and Psychological Services (CAPS). CAPS helps Duke Students enhance strengths and develop abilities to successfully live, grow and learn in their personal and academic lives. CAPS recognizes that we are living in unprecedented times and that the changes, challenges and stressors brought on by the COVID-19 pandemic have impacted everyone, often in ways that are tax our well-being. CAPS offers many services to Duke undergraduate students, including brief individual and group counseling, couples counseling and more. CAPS staff also provides outreach to student groups, particularly programs supportive of at-risk populations, on a wide range of issues impacting them in various aspects of campus life. CAPS provides services to students via Telehealth. To initiate services, you can contact their front desk at 919-660-1000. studentaffairs.duke.edu/caps\nBlue Devils Care. A convenient and cost-effective way for Duke students to receive 24/7 mental health support through TalkNow and scheduled counseling. bluedevilscare.duke.edu\nTwo-Click Support. Duke Student Government and DukeReach partnership that connects students to help in just two clicks. bit.ly/TwoClickSupport\nWellTrack. Sign up for WellTrack at app.welltrack.com."
  },
  {
    "objectID": "course-support.html#technology-accommodations",
    "href": "course-support.html#technology-accommodations",
    "title": "Course support",
    "section": "Technology accommodations",
    "text": "Technology accommodations\nStudents with demonstrated high financial need who have limited access to computers may request assistance in the form of loaner laptops. For technology assistance requests, please go here. Please note that supplies are limited.\nNote that we will be using Duke‚Äôs computational resources in this course. These resources are freely available to you. As long as your computer can connect to the internet and open a browser window, you can perform the necessary computing for this course. All software we use is open-source and/or freely available."
  },
  {
    "objectID": "course-support.html#course-materials-costs",
    "href": "course-support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.)."
  },
  {
    "objectID": "course-support.html#assistance-with-zoom-or-sakai",
    "href": "course-support.html#assistance-with-zoom-or-sakai",
    "title": "Course support",
    "section": "Assistance with Zoom or Sakai",
    "text": "Assistance with Zoom or Sakai\nFor technical help with Sakai or Zoom, contact the Duke OIT Service Desk at oit.duke.edu/help. You can also access the self-service help documentation for Zoom here and for Sakai here.\nNote that we will be making minimal use of Sakai in this course (primarily for announcements and grade book). All assignment submission and discussion will take place on GitHub instead.\nZoom will be used for online office hours as well as as a backup option should we need to hold the course online instead of in person."
  },
  {
    "objectID": "project-description.html",
    "href": "project-description.html",
    "title": "MSSC 6250 Project Description",
    "section": "",
    "text": "To be announced."
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Click here to download the syllabus."
  },
  {
    "objectID": "course-syllabus.html#time-and-location",
    "href": "course-syllabus.html#time-and-location",
    "title": "Syllabus",
    "section": "Time and location",
    "text": "Time and location\n\n\n\n\nDay\nTime\nLocation\n\n\n\n\nLectures\nTu & Th\n3:30 - 4:45 PM\nCuday Hall 143\n\n\nLab\nNone\nNone\nNone"
  },
  {
    "objectID": "course-syllabus.html#office-hours",
    "href": "course-syllabus.html#office-hours",
    "title": "Syllabus",
    "section": "Office Hours",
    "text": "Office Hours\n\nMy in-person office hours are TuTh 4:50 - 5:50 PM, and Wed 12 - 1 PM in Cudahy Hall room 353.\nYou are welcome to schedule an online meeting via Microsoft Teams if you need/prefer."
  },
  {
    "objectID": "course-syllabus.html#prerequisites",
    "href": "course-syllabus.html#prerequisites",
    "title": "Syllabus",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nMATH 4720 (Intro to Statistics), MATH 3100 (Linear Algebra) and MATH 4780 (Regression Analysis)\nHaving taken MATH 4700 (Probability) and MATH 4710 (Statistical Inference) or more advanced ones is strongly recommended.\nThis course is supposed to be taken in the last semester for the applied statistics (APST) master students. Talk to me if you are not sure whether or not this is the right course for you."
  },
  {
    "objectID": "course-syllabus.html#e-mail-policy",
    "href": "course-syllabus.html#e-mail-policy",
    "title": "Syllabus",
    "section": "E-mail Policy",
    "text": "E-mail Policy\n\nI will attempt to reply your email quickly, at least within 24 hours.\nExpect a reply on Monday if you send a question during weekends. If you do not receive a response from me within two days, re-send your question/comment in case there was a ‚Äúmix-up‚Äù with email communication (Hope this won‚Äôt happen!).\nPlease start your subject line with [mssc6250] followed by a clear description of your question. See an example below.\n\n\n\n\nEmail Subject Line Example\n\n\n\nEmail etiquette is important. Please read this article to learn more about email etiquette.\nI am more than happy to answer your questions about this course or data science/statistics in general. However, with tons of email messgaes everyday, I may choose NOT to respond to students‚Äô e-mail if\n\nThe student could answer his/her own inquiry by reading the syllabus or information on the course website or D2L.\nThe student is asking for an extra credit opportunity. The answer is ‚Äúno‚Äù.\nThe student is requesting an extension on homework. The answer is ‚Äúno‚Äù.\nThe student is asking for a grade to be raised for no legitimate reason. The answer is ‚Äúno‚Äù.\nThe student is sending an email with no etiquette."
  },
  {
    "objectID": "course-syllabus.html#required-textbook",
    "href": "course-syllabus.html#required-textbook",
    "title": "Syllabus",
    "section": "Required Textbook",
    "text": "Required Textbook\n\n(ISL) An Introduction to Statistical Learning, by James et al.¬†Publisher: Springer. (Undergraduate to master level, R and Python code)"
  },
  {
    "objectID": "course-syllabus.html#optional-references",
    "href": "course-syllabus.html#optional-references",
    "title": "Syllabus",
    "section": "Optional References",
    "text": "Optional References\n\n(PML) Probabilistic Machine Learning: An Introduction, by Kevin Murphy. Publisher: MIT Press. (Master to PhD level, lots of mathematics foundations, Python code)\n(PMLA) Probabilistic Machine Learning: Advanced Topics, by Kevin Murphy. Publisher: MIT Press. (PhD level, more probabilistic-based or Bayesian)\n(ESL) The Elements of Statistical Learning, 2nd edition, by Hastie et. al.¬†Publisher: Springer. (PhD level, more frequentist-based)"
  },
  {
    "objectID": "course-syllabus.html#grading-policy",
    "href": "course-syllabus.html#grading-policy",
    "title": "Syllabus",
    "section": "Grading Policy",
    "text": "Grading Policy\n\nYour final grade is earned out of 1000 total points distributed as follows:\n\nHomework: 500 pts\nIn-class Activity: 200 pts\nFinal project presentation and/or written report: 300 pts\n\n\nYou will NOT be allowed any extra credit projects/homework/exam to compensate for a poor average. Everyone must be given the same opportunity to do well in this class. Individual exam will NOT be curved. \nThe final grade is based on your percentage of points earned out of 1000 points and the grade-percentage conversion Table. \\([x, y)\\) means greater than or equal to \\(x\\) and less than \\(y\\). For example, 94.1 is in \\([93, 100]\\) and the grade is A and 92.8 is in \\([90, 94)\\) and the grade is A-.\n\n\n\n\nGrade-Percentage Conversion\n\n\nGrade\nPercentage\n\n\n\n\nA\n[94, 100]\n\n\nA-\n[90, 94)\n\n\nB+\n[87, 90)\n\n\nB\n[83, 87)\n\n\nB-\n[80, 83)\n\n\nC+\n[77, 80)\n\n\nC\n[70, 77)\n\n\nF\n[0, 70)\n\n\n\n\n\n\n\nYou may use any programming language to complete your homework and/or your project.\n\n\nHomework\n\n\nHomework will be assigned through the course website in weekly modules.\nTo submit your homework, please go to D2L > Assessments > Dropbox and upload your homework in PDF format.\nNo late or make-up homework for any reason.\n\n\n\nIn-Class Activity\n\nThere will be 3 to 4 in-class activities.\nStudents will learn from each other by presenting and discussing the assigned topics.\nMore details about the in-class activities will be released later.\n\n\n\nProject\n\nThe final project includes two parts: written report and oral presentation.\nYou need to participate (in-person) in the final presentation in order to pass the course.\nThe final project presentation is on Thursday, 5/9, 8 - 10 AM.\nMore details about the written report and oral presentation will be released later."
  },
  {
    "objectID": "course-syllabus.html#sharingreusing-code-policy",
    "href": "course-syllabus.html#sharingreusing-code-policy",
    "title": "Syllabus",
    "section": "Sharing/Reusing Code Policy",
    "text": "Sharing/Reusing Code Policy\n\nUnless explicitly stated otherwise, you may make use of any online resources, but you must explicitly cite where you obtained any code you directly use or use as inspiration in your solutions.\nAny recycled code that is discovered and is not explicitly cited will be treated as plagiarism, regardless of source."
  },
  {
    "objectID": "course-syllabus.html#university-and-college-policies",
    "href": "course-syllabus.html#university-and-college-policies",
    "title": "Syllabus",
    "section": "University and college policies",
    "text": "University and college policies\nAs a student in this course, you have agreed to comply with Marquette undergraduate policies and regulations."
  },
  {
    "objectID": "course-syllabus.html#accommodation",
    "href": "course-syllabus.html#accommodation",
    "title": "Syllabus",
    "section": "Accommodation",
    "text": "Accommodation\nIf you need to request accommodations, or modify existing accommodations that address disability-related needs, please contact Disability Service."
  },
  {
    "objectID": "course-syllabus.html#important-dates",
    "href": "course-syllabus.html#important-dates",
    "title": "Syllabus",
    "section": "Important dates",
    "text": "Important dates\n\nJan 24: Last day to add/swap/drop\nMar 10-16: Spring break\nMar 12: Midterm grade submission\nMar 28 - Apr 1: Easter break\nApr 12: Withdrawal deadline\nMay 4: Last day of class\nMay 9: Final project presentation/report submission\nMay 14: Final grade submission\n\nClick here for the full Marquette academic calendar."
  },
  {
    "objectID": "computing-troubleshooting.html",
    "href": "computing-troubleshooting.html",
    "title": "Computing troubleshooting",
    "section": "",
    "text": "If you‚Äôre having difficulty launching an RStudio session from your reserved container, go to status.oit.duke.edu and scroll down to Teaching and Learning Tools. Under this heading you‚Äôll find an entry called Container Manager (CMGR Coursework Containers).\n\nIf the status shows something other than Operational, this means there is a known incident with the containers. Check back later to see if it‚Äôs been resolved. If there‚Äôs a deadline coming up soon, post on the course forum to let us know that there‚Äôs an issue. We can look into how quickly it might get resolved and decide on what to do about the deadline accordingly. Note: We don‚Äôt anticipate this to happen regularly, the systems are Operational a huge majority of the time!\nIf the status shows Operational, this means the system is expected to be working. Check your internet connection, if need be, restart your computer to ensure a fresh new connection. If your issue persists, post on the course forum with details on what you‚Äôve tried and the errors you see (including verbatim errors and/or screenshots).\nEither way you can also fill out the form here, which will notify our the R TA for the department as well as our undergraduate coordinator. They‚Äôll be able to help diagnose the issue."
  }
]