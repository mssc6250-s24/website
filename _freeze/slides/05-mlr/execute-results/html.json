{
  "hash": "111c34060696bae086013ab219c58b2f",
  "result": {
    "markdown": "---\ntitle: \"Multiple Linear Regression üë®‚Äçüíª\"\nsubtitle: \"MATH 4780 / MSSC 5780 Regression Analysis\"\nauthor: \"Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University\"\n# date: \"September 19 2023\"\n# macros: _macros.tex # import a list of TeX/LaTeX definitions\nformat: \n  revealjs:\n    code-line-numbers: false\n    #     - \"macros.tex\"\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n    # include-in-header:\n    highlight-style: arrow\n    code-block-bg: true\n    self-contained: false\n    slide-number: c/t    \n    incremental: false\n    width: 1800\n    height: 1000\n    margin: 0.05\n    logo: \"https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg\"\n    footer: \"[math4780-f23.github.io/website](https://math4780-f23.github.io/website/)\"\n    theme: [\"simple\", \"slides.scss\"]\n    multiplex: true\n    code-link: true\n    fig-cap-location: bottom\n    fig-align: center\n    transition: none ## fade slide convex concave zoom\n    title-slide-attributes:\n      data-background-color: \"#447099\"\n      # data-background-image: images/paper-texture.jpg\n      # data-background-size: cover\n      # data-background-color: \"#698ED5\"\neditor: source\nexecute:\n  freeze: true\n  echo: false\n  purl: true\n---\n\n\n#  {visibility=\"hidden\"}\n\n\\def\\bx{\\mathbf{x}}\n\\def\\bg{\\mathbf{g}}\n\\def\\bw{\\mathbf{w}}\n\\def\\bbeta{\\boldsymbol \\beta}\n\\def\\bX{\\mathbf{X}}\n\\def\\by{\\mathbf{y}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bW{\\mathbf{W}}\n\\def\\T{\\text{T}}\n\\def\\cov{\\mathrm{Cov}}\n\\def\\cor{\\mathrm{Corr}}\n\\def\\var{\\mathrm{Var}}\n\\def\\E{\\mathrm{E}}\n\\def\\bmu{\\boldsymbol \\mu}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\def\\Trace{\\text{Trace}}\n\n\n\n\n\n\n\n\n# Multiple Linear Regression Model (MLR)\n\n\n::: notes\n- All right, we are finally going to talk about MLR. As I mentioned before, MLR is pretty like SLR. \n- They share the same idea and concept in terms of prediction and inference. \n- The major different is that MLR uses more than one regressor in the model, and we usually use matrix notation to describe a MLR and do calculations.\n- It does not mean that we cannot use the math notations used in SLR to express a MLR model. But you'll find out it is much more tedious if we don't use matrix, and using matrix algebra makes our life easier.\n:::\n\n\n## Why Multiple Regression?\n- Our target response may be affected by several factors.\n- Total sales $(Y)$ and amount of money spent on advertising on YouTube (YT) $(X_1)$, Facebook (FB) $(X_2)$, Instagram (IG) $(X_3)$.\n\n\n<!-- ![](./img/tv.jpeg) ![](./img/yt.jpeg) -->\n\n\n:::: {.columns}\n\n::: {.column width=\"33.3%\"}\n<br>\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/05-mlr/yt.svg){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: {.column width=\"33.3%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/05-mlr/fb.jpeg){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: {.column width=\"33.3%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/05-mlr/ig.jpeg){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n::::\n\n- Predict sales based on the three advertising expenditures and see which medium is more effective.\n\n\n\n::: notes\n\n- The first question you may ask is Why we want Multiple Regression?\n- In practice, we often have more than one predictor. \n- How amount of money spent on advertising on different media affect the total sales of some product, we may need more than one predictors. \n- Because usually company will spend money on several different media, not just one.\n- Data: total sales $(Y)$ and amount of money spent on advertising on TV $(X_1)$, YouTube $(X_2)$, and Instagram $(X_3)$.\n- We want to predict sales based on the three advertising expenditures and see which medium is more effective.\n\n:::\n\n## Fit Separate Simple Linear Regression Models\n\nFit three separate independent SLR models:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/05-mlr/unnamed-chunk-5-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n. . .\n\n‚ùå Fitting a separate SLR model for each predictor is not satisfactory. \n\n::: notes\n\n- You may wonder, how about we just fit three Separate Simple Linear Regression Models, one for each predictor. \n- Yes, we could do that. And if we do this, we'll see that advertising on the 3 media is valuable because the more the money we put in, the higher sales of products we'll get.\n- Fitting a separate SLR model for each predictor is not satisfactory. That's see why.\n\n:::\n\n## Don't Fit a Separate Simple Linear Regression\n\n- üëâ How to make a *single* prediction of sales given levels of the 3 advertising media budgets?\n  + <span style=\"color:blue\"> How to predict the sales when the amount spent on YT is 50, 100 on FB and 30 on IG? </span>\n  \n  \n. . .\n\n- üëâ Each regression equation ignores the other 2 media in forming coefficient estimates.\n  + <span style=\"color:blue\"> The effect of FB advertising on sales may be increased or decreased when YT and IG advertising are in the model. </span>\n  + <span style=\"color:blue\"> IG advertising may have no impact on sales when YT and FB advertising are in the model. </span>\n\n\n\n. . .\n\n- üëçüëç Better approach: *extend the SLR model so that it can __directly accommodate multiple predictors__.*\n\n::: notes\n\n- Fitting a separate SLR model for each predictor is not satisfactory. \n  + It is unclear how to make a single prediction of sales given levels of the three advertising media budgets, since each of the budgets is associated with a separate regression equation.\n  + Each of the three regression equations ignores the other two media in forming estimates for the regression coefficients.\n  + If the three media budgets are correlated with each other, this can lead to very misleading estimates of the individual media effects on sales.\n- A better approach is to extend the simple linear regression model so that it can directly accommodate multiple predictors.  \n\n:::\n\n\n##\n\n**I hope you don't feel...**\n\n::: small\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: https://memegenerator.net/instance/62248186/first-world-problems-multiple-regression-more-like-multiple-depression](./images/05-mlr/mlr_depress.jpeg){fig-align='center' width=46%}\n:::\n:::\n\n\n:::\n\n\n\n##\n\n**What I hope is...**\n\n::: small\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![https://www.tldrpharmacy.com/content/how-to-be-awesome-at-biostatistics-and-literature-evaluation-part-iii](./images/05-mlr/mlr_meme.jpeg){fig-align='center' width=82%}\n:::\n:::\n\n:::\n\n\n\n## Multiple Linear Regression (MLR) Model\n- We have $k$ distinct predictors. The (population) multiple linear regression model:\n$$Y_i= \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\dots + \\beta_kX_{ik} + \\epsilon_i$$\n- $X_{ij}$: $j$-th regressor value on $i$-th measurement, $j = 1, \\dots, k$.\n- $\\beta_j$: $j$-th coefficient quantifying the association between $X_j$ and $Y$.\n\n. . .\n\n- In the advertising example, $k = 3$ and\n$$\\texttt{sales} = \\beta_0 + \\beta_1 \\times \\texttt{YouTube} + \\beta_2 \\times  \\texttt{Facebook} + \\beta_3 \\times \\texttt{Instagram} + \\epsilon$$\n\n. . .\n\n- Assumptions as SLR\n  + $\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)$\n  \n\n. . .\n\n- When $k = 1$, MLR is reduced to SLR.\n\n::: notes\n- Two subscripts of X.\n- Later, we will learn how to interpret the coefficients correctly.\n- We interpret $\\beta_j$, $j = 1, \\dots, p$, as the average effect on $Y$ of a one unit increase in $X_j$, **holding all other predictors fixed**.\n:::\n\n. . .\n\n::: question\nHow many parameters are there in the model?\n:::\n\n\n## Sample MLR Model\n- Given the training sample data $(x_{11}, \\dots, x_{1k}, y_1), (x_{21}, \\dots, x_{2k}, y_2), \\dots, (x_{n1}, \\dots, x_{nk}, y_n),$\n- The sample MLR model:\n$$\\begin{align}\ny_i &= \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_k x_{ik} + \\epsilon_i \\\\\n&= \\beta_0 + \\sum_{j=1}^k\\beta_j x_{ij} + \\epsilon_i, \\quad i = 1, 2, \\dots, n\n\\end{align}$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/05-mlr/mlr_data_matrix.png){fig-align='center' width=48%}\n:::\n:::\n\n\n\n::: notes\n- Writing our data and model without using matrix starts getting tedious.\n- Later we'll see our to write it in a more clean way using matrix notation.\n- our mpg data set in the homework has this type of structure.\n- In R we usually store this kind of data set in R structure call data frame.\n:::\n\n\n## Regression Hyperplane\n- **SLR**: regression line\n- **MLR: regression hyperplane or response surface**\n- $y = \\beta_0 + \\beta_1x_1 + \\beta_{2}x_2 + \\epsilon$\n- $E(y \\mid x_1, x_2) = 50 + 10x_1 + 7x_2$\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/05-mlr/unnamed-chunk-10-1.png){fig-align='center' width=110%}\n:::\n:::\n\n:::\n\n\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/05-mlr/unnamed-chunk-11-1.png){fig-align='center' width=110%}\n:::\n:::\n\n:::\n::::\n\n\n::: notes\n- Now I'm gonna show you what a MLR looks like when we fit it to the data.\n- If we have two predictors, we will have a sample regression plane.\n- If we have more than two predictors in the model, we are not able to visualize it, but the idea is the same.\n- We will have a something called hyperplane or response surface that basically play the same role as the regression plane in 2D or regression line in 1D.\n- The plot on the right is the contour plot when we project the plot onto the x1-x2 plane. You can see that basically the higher x1 and/or the higher x2, the higher value of y.\n- Moreover, you can see that the level curves are straight and parallel, meaning that the effect of x1 on y does not change with the values of x2 or the effect does not depend on the level of x2.\n\n- https://cran.r-project.org/web/packages/rsm/vignettes/rsm-plots.pdf\n- http://www.sthda.com/english/wiki/scatterplot3d-3d-graphics-r-software-and-data-visualization \n- https://stackoverflow.com/questions/18147595/plot-3d-plane-true-regression-surface\n\n:::\n\n\n## Response Surface : Interaction Model\n- $y = \\beta_0 + \\beta_1x_1 + \\beta_{2}x_2 + \\beta_{12}x_1x_2 + \\epsilon$\n- This is in fact a linear regression model: let $\\beta_3 = \\beta_{12}, x_3 = x_1x_2$.\n- $E(y \\mid x_1, x_2) = 50 + 10x_1 + 7x_2 + 5x_1x_2$\n- üòé ü§ì **A linear model generates a nonlinear response surface!**\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/05-mlr/unnamed-chunk-12-1.png){fig-align='center' width=110%}\n:::\n:::\n\n:::\n\n\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/05-mlr/unnamed-chunk-13-1.png){fig-align='center' width=110%}\n:::\n:::\n\n:::\n::::\n\n\n\n::: notes\n- Remember in SLR, we can have a liner model that describes a nonlinear relationship.\n- Same in MLR. We can have a linear model that generates a nonlinear response surface!\n- $y = \\beta_0 + \\beta_1x_1 + \\beta_{2}x_2 + \\beta_{12}x_1x_2 + \\epsilon$\n- This is in fact a linear regression model: let $\\beta_3 = \\beta_{12}, x_3 = x_1x_2$.\n:::\n\n\n\n\n## Response Surface : 2nd Order with Interaction Model\n- $y = \\beta_0 + \\beta_1x_1 + \\beta_{2}x_2 + \\beta_{11}x_1^2 + \\beta_{22}x_2^2 + \\beta_{12}x_1x_2 + \\epsilon$\n- $E(y) = 800+10x_1+7x_2 -8.5x_1^2-5x_2^2- 4x_1x_2$\n- üòé ü§ì A **linear** regression model can describe a **complex nonlinear relationship** between the response and predictors!\n\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/05-mlr/unnamed-chunk-14-1.png){fig-align='center' width=110%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/05-mlr/unnamed-chunk-15-1.png){fig-align='center' width=110%}\n:::\n:::\n\n:::\n::::\n\n\n\n\n# Point Estimation of Model Parameters\n\n\n\n## Least Square Estimation of the Coefficients\n\n$$\\begin{align}\ny_i &= \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_k x_{ik} + \\epsilon_i \\\\\n&= \\beta_0 + \\sum_{j=1}^k\\beta_j x_{ij} + \\epsilon_i, \\quad i = 1, 2, \\dots, n\n\\end{align}$$\n\n- The least-squares function is\n$$S(\\alpha_0, \\alpha_1, \\dots, \\alpha_k) = \\sum_{i=1}^n\\left(y_i - \\alpha_0 - \\sum_{j=1}^k\\alpha_j x_{ij}\\right)^2$$\nThe function $S(\\cdot)$ must be minimized with respect to the coefficients, i.e.,\n$$(b_0, b_1, \\dots, b_k) = \\underset{{\\alpha_0, \\alpha_1, \\dots, \\alpha_k}}{\\mathrm{arg \\, min}}  S(\\alpha_0, \\alpha_1, \\dots, \\alpha_k)$$\n\n\n::: notes\n- As SLR, we can define the least-squares function as the sum of squares of epsilon.\n- The idea is to minimize the function $S$ w.r.t. the model coefficients $\\beta_0, \\beta_1, \\dots, \\beta_k$.\n- In other words, we are going to choose the sample statistics $b_0$, $b_1$, ..., $b_k$ as the estimates of $\\beta_0, \\beta_1, \\dots, \\beta_k$ so that $S(.)$ is minimized when $b_0$, $b_1$, ..., $b_k$ are plugged in the function.\n:::\n\n\n\n## Geometry of Least Square Estimation\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/05-mlr/unnamed-chunk-16-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n::: notes\n- If we look at the geometry of Least Square Estimation of the MLR, we have a visualization like this.\n- Again, in SLR, different $b_0$ and $b_1$s give us different sample regression lines.\n- In MLR, suppose we have two predictors, and different $b_0$ and $b_1$ and $b_2$ give us different sample regression planes.\n- so geometrically speaking, we are trying to find a sample regression plane such that the sum of the squared distance between the observations (denoted by those blue points) and the plane is minimized.\n- For more than 2 predictor case, we are not able to visualize it because we live a 3D world, but the idea is exactly the same. And we called the regression plane a hyperplane. OK.\n:::\n\n\n\n\n## Least-squares Normal Equations\n$$\\begin{align}\n\\left.\\frac{\\partial S}{\\partial\\alpha_0}\\right\\vert_{b_0, b_1, \\dots, b_k} &= -2 \\sum_{i=1}^n\\left(y_i - b_0 - \\sum_{j=1}^k b_j x_{ij}\\right) = 0\\\\\n\\left.\\frac{\\partial S}{\\partial\\alpha_j}\\right\\vert_{b_0, b_1, \\dots, b_k} &= -2 \\sum_{i=1}^n\\left(y_i - b_0 - \\sum_{j=1}^k b_j x_{ij}\\right)x_{ij} = 0, \\quad j = 1, 2, \\dots, k\n\\end{align}$$\n\n- $p = k + 1$ equations with $p$ unknown parameters.\n- The ordinary least squares estimators are the solutions to the normal equations.\n\n::: notes\n- So again similar to SLR, we can take derivative w.r.t $\\beta_0$, $\\beta_1$, to the $\\beta_k$.\n- And we are gonna have $p = k + 1$ equations with $p$ unknown parameters.\n- So we can find one and only one solution to $\\beta_0$, $\\beta_1$, to the $\\beta_k$, which are $b_0$, $b_1$, ..., $b_k$.\n- And the $p$ equations are the least squares normal questions.\n- You guys solve the equations when k = 1.\n- Do you think it is easy to solve the questions when the number of predictors is 2 or more than 2?\n:::\n\n. . .\n\n::: question\nüçπ üç∫ üç∏  ü•Ç  I buy you a drink if you solve the equations for $k \\ge 2$ by hand without using matrix notations or operations!\n:::\n\n\n::: notes\n- Here I am happy to treat you and buy you a drink if you can solve the equations for $k \\ge 2$ by hand without using matrix notations or operations!\n- Come to my office hours and show your work. And we go to a bar together. Sounds good?\n- In fact, if you try, you are gonna deal with lots of algebra, but if we write the system of equations in a matrix form, the solution becomes very clean.\n- That's see why.\n:::\n\n\n## [R Lab]{.pink} [Delivery Time Data](./data/data-ex-3-1.csv)\n\n:::: {.columns}\n\n::: {.column width=\"70%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load the data set\ndelivery <- read.csv(file = \"./data/data-ex-3-1.csv\",\n                     header = TRUE)\ndelivery_data <- delivery[, -1]\ncolnames(delivery_data) <- c(\"time\", \"cases\", \"distance\")\nstr(delivery_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t25 obs. of  3 variables:\n $ time    : num  16.7 11.5 12 14.9 13.8 ...\n $ cases   : int  7 3 3 4 6 7 2 7 30 5 ...\n $ distance: int  560 220 340 80 150 330 110 210 1460 605 ...\n```\n:::\n:::\n\n\n- $y$: the amount of time required by the route driver to stock the vending machines with beverages\n- $x_1$: the number of cases stocked\n- $x_2$: the distance walked by the driver\n:::\n\n\n\n::: {.column width=\"30%\"}\n\n::: midi\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndelivery_data\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.my_class800}\n   time cases distance\n1  16.7     7      560\n2  11.5     3      220\n3  12.0     3      340\n4  14.9     4       80\n5  13.8     6      150\n6  18.1     7      330\n7   8.0     2      110\n8  17.8     7      210\n9  79.2    30     1460\n10 21.5     5      605\n11 40.3    16      688\n12 21.0    10      215\n13 13.5     4      255\n14 19.8     6      462\n15 24.0     9      448\n16 29.0    10      776\n17 15.3     6      200\n18 19.0     7      132\n19  9.5     3       36\n20 35.1    17      770\n21 17.9    10      140\n22 52.3    26      810\n23 18.8     9      450\n24 19.8     8      635\n25 10.8     4      150\n```\n:::\n:::\n\n:::\n:::\n::::\n\n\n::: notes\n- $y$: the amount of time required by the route driver to stock the vending machines with beverages in an outlet.\n- $x_1$: the number of cases stocked\n- $x_2$: the distance walked by the driver\n- Goal: fit a MLR model $y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\epsilon$ to the amount of time required by the route driver to service the vending machines\n(use readr::read_csv if the file is large)\n:::\n\n\n\n\n## [R Lab]{.pink} Scatterplot Matrix\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npairs(delivery_data)\n```\n\n::: {.cell-output-display}\n![](./images/05-mlr/unnamed-chunk-19-1.png){fig-align='center' width=78%}\n:::\n:::\n\n\n\n::: notes\n- Each plot shows the relationship between a pair of variables.\n:::\n\n\n## [R Lab]{.pink} 3D Scatterplot\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mgp = c(2, 0.8, 0), las = 1, mar = c(4, 4, 0, 0))\nlibrary(scatterplot3d)\nscatterplot3d(x = delivery_data$cases, y = delivery_data$distance, z = delivery_data$time,\n              xlab =\"cases\", ylab = \"distance\", zlab = \"time\",\n              xlim = c(2, 30), ylim = c(36, 1640), zlim = c(8, 80),\n              box = TRUE, color = \"blue\", mar = c(3, 3, 0, 2), angle = 30, pch = 16)\n```\n\n::: {.cell-output-display}\n![](./images/05-mlr/unnamed-chunk-20-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\n\n::: notes\n- Sometimes a 3D scatterplot is useful in visualizing the relationship between the response and the regressors when there are only two regressors.\n:::\n\n\n\n## [R Lab]{.pink} Multiple Linear Regression\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndelivery_lm <- lm(time ~ cases + distance, data = delivery_data)\ndelivery_lm$coef\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)       cases    distance \n      2.341       1.616       0.014 \n```\n:::\n:::\n\n\n$$\\hat{y} = 2.34 + 1.62x_1 + 0.014x_2$$\n\n- $b_1$: **All else held constant**, for one case of product stocked increase, we expect the delivery time to be longer, *on average*, by 1.62 minutes.\n\n- $b_2$: **All else held constant**, one additional foot walked by the driver causes the delivery time, *on average*, to be 0.014 minutes longer.\n\n- $b_0$: The delivery time with no number of cases of product stocked and no distance walked by the driver is expected to be 2.34 minutes. (Make sense?!)\n\n## [R Lab]{.pink} Regression Plane\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/05-mlr/unnamed-chunk-22-1.png){fig-align='center' width=82%}\n:::\n:::\n\n\n\n\n## Estimation of $\\sigma^2$\n- $SS_{res} = \\sum_{i=1}^ne_i^2 = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2$.\n\n- $MS_{res} = \\frac{SS_{res}}{n - p}$ with $p = k + 1$.\n\n- $S^2 = MS_{res}$ is *unbiased* for $\\sigma^2$, i.e., $E[MS_{res}] = \\sigma^2$.\n\n<!-- - $\\hat{\\sigma}^2 = MS_{res}$ is model dependent. Its value varies with change of the model. -->\n\n- $S^2$ of SLR may be quite larger than the $S^2$ of MLR.\n\n- $S^2$ measures the variation of the *unexplained* noise about the fitted regression line/hyperplane, so we prefer a small residual mean square.\n\n\n::: notes\n- $SS_{res} = \\sum_{i=1}^ne_i^2 = {\\bf e'e} = {\\bf (y-Xb)'(y-Xb)} = {\\bf y'y - b'X'y} = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2$.\n- If our model is specified correctly, $\\hat{\\sigma}^2$ depends on our data quality.\n- If our data have lots of noise itself, there is nothing we can do.\n:::\n\n\n## [R Lab]{.pink} Estimation of $\\sigma^2$\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## method 1\nsumm_delivery <- summary(delivery_lm)  ## check names(summ_delivery)\nsumm_delivery$sigma ^ 2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 11\n```\n:::\n:::\n\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## method 2\nn <- length(delivery_lm$residuals)\n(SS_res <- sum(delivery_lm$residuals * delivery_lm$residuals))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 234\n```\n:::\n\n```{.r .cell-code}\nSS_res / (n - 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 11\n```\n:::\n:::\n\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## method 3\n(SS_res1 <- sum((delivery_data$time - delivery_lm$fitted.values) ^ 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 234\n```\n:::\n\n```{.r .cell-code}\nSS_res1 / (n - 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 11\n```\n:::\n:::\n\n\n::: notes\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n:::\n\n\n\n# Confidence Interval\n<h2> CI for Coefficients </h2>\n<h2> CI for the Mean Response  </h2>\n<h2> PI for New Observations  </h2>\n\n\n## Properties of LSEs\n- üëç ${\\bf b} = (b_0, b_1, \\dots, b_k)'$ is BLUE.\n- <span style=\"color:#DE3163\"> **Linear** </span>: Each $b_j$ is a linear combination of $y_1, \\dots, y_n$.\n- <span style=\"color:#DE3163\"> **Unbiased** </span>: Each $b_j$ is normally distributed with mean $\\beta_j$.\n- <span style=\"color:#DE3163\"> **Best** </span>: Each $b_j$ has the minimum variance, comparing to all other unbiased estimator for $\\beta_j$ that is a linear combo of $y_1, \\dots, y_n$.\n\n\n## Wald CI for Coefficients\nThe $(1-\\alpha)100\\%$ Wald CI for $\\beta_j$, $j = 0, 1, \\dots, k$ is\n$$\\left(b_j- t_{\\alpha/2, n-p}~se(b_j), \\quad b_j + t_{\\alpha/2, n-p}~ se(b_j)\\right)$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(ci <- confint(delivery_lm))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             2.5 % 97.5 %\n(Intercept) 0.0668  4.616\ncases       1.2618  1.970\ndistance    0.0069  0.022\n```\n:::\n:::\n\n\n- These are **marginal** CIs seperately for each $b_j$.\n\n\n::: notes\n- Each of the statistics $\\frac{b_j - \\beta_j}{\\sqrt{\\hat{\\sigma}^2C_{jj}}}, j = 0, 1, 2, \\dots, k$ follows $t_{n-p}$ distribution, where $\\hat{\\sigma}^2 = MS_{res}$, and $C_{jj}$ is the $j$-th diagonal element of ${\\bf (X'X)}^{-1}$.\n- The $(1-\\alpha)100\\%$ CI for $\\beta_j$, $j = 0, 1, \\dots, k$ is\n$\\left(b_j- t_{\\alpha/2, n-p} \\sqrt{\\hat{\\sigma}^2C_{jj}}, \\quad b_j + t_{\\alpha/2, n-p} \\sqrt{\\hat{\\sigma}^2C_{jj}}\\right)$\n- These interval estimates do not take correlation of coefficients into account. One coefficient may be higher when another is lower.\n- We can only use the intervals one at a time when doing interval estimation. When we want to do interval estimation for several coefficients together at the same time, these intervals are not that accurate.\n:::\n\n\n## Correlated Coefficients {visibility=\"hidden\"}\n\n::: fact\nCovariance of random variables $X$ and $Y$, $\\cov(X, Y)$ is defined as \n$$\\small \\cov(X, Y) = E[(X - E(X))(Y - E(Y))]$$\n::: \n\n\n::: question\nWhat is $\\cov(X, X)$?\n:::\n\n. . .\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n::: fact\nThe covariance matrix of the coefficient vector ${\\bf b} = (b_0, b_1, b_2)'$ is  $$\\scriptsize \\begin{align} \\cov({\\bf b}) &= \\begin{bmatrix} \\cov(b_0, b_0) & \\cov(b_0, b_1) & \\cov(b_0, b_2) \\\\ \\cov(b_1, b_0) & \\cov(b_1, b_1) & \\cov(b_1, b_2) \\\\ \\cov(b_2, b_0) & \\cov(b_2, b_1) & \\cov(b_2, b_2) \\end{bmatrix} \\end{align}$$\n:::\n:::\n\n\n\n::: {.column width=\"50%\"}\n::: fact\nThe correlation matrix of the coefficient vector ${\\bf b} = (b_0, b_1, b_2)'$ is  $$\\scriptsize \\begin{align} \\cor({\\bf b}) &= \\begin{bmatrix} 1 & r_{01} & r_{02} \\\\ r_{10} & 1 & r_{12} \\\\ r_{20} & r_{21} & 1 \\end{bmatrix} \\end{align}$$\n:::\n:::\n::::\n\n\n> <span style=\"color:blue\"> But all $b_j$ are correlated! </span>\n\n## Correlated Coefficients\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## variance-covariance matrix\nV <- vcov(delivery_lm)\n\n## standard error\nsqrt(diag(V))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)       cases    distance \n     1.0967      0.1707      0.0036 \n```\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## correlation matrix\ncov2cor(V)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            (Intercept) cases distance\n(Intercept)        1.00 -0.25    -0.22\ncases             -0.25  1.00    -0.82\ndistance          -0.22 -0.82     1.00\n```\n:::\n:::\n\n:::\n::::\n\n- $b_1$ and $b_2$ are negatively correlated.\n\n- Individual CI ignores the correlation between $b_j$s.\n\n. . .\n\n::: question\nHow do we specify a confidence level that applies **simultaneously** to a **set** of interval estimates? For example, a $95\\%$ confidence \"interval\" for both $b_1$ and $b_2$.\n:::\n\n\n## Confidence Region\n- The $(1-\\alpha)100\\%$ CI for a **set** of $b_j$s will be an **elliptically-shaped region**!\n\n<!-- :::: {.columns} -->\n\n<!-- ::: {.column width=\"50%\"} -->\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mgp = c(2.8, 0.9, 0), mar = c(4, 4, 2, 0))\n## confidence region\ncar::confidenceEllipse(\n    delivery_lm, \n    levels = 0.95, \n    which.coef = c(\"cases\", \"distance\"), \n    main = expression(\n        paste(\"95% Confidence Region for \", \n              beta[1], \" and \",  beta[2])\n        )\n    )\n## marginal CI for cases\nabline(v = ci[2, ], lty = 2, lwd = 2)  \n## marginal CI for distance\nabline(h = ci[3, ], lty = 2, lwd = 2)\npoints(x = 1.4, y = 0.01, col = \"red\", cex = 2, pch = 16)\npoints(x = 2, y = 0.008, col = \"black\", cex = 2, pch = 16)\n```\n\n::: {.cell-output-display}\n![](./images/05-mlr/unnamed-chunk-30-1.png){fig-align='center' width=85%}\n:::\n:::\n\n<!-- ::: -->\n\n\n<!-- ::: {.column width=\"50%\"} -->\n<!-- # ```{r} -->\n<!-- # #| out-width: 100% -->\n<!-- # par(mgp = c(2.8, 0.9, 0), mar = c(4, 4, 2, 0)) -->\n<!-- # car::confidenceEllipse(delivery_lm, levels = 0.95, which.coef = c(\"cases\", \"distance\"),  -->\n<!-- #                        main = expression(paste(\"95% Confidence Region for \", beta[1], \" and \",  beta[2]))) -->\n<!-- # abline(v = ci[2, ], lty = 2, lwd = 2)  ## marginal CI for cases -->\n<!-- # abline(h = ci[3, ], lty = 2, lwd = 2)  ## marginal CI for distance -->\n<!-- # points(x = 1.4, y = 0.01, col = \"red\", cex = 2, pch = 16) -->\n<!-- # points(x = 2, y = 0.008, col = \"black\", cex = 2, pch = 16) -->\n<!-- # ``` -->\n<!-- ::: -->\n<!-- :::: -->\n\n- Some points (red) within the marginal intervals are implausible according to the joint region.\n- The joint region includes values of the coefficient for cases (black), for example, that are excluded from the marginal interval.\n\n::: notes\n- Bonferroni\n- Scheffe S-method\n- maximum modulus t procedure\nWith repeated sampling, 95% of\nsuch ellipses will simultaneously include Œ≤1 and Œ≤2, if the fitted model is correct and normality holds. The orientation of the ellipse reflects the negative correlation between the estimates. \nContrast the 95% confidence ellipse with the marginal 95% confidence intervals, also shown on the plot. Some points within the marginal intervals‚Äîwith larger values for both of the coefficients, for example‚Äîare implausible according to the joint region. Similarly, the joint region includes values of the coefficient for income, for example, that are excluded from the marginal interval.\n:::\n\n\n## CI for the Mean Response $E(y \\mid {\\bf x}_0)$\n- The fitted value at a point ${\\bf x}_0 = (1, x_{01}, x_{02}, \\dots, x_{0k})'$ is $$\\hat{y}_0 = b_0 + b_1x_{01} + \\cdots + b_kx_{0k}$$\n- This is an unbiased estimator for $E(y \\mid {\\bf x}_0)$\n<!-- - $\\frac{\\hat{y}_0 - E(y|{\\bf x}_0)}{\\sqrt{\\hat{\\sigma}^2{\\bf x}_0'{\\bf (X'X)}^{-1}{\\bf x}_0}}\\sim t_{n-p}$. -->\n- The $(1-\\alpha)100\\%$ CI for $E(y \\mid {\\bf x}_0)$ is\n$$\\left(\\hat{y}_0 - t_{\\alpha/2, n-p} ~ se(\\hat{y}_0), \\quad \\hat{y}_0 + t_{\\alpha/2, n-p} ~ se(\\hat{y}_0)\\right)$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(delivery_lm,\n        newdata = data.frame(cases = 8, distance = 275),\n        interval = \"confidence\", level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  fit lwr upr\n1  19  18  21\n```\n:::\n:::\n\n\n\n::: notes\n- The fitted value at a point ${\\bf x}_0 = (1, x_{01}, x_{02}, \\dots, x_{0k})'$ is $\\hat{y}_0 = {\\bf x}_0'\\bf b$.\n- This is an unbiased estimator for $E(y \\mid {\\bf x}_0)$.\n- $\\frac{\\hat{y}_0 - E(y|{\\bf x}_0)}{\\sqrt{\\hat{\\sigma}^2{\\bf x}_0'{\\bf (X'X)}^{-1}{\\bf x}_0}}\\sim t_{n-p}$.\n- The $(1-\\alpha)100\\%$ CI for $E(y \\mid {\\bf x}_0)$ is\n$\\left(\\hat{y}_0 - t_{\\alpha/2, n-p} \\sqrt{\\hat{\\sigma}^2{\\bf x}_0'{\\bf (X'X)}^{-1}{\\bf x}_0}, \\quad \\hat{y}_0 + t_{\\alpha/2, n-p} \\sqrt{\\hat{\\sigma}^2{\\bf x}_0'{\\bf (X'X)}^{-1}{\\bf x}_0}\\right)$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n  fit lwr upr\n1  19  18  21\n```\n:::\n:::\n\n:::\n\n\n## PI for New Observations\n- Predict the future observation $y_0$ when ${\\bf x} = {\\bf x}_0$.\n- A point estimate is $\\hat{y}_0 = b_0 + b_1x_{01} + \\cdots + b_kx_{0k}$.\n- The $(1-\\alpha)100\\%$ PI for $y_0$ is\n$$\\left(\\hat{y}_0 - t_{\\alpha/2, n-p} ~se(y_0 - \\hat{y}_0), \\quad \\hat{y}_0 + t_{\\alpha/2, n-p} ~se(y_0 - \\hat{y}_0)\\right)$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(delivery_lm,\n        newdata = data.frame(cases = 8, distance = 275),\n        interval = \"predict\", level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  fit lwr upr\n1  19  12  26\n```\n:::\n:::\n\n\n\n::: notes\n- Predict the future observation $y_0$ when ${\\bf x} = {\\bf x}_0$.\n- A point estimate is $\\hat{y}_0 = {\\bf x}_0'\\bf b$.\n- The $(1-\\alpha)100\\%$ PI for $y_0$ is\n$\\left(\\hat{y}_0 - t_{\\alpha/2, n-p} \\sqrt{\\hat{\\sigma}^2\\left(1+{\\bf x}_0'{\\bf (X'X)}^{-1}{\\bf x}_0\\right)}, \\quad \\hat{y}_0 + t_{\\alpha/2, n-p} \\sqrt{\\hat{\\sigma}^2\\left(1+{\\bf x}_0'{\\bf (X'X)}^{-1}{\\bf x}_0\\right)}\\right)$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n  fit lwr upr\n1  19  12  26\n```\n:::\n:::\n\n:::\n\n\n## Predictor Effect Plots\n- A complete picture of the regression surface requires drawing a $p$-dimensional graph.\n\n- **Predictor effect plots** look at 1 or 2D plots for each predictor.\n\n. . .\n\n- The predictor effect plot for $x_1$: \n  + fix the values of all other predictors ( $x_2$ in the example)\n  + substitute these fixed values into the fitted regression equation.\n\n- Fix $x_2$ at its average, $\\hat{y} = 2.34 + 1.62 ~x_1 + 0.014 (409.28)$\n\n. . .\n\n- **Same** slope for *any* choice of fixed values of other predictors.\n\n- The intercepts depend on the values of other predictors.\n\n\n\n::: notes\n- A complete picture of the regression surface generated by the fitted model requires drawing a $p$-dimensional graph.\n\n- Fix $x_1$ at its average, $\\hat{y} = 2.34 + 1.62 (8.76) + 0.014 ~x_2$\n\n- The slope would be the **same** for *any* choice of fixed values of other predictors.\n:::\n\n\n\n## [R Lab]{.pink} Predictor Effect Plots\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(effects)\nplot(effects::predictorEffects(mod = delivery_lm))\n```\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/05-mlr/unnamed-chunk-36-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/05-mlr/unnamed-chunk-37-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n::::\n\n\n::: notes\n- rug plot\n- the mean of time increases linearly as cases or distance increases. \n- Given x2 variable in the model, what is the effect on x1 on the response.\n- Without correction for simultaneous statistical inference\n- plot(effects::predictorEffects(mod = delivery_lm, confint = list(type = \"Scheffe\")))\n'arg' should be one of ‚Äúpointwise‚Äù, ‚ÄúScheffe‚Äù, ‚Äúscheffe‚Äù\n:::\n\n## Extrapolation {visibility=\"hidden\"}\n- In MLR, it's easy to inadvertently extrapolate since the regressors jointly define the region containing the data.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/05-mlr/unnamed-chunk-38-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n::: notes\n- define the smallest convex set containing all of the original $n$ regressor points, as the regressor vairiable hull (RVH).\n- a point within the ranges of x1 and x2 may not be necessarily a interpolation point.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/05-mlr/unnamed-chunk-39-1.png){fig-align='center' width=85%}\n:::\n:::\n\n:::\n\n\n# Hypothesis Testing\n<h2> Test for Significance of Regression </h2>\n<h2> Tests on Individual Coefficients </h2>\n<h2> Tests on Subsets of Coefficients (Later) </h2>\n\n\n::: notes\n- Test for Significance: test if there is any regressor that has a significant effect on predicting $Y$ or explaining the variation of $Y$.\n- In a Test for Significance, we are looking all the regressors.\n- Tests on Individual Coefficients: we are only interested in single one predictor's effect.\n- But sometimes, we may be interested in a subset of Coefficients, not all, not single one, but two or three coefficients. And we'll see how to do this kind of test later.\n- Finally, we'll learn the so-called General Linear Hypothesis that can perform a quite general test.\n- The three tests above are in fact special case of the General Linear Hypothesis.\n:::\n\n\n## Test for Significance of Regression\n- **Test for significance**: Determine if there is a **linear** relationship between the response and **any** of the regressor variables.\n- <span style=\"color:blue\">\n$H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_k = 0 \\quad H_1: \\beta_j \\ne 0 \\text{ for at least one } j$ </span>\n<!-- - The $F$ test of ANOVA performs the test. -->\n\n<!-- # ```{r} -->\n<!-- # knitr::include_graphics(\"./images/05-mlr/anova_mlr.png\") -->\n<!-- # ``` -->\n<!-- - $SS_T = {\\bf y'y} - \\frac{\\left(\\sum_{i=1}^n y_i \\right)^2}{n}$, $SS_R = {\\bf b'X'y}-\\frac{\\left(\\sum_{i=1}^n y_i \\right)^2}{n}$, $SS_{res} = {\\bf y'y} - {\\bf b'X'y}$ -->\n\n. . .\n\n\n| Source of Variation | SS | df | MS | F | $p$-value |\n|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n| Regression | $SS_R$| $k$ | $MS_R$  |  $\\frac{MS_R}{MS_{res}} = F_{test}$ |  $P(F_{k, n-k-1} > F_{test})$ |\n| Residual | $SS_{res}$ | $n-k-1$   |  $MS_{res}$ |   |  | \n| Total | $SS_{T}$ | $n-1$   |   |   |  | \n\n- Reject $H_0$ if $F_{test} > F_{\\alpha, k, n - k - 1}$.\n\n::: notes\n- In SLR, test for significance is the same as testing individual coefficient $\\beta_1 = 0$.\n- We may reject the null when the truth is all beta's are nonzero, or only one single beta is nonzero.\n- If there is only one nonzero beta, and we reject the null, basically, the nearly all the variation of Y is explained by the corresponding predictor that has the nonzero beta.\n- We can still say the model is significant because that particular predictor has a significant effect on explaining y.\n- Another way to represent SS_T\n:::\n\n\n## [R Lab]{.pink} Test for Significance\n\n<span style=\"color:blue\">\n$H_0: \\beta_{1} = \\beta_{2} = 0 \\quad H_1: \\beta_j \\ne 0 \\text{ for at least one } j$ </span>\n\n\n::: {.cell layout-align=\"center\" output.lines='[9,10,11,12,13,14,15,16,17,18,19]'}\n\n```{.r .cell-code}\nsumm_delivery\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.my_class800}\n...\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.34123    1.09673    2.13  0.04417 *  \ncases        1.61591    0.17073    9.46  3.3e-09 ***\ndistance     0.01438    0.00361    3.98  0.00063 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.3 on 22 degrees of freedom\nMultiple R-squared:  0.96,\tAdjusted R-squared:  0.956 \nF-statistic:  261 on 2 and 22 DF,  p-value: 4.69e-16\n...\n```\n:::\n:::\n\n\n\n::: notes\n- How do we obtain the ANOVA Table?\n:::\n\n\n## [R Lab]{.pink} Wrong ANOVA Table\n\n| Source of Variation | SS | df | MS | F | $p$-value |\n|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n| Regression | $SS_R$| $k$ | $MS_R$  |  $\\frac{MS_R}{MS_{res}} = F_{test}$ |  $P(F_{k, n-k-1} > F_{test})$ |\n| Residual | $SS_{res}$ | $n-k-1$   |  $MS_{res}$ |   |  | \n| Total | $SS_{T}$ | $n-1$   |   |   |  | \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nanova(delivery_lm) ## This is for sequential F-test\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: time\n          Df Sum Sq Mean Sq F value  Pr(>F)    \ncases      1   5382    5382   506.6 < 2e-16 ***\ndistance   1    168     168    15.8 0.00063 ***\nResiduals 22    234      11                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n- This is so called **Type-I** ANOVA table for a *sequential F-test*, which is **NOT** what we want.\n\n\n<!-- # ```{r} -->\n<!-- #  -->\n<!-- # knitr::include_graphics(\"./images/05-mlr/anova_mlr.png\") -->\n<!-- # ``` -->\n\n\n\n## [R Lab]{.pink} ANOVA Table: Null vs. Full\n::: alert\nTesting coefficients is like model comparison: **Compare the full model with the model under $H_0$**!\n:::\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n+ **Full model**: including both `cases` and `distance` predictors\n+ **Null model**: no predictors $(\\beta_1 = \\beta_2 = 0)$\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/05-mlr/anova_ex.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n::: {.fragment}\n\n::: {.cell layout-align=\"center\" output.lines='[1,2,3,4,5,6,7]'}\n\n```{.r .cell-code}\n## regression with intercept only\nnull_model <- lm(time ~ 1,\n                 data = delivery_data)\nanova(null_model, delivery_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n...\nAnalysis of Variance Table\n\nModel 1: time ~ 1\nModel 2: time ~ cases + distance\n  Res.Df  RSS Df Sum of Sq   F  Pr(>F)    \n1     24 5785                             \n2     22  234  2      5551 261 4.7e-16 ***\n...\n```\n:::\n:::\n\n:::\n:::\n::::\n\n\n\n\n::: notes\n- first row: info of SS_T\n- second row: info of SS_R and SS_res\n- does not show MS\n:::\n\n\n\n\n## $R^2$ and Adjusted $R^2$\n- $R^2 = \\frac{SS_R}{SS_T} = 1 - \\frac{SS_{res}}{SS_T}$\n   + calculated as in SLR.\n   + accesses how well the regression model fits the data.\n   + measures the proportion of variability in $Y$ that is explained by the regression or the $k$ predictors.\n   + The model with one additional predictor **always gets a higher $R^2$**.\n\n\n::: notes\n- There are other criterion or metrics for accessing model adequacy or model fit that may be better than R^2\n- The model with one additional predictor **always gets a higher $R^2$** even the new predictor has no explanatory power or useless in predicting y. Because the predictor may capture the random noise variation, leading to decrease in $SS_{res}$.\n- If we use R^2 as the measure of model fit, then we will tend to include lots of regressors in the model, and we will always choose the model that has the most predictors, even though lots of predictors have no or little contribution to the fitting or prediction.\n- The idea is a little bit like overfitting in machine learning.\n- A complex or a larger model has several disadvantages.\n  + First, it's more difficult to interpret your model and results. It reduces the interpretability.\n  + Second, the computing or running time is usually longer, and sometimes much longer depending on the order of complexity.\n  + Also, the data or the model itself may consume lots of memory spaces.\n  \n:::\n\n\n. . .\n\n::: alert\n**Occam's Razor: Don't use a complex model if a simpler model can perform equally well!**\n:::\n\n\n. . .\n\n- **Adjusted $R^2$**\n  + $R^2_{adj} = 1 - \\frac{SS_{res}/(n-p)}{SS_T/(n-1)}$\n  + applies a penalty (through $p$) for number of variables included in the model.\n\n\n::: notes\n- Not the more the better anymore\n- Adjusted R2 doesn't increase if the new variable does not provide very little information for prediction\n- Adjusted R2 will only increase on adding a variable to the model if the addition of the regressor reduces $MS_{res}$\n- This makes adjusted R2 a preferable metric for model selection in multiple regression models.\n:::\n\n\n## Adjusted $R^2$ Example\n- For a model with 3 predictors, $SS_{res} = 90$, $SS_T = 245$, and $n = 15$.\n$$R^2_{adj} = 1 - \\frac{90/(15-4)}{245/(15-1)} = 0.53$$\n- The 4-th regressor is added into the model, and $SS_{res} = 88$ (always decreases). Then\n$$R^2_{adj} = 1 - \\frac{88/(15-5)}{245/(15-1)} = 0.49$$\n\n. . .\n\n**Intuition**: The new added regressor should have explanatory power for $y$ large enough, so that $MS_{res}$ is decreased.\n\n\n## [R Lab]{.pink} $R^2$ and Adjusted $R^2$\n\n::: {.cell layout-align=\"center\" output.lines='[16,17,18]'}\n\n```{.r .cell-code}\nsumm_delivery\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n...\n\nResidual standard error: 3.3 on 22 degrees of freedom\nMultiple R-squared:  0.96,\tAdjusted R-squared:  0.956 \n...\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsumm_delivery$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.96\n```\n:::\n\n```{.r .cell-code}\nsumm_delivery$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.96\n```\n:::\n:::\n\n\n\n## Tests on Individual Regression Coefficients\n\n- Hypothesis test on any **single** regression coefficient.\n- <span style=\"color:blue\">\n$H_0: \\beta_{j} = 0 \\quad H_1: \\beta_j \\ne 0$ </span>\n- $t_{test} = \\frac{b_j}{se(b_j)}$\n- Reject $H_0$ if $|t_{test}| > t_{\\alpha/2, n-k-1}$\n- This is a **partial** or **marginal test**: a test of the **contribution of $X_j$ given ALL other regressors in the model**.\n\n::: notes\n- We can also do a one-side test for sure. But I am not sure if there is a R function to do a one-sided test.\n- But we can always compute the test statistic or the p-value ourselves.\n- Hypothesis test on any **single** regression coefficient.\n- <span style=\"color:blue\">\n$H_0: \\beta_{j} = 0 \\quad H_1: \\beta_j \\ne 0$ </span>\n- $t_{test} = \\frac{b_j}{\\sqrt{\\hat{\\sigma}^2C_{jj}}}$, where $C_{jj}$ is the $j$-th diagonal element of $({\\bf X'X})^{-1}$.\n- Reject $H_0$ if $|t_{test}| > t_{\\alpha/2, n-k-1}$\n- This is a **partial** or **marginal test**: a test of the **contribution of $X_j$ given ALL other regressors in the model**.\n:::\n\n\n\n## [R Lab]{.pink} Tests on Individual Coefficients\n- Assess the value of $x_2$ (distance) given that $x_1$ (cases) is in the model.\n- <span style=\"color:blue\">\n$H_0: \\beta_{2} = 0 \\quad H_1: \\beta_2 \\ne 0$ </span>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsumm_delivery$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)    2.341     1.0967     2.1  4.4e-02\ncases          1.616     0.1707     9.5  3.3e-09\ndistance       0.014     0.0036     4.0  6.3e-04\n```\n:::\n:::\n\n\n:::  notes\nqt(0.05/2, df = delivery_lm$df.residual, lower.tail = FALSE) ## t critical value df = n - 3 = 22\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\ndistance \n       4 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ndistance \n 0.00063 \n```\n:::\n:::\n\n:::\n\n\n## Inference Pitfalls\n::: danger\n- The test $H_0:\\beta_j = 0$ will always be rejected as long as the sample size is large enough, even $x_j$ has a very small effect on $y$.\n  + Consider the **practical significance** of the result, not just the statistical significance.\n  + Use the confidence interval to draw conclusions instead of relying only p-values.\n\n:::\n\n. . .\n\n<br>\n\n::: danger\n- If the sample size is small, there may not be enough evidence to reject $H_0:\\beta_j = 0$.\n  + DON'T immediately conclude that the variable has no association with the response.\n  + There may be a linear association that is just not strong enough to detect given your data, or there may be a non-linear association.\n\n:::\n\n\n\n\n\n",
    "supporting": [
      "05-mlr_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}