{
  "hash": "f8fd31800e6599645151d5867d5a7d74",
  "result": {
    "markdown": "---\ntitle: \"Collinearity\"\nsubtitle: \"MATH 4780 / MSSC 5780 Regression Analysis\"\nauthor: \"Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University\"\n# date: \"November 17 2023\"\n# macros: _macros.tex # import a list of TeX/LaTeX definitions\nformat: \n  revealjs:\n    # code-line-numbers: false\n    #     - \"macros.tex\"\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n    # include-in-header:\n    highlight-style: arrow\n    code-block-bg: true\n    self-contained: false\n    slide-number: c/t    \n    incremental: false\n    width: 1800\n    height: 1000\n    margin: 0.05\n    logo: \"https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg\"\n    footer: \"[math4780-f23.github.io/website](https://math4780-f23.github.io/website/)\"\n    theme: [\"simple\", \"slides.scss\"]\n    multiplex: true\n    code-link: true\n    fig-cap-location: bottom\n    fig-align: center\n    transition: none ## fade slide convex concave zoom\n    title-slide-attributes:\n      data-background-color: \"#447099\"\n      # data-background-image: images/paper-texture.jpg\n      # data-background-size: cover\n      # data-background-color: \"#698ED5\"\neditor: source\nexecute:\n  freeze: true\n  echo: false\n  purl: true\n---\n\n\n#  {visibility=\"hidden\"}\n\n\\def\\bx{\\mathbf{x}}\n\\def\\bg{\\mathbf{g}}\n\\def\\bw{\\mathbf{w}}\n\\def\\balpha{\\boldsymbol \\alpha}\n\\def\\bbeta{\\boldsymbol \\beta}\n\\def\\beps{\\boldsymbol \\epsilon}\n\\def\\bLambda{\\boldsymbol \\Lambda}\n\\def\\bX{\\mathbf{X}}\n\\def\\by{\\mathbf{y}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bW{\\mathbf{W}}\n\\def\\T{\\text{T}}\n\\def\\cov{\\mathrm{Cov}}\n\\def\\cor{\\mathrm{Corr}}\n\\def\\var{\\mathrm{Var}}\n\\def\\E{\\mathrm{E}}\n\\def\\bmu{\\boldsymbol \\mu}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\def\\Trace{\\text{Trace}}\n\n\n\n\n\n\n\n# Collinearity\n<h2> Meaning </h2>\n<h2> Sources </h2>\n<h2> Effects </h2>\n<h2> Diagnostics </h2>\n<h2> Solutions </h2>\n\n## What is Collinearity\n\n- **Collinearity** refers to the situation in which **two or more predictors are closely related to one another**.\n- `limit` and `age` appear to have no obvious relationship, which is good!\n- `limit` and `rating` are highly correlated, and they are said to be *collinear*.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/14-collinearity/unnamed-chunk-2-1.png){fig-align='center' width=65%}\n:::\n:::\n\n\n\n::: notes\n- Collinearity is a fundamental problem with the data rather than model specification.\n- There is usually no satisfactory solution for a true collinearity problem.\n- Collinearity means some regressors in your model are highly correlated.\n- We want to have regressors that are **NOT _moving with each other_**.\n- Ideally we desire to have **orthogonal** regressors.\n- The intuition is, we hope predictors can explain the variation of $y$, right? That's why we put these predictors in the model.\n- And ideally we hope each predictor can explain a part of variation of $y$ that can only be explained by that predictor and cannot be explained by any other predictors.\n- This kind of partition can be done if all the predictors are orthogonal.\n- If x1 and x2 are highly correlated, meaning that they are moving together, then the two predictors are gonna explain a large part of the same variation of of $y$.\n- Think about it. If x1 and y are correlated in some way, and x1 and x2 are highly correlated, it means that x2 and y are going to be correlated in same way as x1 and y.\n- So we actually use two predictors to explain the same variation of $y$, which is redundant. \n- The model will be confused and may not be able to understand this explained variation is due to x1 or due to x2.\n- Later, we'll see why we don't want the predictors to be correlated. \n- There are lots of bad effects on our regression model.\n:::\n\n\n## Sources of Collinearity\nFour primary sources\n\n+ The data collection method employed\n+ Constraints on the model or in the population\n+ Model specification\n+ A model with $p>n$\n\n\n::: notes\n- Let's see what causes Collinearity. Here are 4 sources of Collinearity.\n+ The data collection method employed: the way we collect our data may cause the predictors to be moving together. We need to be more careful, and see if we can collect our data in another way, so that the correlation between predictors can be alleviated or weakened.\n+ Constraints on the model or in the population: If two predictors are correlated in nature, then it is unavoidable to have Collinearity, and if we want to keep the two predictors in our model, we should consider other methods other than OLS.\n+ Model specification: Polynomial regression.\n+ A $p>n$ model\n:::\n\n## Data Collection\n\n- Collinearity occurs when only a subspace of the entire sample space has been explored.\n- May be able to reduce this collinearity through the sampling technique used. \n- *There is no physical reason why you can't sample in that area*.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/14-collinearity/unnamed-chunk-3-1.png){fig-align='center' width=55%}\n:::\n:::\n\n\n\n## Constraints\n- Physical constraints are present, and the collinearity will exist regardless of collection method.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/14-collinearity/unnamed-chunk-4-1.png){fig-align='center' width=65%}\n:::\n:::\n\n\n\n## Model Specification\n- Polynomial terms can cause ill-conditioning in ${\\bf X'X}$.\n- As the order of the model increases, ${\\bf X'X}$ matrix inversion will become inaccurate, and error can be introduced into the parameter estimates.\n- If range on a regressor variable is small, adding an $x^2$ term can result in significant collinearity.\n\n\n::: notes\n$x$ and $x^2$ are highly (linearly correlated)\n- Polynomial terms can cause ill-conditioning (a sign of Collinearity) in the $({\\bf X'X})$ matrix.\n- As the order of the model increases, $({\\bf X'X})$ matrix inversion will become more inaccurate, and error can be introduced into the parameter estimates\n- If range on a regressor variable is small, adding $x^2$ term can result in significant Collinearity.\n:::\n\n\n## $p>n$ Model\n- More regressor variables than observations.\n- The best way to counter this is to remove/reconstruct regressor variables.\n  + Principal Component Regression\n  + Variable Selection (Next Topic)\n\n\n## Effect of Collinearity\n\n1. ðŸ‘‰ **Large variances and covariances** for the LSE $b_j$s.\n\n\n<!-- $$({\\bf X}_2'{\\bf X}_2)^{-1} = \\begin{bmatrix} 63.94 & -63.44 \\\\ -63.44 & 63.94 \\end{bmatrix}$$  -->\n\n2. ðŸ‘‰ Tends to produce LSE $b_j$ that are **too large in absolute value**. Therefore, the vector ${\\bf b}$, on average, is much longer than the vector $\\bbeta$.\n\n::: alert\n- Large variances and large magnitude of coefficients lead to **instable** and **wrong signed** coefficients.\n- Poor coefficients do not necessarily imply bad fit or poor prediction. \n- The predictions should be confined to the $x$ space where the collinearity holds approximately.\n- Collinearity causes very poor *extrapolated* prediction.\n:::\n\n\n\n\n::: notes\n- The squared distance from ${\\bf b}$ to $\\bsbeta$ is $D^2 = \\sum_{j=1}^k(b_j - \\beta_j)^2$.\n- The *expected* squared distance is $$E(D^2) = E\\left(\\sum_{j=1}^k(b_j - \\beta_j)^2 \\right) = \\sum_{j=1}^kE\\left[(b_j - \\beta_j)^2\\right] = \\sum_{j=1}^k\\var(b_j) = \\sigma^2 \\mathrm{tr}\\left[({\\bf X'X})^{-1}\\right]$$\n- Strong Collinearity between $x$'s results in \n- the trace of a matrix (Tr) is the sum of the diagonal elements.\n:::\n\n\n\n## Perfectly Correlated Regressors\n\n- Suppose the true population regression equation is $y = 3 + 4x$.\n\n- Suppose we try estimating that equation using perfected correlated variables $x$ and $z = x/10$.\n\n$$\n\\begin{aligned}\\hat{y}&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2z\\\\\n&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2\\frac{x}{10}\\\\\n&= \\hat{\\beta}_0 + \\bigg(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10}\\bigg)x\n\\end{aligned}\n$$\n<!-- $$\\hat{y} = \\hat{\\beta}_0 + \\bigg(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10}\\bigg)x$$ -->\n\n. . .\n\n-   Can set $\\hat{\\beta}_1$ and $\\hat{\\beta}_2$ to any two numbers such that $\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10} = 4$.\n\n-   Unable to choose the \"best\" combination of $\\hat{\\beta}_1$ and $\\hat{\\beta}_2$.\n\n\n::: notes\n- Collinearity is a fundamental problem with the data rather than model specification.\n- There is usually no satisfactory solution for a true collinearity problem.\n- Collinearity means some regressors in your model are highly correlated.\n- We want to have regressors that are **NOT _moving with each other_**.\n- Ideally we desire to have **orthogonal** regressors.\n- The intuition is, we hope predictors can explain the variation of $y$, right? That's why we put these predictors in the model.\n- And ideally we hope each predictor can explain a part of variation of $y$ that can only be explained by that predictor and cannot be explained by any other predictors.\n- This kind of partition can be done if all the predictors are orthogonal.\n- If x1 and x2 are highly correlated, meaning that they are moving together, then the two predictors are gonna explain a large part of the same variation of of $y$.\n- Think about it. If x1 and y are correlated in some way, and x1 and x2 are highly correlated, it means that x2 and y are going to be correlated in same way as x1 and y.\n- So we actually use two predictors to explain the same variation of $y$, which is redundant. \n- The model will be confused and may not be able to understand this explained variation is due to x1 or due to x2.\n- Later, we'll see why we don't want the predictors to be correlated. \n- There are lots of bad effects on our regression model.\n:::\n\n\n## Collinearity Diagnostics\n\nIdeal characteristics of a collinearity diagnostic: \n\n+ Correctly indicate if collinearity is present\n+ How severe the problem is\n+ Provide insight as to which regressors are causing the problem\n\n\n## Examination of the Correlation Matrix of $x$s\n\n\n- After *unit length scaling*[^1], ${\\bf X'X} = \\left[r_{ij}\\right]_{k\\times k}$ is the *correlation* matrix of $x$s denoted as ${\\bf \\Sigma}$. [^2] For example,\n$${\\bf X'X} = \\begin{bmatrix} 1 & 0.992 \\\\ 0.992 & 1 \\end{bmatrix}$$\n\n\n- $r_{ij}$ is the **pairwise** correlation between $x_i$ and $x_j$.\n- Large $|r_{ij}|$ is an indication of collinearity.\n- When **more than two** regressors are involved in collinearity, there may be instances when *collinearity is present, but the pairwise correlations are not large.*\n- Inspecting $r_{ij}$ is not sufficient for detecting more complex collinearity.\n\n\n[^1]: $\\tilde{x}_{ij} = \\frac{x_{ij} - \\overline{x}_j}{\\sqrt{\\sum_{i=1}^n (x_{ij} - \\overline{x}_j)^2}}$ for $~~i = 1, 2, \\dots, n$, $~~ j = 1, 2, \\dots, k$.\n\n[^2]: The term *correlation* is a bit of misnomer. The regressors are not random variables. The correlation coefficient $r_{ij}$ does measure linear dependency between *$x_i$ and $x_j$ in the data*.\n\n\n\n\n::: notes\nAfter unit length scaling, ${\\bf X'X}$ is the correlation matrix of regressors.\n- If we scale and center the regressors, we have the correlation matrix. $${\\bf X'X} = \\begin{bmatrix} 1 & 0.992 \\\\ 0.992 & 1 \\end{bmatrix}$$\n- The off diagonal elements of the centered and scaled  ${\\bf X'X}$ matrix are the **pairwise** correlations between $x_i$ and $x_j$, denoted as $r_{ij}$. For example, $r_{12} = 0.992$.\n- If $|r_{ij}| \\approx 1$, there is an indication of Collinearity. But, the opposite does not always hold.\n- When there are *more than two regressors*, there may be instances when Collinearity is present, but the pairwise correlations do not indicate a problem. (Webster, Gunst, and Mason [1974] Table 9.4)\n- Inspection of the $r_{ij}$ is not sufficient for detecting anything more complex than *pairwise* Collinearity.\n:::\n\n\n## Variance Inflation Factors\n<!-- - $\\small ({\\bf X}_1'{\\bf X}_1)^{-1} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$ $\\quad \\small ({\\bf X}_2'{\\bf X}_2)^{-1} = \\begin{bmatrix} 63.94 & -63.44 \\\\ -63.44 & 63.94 \\end{bmatrix}$ -->\n- The diagonals of ${\\bf \\Sigma}^{-1} = {\\bf C}$ in correlation form are called **variance inflation factors**\n$$\\text{VIF}_j = {\\bf C}_{jj}$$ \n\n- Example:\n$${\\bf \\Sigma} = \\begin{bmatrix} 1 & 0.992 \\\\ 0.992 & 1 \\end{bmatrix}; \\quad {\\bf \\Sigma}^{-1} = \\begin{bmatrix} 62.8 & -62.2 \\\\ -62.2 & 62.8 \\end{bmatrix}$$ and $\\text{VIF}_1 = 62.8$.\n\n::: alert\n- The collinearity produces an inflation in the variances of the estimated coefficients, an increase in *60-fold* over the *ideal case* when the two regressors are orthogonal.\n- VIFs $> 10$ are considered significant.\n:::\n\n\n## Variance Inflation Factors\n\n$$\\text{VIF}_j = \\frac{1}{1 - R^2_{X_j | X_{-j}}}$$ \nwhere $R^2_{X_j | X_{-j}}$ is the coefficient of determination obtained when $x_j$ is regressed on the other regressors $x_i, i \\ne j$. \n\n$$\\var(b_j) = \\frac{s^2}{\\sum_{i=1}^n(x_{ij} - \\bar{x}_j)^2} \\times \\text{VIF}_j$$\n\n- $\\text{VIF}_j$ measures the **combined** effect of the dependencies among the regressors on the variance of $b_j$.\n\n- If $x_j$ is *near linearly dependent* on some subset of the remaining regressors, ${\\bf C}_{jj}$ is large.\n\n::: question\nRemember what does linear (in)dependence mean?\n:::\n\n::: notes\n$$\\text{VIF}_j = {\\bf C}_{jj} = \\frac{1}{1 - R^2_j}$$ \n  + $R_j^2$: the coefficient of determination obtained when $x_j$ is regressed on the remaining regressors $x_i, i \\ne j$.\n- If $x_j$ can be explained a lot from other regressors, $x_j$ is probably unnecessary in the regression model when all others are in the model. When it is in the model, the model cannot understand what the real effect the predictor can provide, and therefore its coefficient has a large variance.\n- The regressors that have high VIFs probably have poorly estimated coefficients.\n- The Collinearity produces an inflation in the variances of the estimated coefficients, an increase in *60-fold* over the *ideal case* when the two regressors are orthogonal.\n- Since the variance of the j th regression coefficients is C jj Ïƒ 2 , we can view C jj as the factor by which the variance of Ë†Î²j is increased due to near - linear dependences among the regressors.\n\n:::\n\n\n## [R Lab]{.pink} [Hospital Manpower](./data/manpower.csv) Data\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nmanpower\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.my_class600}\n       y  x1    x2    x3    x4   x5\n1    567  16  2463   473  18.0  4.5\n2    697  44  2048  1340   9.5  6.9\n3   1033  20  3940   620  12.8  4.3\n4   1604  19  6505   568  36.7  3.9\n5   1611  49  5723  1498  35.7  5.5\n6   1613  45 11520  1366  24.0  4.6\n7   1854  55  5779  1687  43.3  5.6\n8   2161  59  5969  1640  46.7  5.2\n9   2306  94  8461  2872  78.7  6.2\n10  3504 128 20106  3655 180.5  6.2\n11  3572  96 13313  2912  60.9  5.9\n12  3741 131 10771  3921 103.7  4.9\n13  4027 127 15543  3866 126.8  5.5\n14 10344 253 36194  7684 157.7  7.0\n15 11732 409 34703 12446 169.4 10.8\n16 15415 464 39204 14098 331.4  7.0\n17 18854 510 86533 15524 371.6  6.3\n```\n:::\n:::\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n$y$: Monthly man-hours\n\n<br>\n\n$x_1$: Average daily patient load\n\n<br>\n\n$x_2$: Monthly X-ray exposures\n\n<br>\n\n$x_3$: Monthly occupied bed days\n\n<br>\n\n$x_4$: Eligible population in the area / 1000\n\n<br>\n\n$x_5$: Average length of patients' stay in days\n:::\n::::\n\n::: question\nDo you expect to see positive or negative relationship between $y$ and $x_i$?\n:::\n\n\n## [R Lab]{.pink} Hospital Manpower - Pairwise Dependence\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/14-collinearity/unnamed-chunk-6-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n## [R Lab]{.pink} Hospital Manpower - Model Fit\n\n\n::: {.cell layout-align=\"center\" output.lines='[10,11,12,13,14,15,16,20,21]'}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nlm_full <- lm(y ~ ., data = manpower)\n(summ_full <- summary(lm_full))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n...\n             Estimate Std. Error t value Pr(>|t|)  \n(Intercept) 1962.9482  1071.3617    1.83    0.094 .\nx1           -15.8517    97.6530   -0.16    0.874  \nx2             0.0559     0.0213    2.63    0.023 *\nx3             1.5896     3.0921    0.51    0.617  \nx4            -4.2187     7.1766   -0.59    0.569  \nx5          -394.3141   209.6395   -1.88    0.087 .\nResidual standard error: 642 on 11 degrees of freedom\nMultiple R-squared:  0.991,\tAdjusted R-squared:  0.987 \n...\n```\n:::\n:::\n\n\n::: question\nExcellent fit. But any issues of the this fitted result?\n:::\n\n. . .\n\n- The coefficients $b_1$, $b_4$ and $b_5$ are **negative.**\n- In the case of $x_1$, an increase in patient load, when other $x$'s are held constant, corresponds to a decrease in hospital manpower. (**wrong sign** due to large variance)\n\n\n::: notes\n- Even though the regression model fits the data quite well, the rather curious signs on the regression coefficients may be the result of the effect of Collinearity.\n:::\n\n\n\n## [R Lab]{.pink} Hospital Manpower - VIF\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nX <- manpower[, -1]\n(Sig <- cor(X))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     x1   x2   x3   x4   x5\nx1 1.00 0.91 1.00 0.94 0.67\nx2 0.91 1.00 0.91 0.91 0.45\nx3 1.00 0.91 1.00 0.93 0.67\nx4 0.94 0.91 0.93 1.00 0.46\nx5 0.67 0.45 0.67 0.46 1.00\n```\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n(C <- solve(Sig))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      x1    x2    x3     x4    x5\nx1  9598  11.9 -9247 -318.8 -93.9\nx2    12   7.9   -18   -1.9   1.8\nx3 -9247 -18.5  8933  294.4  83.4\nx4  -319  -1.9   294   23.3   6.4\nx5   -94   1.8    83    6.4   4.3\n```\n:::\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n## VIF \ndiag(C)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    x1     x2     x3     x4     x5 \n9597.6    7.9 8933.1   23.3    4.3 \n```\n:::\n:::\n\n:::\n::::\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n## put the fitted model in vif()\n(vif_all <- car::vif(lm_full))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    x1     x2     x3     x4     x5 \n9597.6    7.9 8933.1   23.3    4.3 \n```\n:::\n:::\n\n\n$x_1$ `Average daily patient load` and $x_3$ `Monthly occupied bed days` are highly correlated.\n\n\n## [R Lab]{.pink} Hospital Manpower - Confidence Interval\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/14-collinearity/unnamed-chunk-11-1.png){fig-align='center' width=100%}\n:::\n:::\n\n- Marginally, $b_1$ and $b_3$ vary a lot. The CI for $\\beta_1$ and CI for $\\beta_3$ both contain zero.\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n    2.5 % 97.5 %\nx1 -230.8  199.1\nx3   -5.2    8.4\n```\n:::\n:::\n\n:::\n\n\n::: {.column width=\"50%\"}\n::: {.fragment}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/14-collinearity/unnamed-chunk-13-1.png){fig-align='center' width=100%}\n:::\n:::\n\n- But very confident that $\\beta_1$ and $\\beta_3$ cannot be both zero.\n:::\n:::\n::::\n\n\n\n\n::: notes\n- The model is quite not sure how much $x_1$ and $x_3$ affect $y$.\n:::\n\n\n## [R Lab]{.pink} Hospital Manpower - Confidence Interval\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/14-collinearity/unnamed-chunk-14-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/14-collinearity/unnamed-chunk-15-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n::::\n\n- $x_4$ and $x_5$ are not highly correlated pairwisely. $r_{45} = 0.46$.\n- However, their CI is still inflated due to the collinearity effect of *other* variables.\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n   2.5 % 97.5 %\nx4   -20     12\nx5  -856     67\n```\n:::\n:::\n\n\n\n::: notes\nDon't just look at the pairwise correlation\n:::\n\n\n\n## Eigensystem Analysis: Condition Indices\n- The eigenvalues of ${\\bf \\Sigma}$ (the correlation matrix of $\\bf X$), $\\lambda_1, \\lambda_2, \\dots, \\lambda_k$, can measure collinearity.  \n- If there are one or more near-linear dependencies, one or more of the $\\lambda_i$s will be (relatively) *small*.\n<!-- - **Condition number** of ${\\bf X'X}$ is $\\kappa = \\frac{\\lambda_{max}}{\\lambda_{min}}$.  -->\n<!-- - $\\kappa > 100$ implies collinearity. -->\n<!-- - $\\kappa$ does not tell us *how many* regressors are involved. -->\n- **Condition indices** of ${\\bf \\Sigma}$ are $\\kappa_j = \\frac{\\lambda_{max}}{\\lambda_{j}}$. \n- The number of $\\kappa_j > 1000$ is a measure of the number of near-linear dependencies in  ${\\bf \\Sigma}.$\n\n\n::: notes\n- The eigenvalues of ${\\bf X'X}$, $\\lambda_1, \\lambda_2, \\dots, \\lambda_k$, can measure Collinearity.  \n- If there are one or more near-linear dependencies in the data, one or more of the $\\lambda_i$s will be small.\n- **Condition number** of ${\\bf X'X}$ is $\\kappa = \\frac{\\lambda_{max}}{\\lambda_{min}}$. \n- $\\kappa > 100$ implies Collinearity.\n- $\\kappa$ does not tell us *how many* regressors are involved.\n- **Condition indices** of ${\\bf X'X}$ is $\\kappa_j = \\frac{\\lambda_{max}}{\\lambda_{j}}$. \n- The number of $\\kappa_j > 1000$ is a measure of the number of near-linear dependencies in  ${\\bf X'X}.$\n:::\n\n## [R Lab]{.pink} Hospital Manpower - Eigensystem Analysis\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\neigen_Sig <- eigen(Sig)\n## eigenvalues\n(lambda <- eigen_Sig$values)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.2e+00 6.7e-01 9.5e-02 4.1e-02 5.4e-05\n```\n:::\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n## Conditional indices\nmax(lambda) / lambda\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]     1.0     6.3    44.4   103.1 77769.7\n```\n:::\n:::\n\n\n- $\\lambda_5 \\approx 0$ and $\\kappa_5 \\approx 77770$, indicating collinearity.\n\n::: alert\nEigenvalues are listed in a decreasing order, $\\lambda_1 > \\lambda_2 > \\cdots > \\lambda_k$, and $\\lambda_5$ is *not* the eigenvalue of $x_5$.\n:::\n\n\n::: notes\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n      [,1]     [,2]  [,3]  [,4]    [,5]\n[1,] -0.49 -0.00203 -0.17 -0.47  0.7195\n[2,] -0.45 -0.33561  0.80  0.19  0.0012\n[3,] -0.48 -0.00085 -0.15 -0.51 -0.6941\n[4,] -0.46 -0.31080 -0.54  0.63 -0.0234\n[5,] -0.33  0.88925  0.12  0.29 -0.0068\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1 1 1 1 1\n```\n:::\n:::\n\nThere is one near linear dependency.\n:::\n\n\n## Eigensystem Analysis: Eigendecomposition\n\n- Eigendecomposition $${\\bf \\Sigma = V\\bLambda V'}$$\n  + $\\bLambda$ is a $k \\times k$ diagonal matrix whose elements are $\\lambda_j$.\n  + ${\\bf V} = [{\\bf v}_1 \\quad {\\bf v}_2 \\quad \\dots \\quad {\\bf v}_k]$ is a $k \\times k$ orthogonal matrix whose columns are the eigenvectors of ${\\bf \\Sigma}$.\n- If $\\lambda_j \\approx 0$, the associated ${\\bf v}_j = (v_{1j}, v_{2j}, \\dots, v_{kj})'$ describes how (and what) regressors are linearly dependent:\n$$\\sum_{i=1}^kv_{ij}{\\bf x}_i \\cong \\mathbf{0}$$\n\n<!-- - $\\sum_{i=1}^kc_i{\\bf x}_i \\cong \\mathbf{0}$: The \"weights\" $c_i$ are the individual elements in the ${\\bf v}_j$. -->\n\n::: notes\n- If $\\lambda_j \\approx 0$, the associated ${\\bf v}_j = (v_{1j}, v_{2j}, \\dots, v_{kj})'$ describes the nature of this linear dependence.\n- $\\sum_{i=1}^kc_i{\\bf x}_i \\cong \\mathbf{0}$: The \"weights\" $c_i$ are the individual elements in the ${\\bf v}_j$.\n:::\n\n## [R Lab]{.pink} Hospital Manpower - Eigensystem Analysis\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"65%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n## eigenvector matrix\n(V <- eigen_Sig$vectors)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      [,1]     [,2]  [,3]  [,4]    [,5]\n[1,] -0.49 -0.00203 -0.17 -0.47  0.7195\n[2,] -0.45 -0.33561  0.80  0.19  0.0012\n[3,] -0.48 -0.00085 -0.15 -0.51 -0.6941\n[4,] -0.46 -0.31080 -0.54  0.63 -0.0234\n[5,] -0.33  0.88925  0.12  0.29 -0.0068\n```\n:::\n:::\n\n- ${\\bf X}{\\bf v}_5 = \\sum_{i=1}^5v_{i5}{\\bf x}_i \\approx {\\bf 0}$.\n- $0.720 {\\bf x}_1 + 0.001 {\\bf x}_2 - 0.694 {\\bf x}_3 - 0.023 {\\bf x}_4 - 0.007 {\\bf x}_5 \\approx {\\bf 0}$\n- Highly correlated $x_1$ and $x_3$ causes collinearity.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n::: alert\n\n- All ${\\bf x}$s here are unit length scaled.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nX_s <- apply(X, 2, unit_len_scale)\n```\n:::\n\n:::\n\n\n:::\n\n::: {.column width=\"35%\"}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nX_s %*% V[, 5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.my_class800}\n          [,1]\n [1,] -0.00037\n [2,] -0.00144\n [3,]  0.00032\n [4,] -0.00058\n [5,] -0.00108\n [6,]  0.00047\n [7,] -0.00131\n [8,]  0.00492\n [9,] -0.00225\n[10,]  0.00230\n[11,] -0.00050\n[12,]  0.00209\n[13,] -0.00251\n[14,] -0.00016\n[15,]  0.00131\n[16,] -0.00098\n[17,] -0.00023\n```\n:::\n:::\n\n\n:::\n::::\n\n\n\n\n::: notes\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]  [,3]  [,4]    [,5]\n[1,]  4.2 0.00 0.000 0.000 0.0e+00\n[2,]  0.0 0.67 0.000 0.000 0.0e+00\n[3,]  0.0 0.00 0.095 0.000 0.0e+00\n[4,]  0.0 0.00 0.000 0.041 0.0e+00\n[5,]  0.0 0.00 0.000 0.000 5.4e-05\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## Orthogonal matrix\nround(t(V)%*%V, 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    0    0    0    0\n[2,]    0    1    0    0    0\n[3,]    0    0    1    0    0\n[4,]    0    0    0    1    0\n[5,]    0    0    0    0    1\n```\n:::\n\n```{.r .cell-code}\n##  V(Lambda)V' = X'X\nV %*% diag(lambda) %*% t(V)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4] [,5]\n[1,] 1.00 0.91 1.00 0.94 0.67\n[2,] 0.91 1.00 0.91 0.91 0.45\n[3,] 1.00 0.91 1.00 0.93 0.67\n[4,] 0.94 0.91 0.93 1.00 0.46\n[5,] 0.67 0.45 0.67 0.46 1.00\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## near-zero vector\nX_s%*%V[, 5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]\n [1,] -0.00037\n [2,] -0.00144\n [3,]  0.00032\n [4,] -0.00058\n [5,] -0.00108\n [6,]  0.00047\n [7,] -0.00131\n [8,]  0.00492\n [9,] -0.00225\n[10,]  0.00230\n[11,] -0.00050\n[12,]  0.00209\n[13,] -0.00251\n[14,] -0.00016\n[15,]  0.00131\n[16,] -0.00098\n[17,] -0.00023\n```\n:::\n:::\n\n:::\n\n\n\n## Eigensystem Analysis: Variance Proportion {visibility=\"hidden\"}\n<!-- - ${\\bf X'X = V\\bsLambda V'}$, therefore ${\\bf (X'X)^{-1} = V\\bsLambda^{-1} V'}$ -->\n<!-- - $\\var\\left( {\\bf b} \\right) = \\sigma^2 {\\bf (X'X)} ^{-1} = \\sigma^2 {\\bf V\\bsLambda^{-1} V'}$ -->\n<!-- - $\\var\\left( b_j \\right) = \\sigma^2 \\sum_{i=1}^k\\frac{v_{ji}^2}{\\lambda_i} =\\sigma^2\\text{VIF}_j$ -->\n- $\\text{VIF}_j = \\sum_{i=1}^k\\frac{v_{ji}^2}{\\lambda_i}$\n<!-- - $\\sum_{j=1}^k \\var\\left( b_j \\right) =\\sigma^2 \\sum_{i=1}^k\\sum_{j=1}^k\\frac{v_{ji}^2}{\\lambda_i} = \\sigma^2\\sum_{i=1}^k\\frac{1}{\\lambda_i}$ (length of ${\\bf v}_j$ is 1) -->\n- $\\pi_{ij} = \\frac{v_{ji}^2/\\lambda_i}{\\text{VIF}_j}$ is the **variance decomposition proportion** that measures the proportion of the variance of $b_j$ contributed by $\\lambda_i$.\n- $\\pi_{ij} > 0.5$ indicates collinearity. \n\n::: alert\n- A small $\\lambda_i$, accompanied by a subset of regressors with high variance proportions $\\pi_{ij}$ represents a dependency involving the regressors in that subset.\n- If $\\pi_{32}$ and $\\pi_{34}$ are large, $\\lambda_3$ is associated with a collinearity that inflats the variance of $b_2$ and $b_4$.\n:::\n\n\n::: notes\n- ${\\bf X'X = V\\bsLambda V'}$, therefore ${\\bf (X'X)^{-1} = V\\bsLambda^{-1} V'}$\n- $\\var\\left( {\\bf b} \\right) = \\sigma^2 {\\bf (X'X)} ^{-1} = \\sigma^2 {\\bf V\\bsLambda^{-1} V'}$\n- $\\var\\left( b_j \\right) = \\sigma^2 \\sum_{i=1}^k\\frac{v_{ji}^2}{\\lambda_i} =\\sigma^2\\text{VIF}_j$\n- $\\sum_{j=1}^k \\var\\left( b_j \\right) =\\sigma^2 \\sum_{i=1}^k\\sum_{j=1}^k\\frac{v_{ji}^2}{\\lambda_i} = \\sigma^2\\sum_{i=1}^k\\frac{1}{\\lambda_i}$ (length of ${\\bf v}_j$ is 1)\n- $\\pi_{ij} = \\frac{v_{ji}^2/\\lambda_i}{\\text{VIF}_j}$ is the **variance decomposition proportion**, which is attributed to (or blamed on) the colliearity characterized by the eigenvalue $\\lambda_i$.\n- $\\pi_{ij} > 0.5$ indicates Collinearity. \n\nA small $\\lambda_i$, accompanied by a subset of regressors with high variance proportions $\\pi_{ij}$: \n+ represents a dependency involving the regressors in that subset, and the dependency is damaging to the precision of estimation of the coefficients in the subset.\n\n:::\n\n\n## [R Lab]{.pink} Hospital Manpower - Variance Proportion {visibility=\"hidden\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n# Variance Proportion\nvar_prop <- function(V, lambda, vif_all) {\n    ## diag(1 / lambda) means divide by lambda row-wise\n    round(t(V ^ 2 %*% diag(1 / lambda)) %*% diag(1/vif_all), 5)\n}\nvar_prop(V = V, lambda = lambda, vif_all = vif_all)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        [,1]   [,2]    [,3]   [,4]   [,5]\n[1,] 0.00001 0.0062 0.00001 0.0022 0.0062\n[2,] 0.00000 0.0213 0.00000 0.0062 0.2768\n[3,] 0.00003 0.8607 0.00003 0.1309 0.0328\n[4,] 0.00056 0.1087 0.00071 0.4237 0.4851\n[5,] 0.99940 0.0031 0.99925 0.4370 0.1991\n```\n:::\n:::\n\n\n- $\\pi_{51}$ and $\\pi_{53}$ are large, indicating that $\\lambda_5$ is inflating the variance of $b_1$ and $b_3$.\n\n\n::: notes\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## diag(V (Lambda)^-1 V') = VIF\ndiag(V %*% diag(1/lambda) %*% t(V))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 9597.6    7.9 8933.1   23.3    4.3\n```\n:::\n\n```{.r .cell-code}\nvif_all\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    x1     x2     x3     x4     x5 \n9597.6    7.9 8933.1   23.3    4.3 \n```\n:::\n\n```{.r .cell-code}\n## sum(vif) = sum(1/lambda)\nsum(vif(lm_full))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 18566\n```\n:::\n\n```{.r .cell-code}\nsum(1/lambda)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 18566\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n:::\n\n\n\n## Other Diagnostics\n\nCollinearity may exist if\n\n- overall $F$-test for regression is significant, but individual $t$-tests are all non-significant.\n\n- the coefficient estimates are **instable**\n  + adding or removing a regressor produces large changes in the estimates\n  + deleting one or more observations results in large changes in the estimates\n  + if the signs or magnitudes of the estimates are contrary to prior expectation\n\n\n## [R Lab]{.pink} Hospital Manpower - Significance\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nsumm_full$coefficients  ## t-test not significant\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) 1962.948    1.1e+03    1.83    0.094\nx1           -15.852    9.8e+01   -0.16    0.874\nx2             0.056    2.1e-02    2.63    0.023\nx3             1.590    3.1e+00    0.51    0.617\nx4            -4.219    7.2e+00   -0.59    0.569\nx5          -394.314    2.1e+02   -1.88    0.087\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nsumm_full$fstatistic  ## F-test significant\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nvalue numdf dendf \n  238     5    11 \n```\n:::\n:::\n\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nlm_no_x3 <- lm(y ~ . -x3, data = manpower)\nsummary(lm_no_x3)$coef\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) 2161.962    967.871     2.2  4.5e-02\nx1            34.284      4.897     7.0  1.4e-05\nx2             0.057      0.021     2.8  1.7e-02\nx4            -6.600      5.311    -1.2  2.4e-01\nx5          -440.297    183.696    -2.4  3.4e-02\n```\n:::\n:::\n\n\n\n\n## Diagonastics Summary {visibility=\"hidden\"}\n::: alert\n\n*Summary*:\n\n+ Eigenvalue (or ratios): assess the seriousness of a particular dependency\n+ Variance proportion: signify what regressors are involved and to what extent\n+ VIF: determine the damage to the individual coefficient\n:::\n\n\n## Methods for Dealing with Collinearity \n**Data collection**: <span style=\"color:blue\"> Collect more data to break up the collinearity </span> in the existing data\n\n<br>\n\n. . .\n\n\n**Model specification/An overdefined model**: Respecify the model\n\n- <span style=\"color:blue\"> *redefining* the regressors </span>: Use $x = x_1+x_2$ or $x = x_1x_2$.\n  + Avoid combining regressors in different units.\n  \n- <span style=\"color:blue\"> *eliminating* regressors </span>: remove $x_1$ or $x_2$.\n  + May damage the predictive power if the removed regressors have significant explanatory power. (Variable selection)\n  + If we remove $x_2$, we estimate the *marginal* relationship between $y$ and $x_1$, ignoring $x_2$, rather than the *partial* relationship conditioning on $x_2$.\n\n. . .\n\n**Constraint on the model or in the population**: Say goodbye to least-squares estimation.\n\n  + <span style=\"color:blue\"> Ridge Regression, Principal Component Regression, Bayesian Regression, etc </span>\n\n\n<!-- ## Unbiased vs. Biased Estimators {visibility=\"hidden\"} -->\n<!-- - The LSE ${\\bf b}$ is *unbiased*, i.e., $E({\\bf b}) = \\bbeta$. -->\n<!-- - LSE is a BLUE: has *minimum variance* among all *unbiased* linear estimators -->\n<!-- - *No guarantee that this variance will be small.* -->\n<!-- - When collinearity exists, $\\var({\\bf b})$ is largely inflated. -->\n\n<!-- ```{r} -->\n<!-- #| fig-asp: 0.35 -->\n<!-- par(mar = c(2, 0, 2, 0), mgp = c(0.5, 0.5, 0), las = 1) -->\n<!-- x <- seq(-3, 3, by = 0.1) -->\n<!-- plot(x, dnorm(x), type = \"l\", lwd = 3, axes = F, xlab = \"\", ylab = \"\") -->\n<!-- title(main = \"Sampling distribution of LSE b\") -->\n<!-- axis(1, at = c(-3, 0.1, 3), labels = c(\"\", expression(bold(beta) == E(b)), \"\"),  -->\n<!--      tick = T, tck = -0.01) -->\n<!-- segments(x0 = 0, y0 = 0, x1 = 0, y1 = dnorm(0), lty = 2) -->\n<!-- text(2, dnorm(1), expression(paste(\"E(b) = \", bold(beta), \" (unbiased)\")), cex = 1.5) -->\n<!-- text(2.5, dnorm(1.5), paste(\"Var(b) large\"), cex = 1.5) -->\n<!-- ``` -->\n\n\n<!-- ::: notes -->\n<!-- implying that confidence intervals on Î² would be wide and the point estimate Ë†b is very unstable. -->\n<!-- ::: -->\n\n\n<!-- ## Biased Estimators {visibility=\"hidden\"} -->\n\n<!-- :::: {.columns} -->\n\n<!-- ::: {.column width=\"45%\"} -->\n<!-- A good estimator is the one that *balances bias and variance well*, or the one that *minimizes the mean square error* $$\\small \\text{MSE}(\\hat{\\beta}) = E[(\\hat{\\beta} - \\beta)^2] = \\var(\\hat{\\beta}) + \\text{bias}(\\hat{\\beta})^2$$ -->\n\n<!-- - Biasedness and variance have a *trade-off* relationship! -->\n<!-- - Find a **biased** estimator ${\\bf \\hat{b}}$ that has *smaller variance and MSE* than ${\\bf b}$. -->\n<!-- ::: -->\n\n<!-- ::: {.column width=\"55%\"} -->\n<!-- ```{r} -->\n<!-- #| out-width: 100% -->\n<!-- par(mar = c(2, 0, 2, 0), mgp = c(0.5, 0.5, 0), las = 1) -->\n<!-- x <- seq(-6, 6, by = 0.1) -->\n<!-- plot(x, dnorm(x), type = \"l\", lwd = 3, axes = F, xlab = \"\", ylab = \"\", -->\n<!--      xlim = c(-8, 8)) -->\n<!-- title(main = list(expression(paste(\"Sampling distribution a biased estimator \", hat(b))), cex = 2)) -->\n<!-- axis(1, at = c(-8, -1, 0, 8),  -->\n<!--      labels = c(\"\", expression(bold(beta)), expression(E(b)), \"\"),  -->\n<!--      tick = T, tck = -0.01, cex.lab = 2) -->\n<!-- segments(x0 = 0, y0 = 0, x1 = 0, y1 = dnorm(0), lty = 2) -->\n<!-- segments(x0 = -1, y0 = 0, x1 = -1, y1 = dnorm(-1), lty = 2) -->\n<!-- text(3.5, dnorm(1), expression(paste(E(hat(b)) != bold(beta), \" (biased)\")), cex = 2) -->\n<!-- text(4, dnorm(1.5), expression(paste(Var(hat(b)), \" small\")), cex = 2) -->\n<!-- ``` -->\n<!-- ::: -->\n<!-- :::: -->\n\n\n<!-- ::: notes -->\n<!-- <!-- .question[ --> -->\n<!-- <!-- Can we obtain an estimator that has a smaller variance? --> -->\n<!-- <!-- ] --> -->\n<!-- ::: -->\n\n\n\n<!-- ## Ridge Regression: Motivation  {visibility=\"hidden\"} -->\n\n<!-- - ${\\bf \\Sigma} = \\begin{bmatrix} 1 & 0.992 \\\\ 0.992 & 1 \\end{bmatrix}$ $\\quad {\\bf  \\Sigma}^{-1} = \\begin{bmatrix} 62.8 & -62.2 \\\\ -62.2 & 62.8 \\end{bmatrix}$ -->\n<!-- - $\\lambda_1 = 1.992$ and $\\lambda_2 = 0.008$ -->\n\n<!-- . . . -->\n\n<!-- - ${\\bf \\Sigma} = \\begin{bmatrix} 1 & 0\\\\ 0 & 1 \\end{bmatrix}$ $\\quad {\\bf \\Sigma}^{-1} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$ -->\n<!-- - $\\lambda_1 = 1$ and $\\lambda_2 = 1$ -->\n\n<!-- . . . -->\n\n<!-- - IDEA: To make ${\\bf \\Sigma}$ behave more like the orthogonal case. But how? -->\n\n\n<!-- . . . -->\n\n<!-- - Use $({\\bf X'X + \\delta I})$ with a small $\\delta >0$. -->\n<!-- - The eigenvalues of $({\\bf X'X + \\delta I})$ are $\\lambda_1+\\delta$ and $\\lambda_2 + \\delta$. -->\n<!-- - If $\\delta = 0.1$, -->\n<!-- $({\\bf X'X + \\delta I}) = \\begin{bmatrix} 1.1 & 0.992 \\\\ 0.992 & 1.1 \\end{bmatrix}$ and $({\\bf X'X + \\delta I})^{-1} = \\begin{bmatrix} 4.87 & -4.39 \\\\ -4.39 & 4.87 \\end{bmatrix}$ -->\n\n\n\n<!-- ::: notes -->\n<!-- - The diagonals *do not dominate* as in the orthogonal case whose matrix diagonals dominate and $\\lambda_1 = \\lambda_2 = 1$. -->\n<!-- Adding $\\delta$ to the main diagonal effectively replaces $\\lambda$ by $\\lambda+\\delta$.  -->\n<!-- ::: -->\n\n\n<!-- ## Ridge Regression Estimator {visibility=\"hidden\"} -->\n\n<!-- - With a small $\\delta >0$, the ridge estimator ${\\bf b}_R$ is $${\\bf b}_R = ({\\bf X'X + \\delta I})^{-1}{\\bf X'y}$$ -->\n\n<!-- - ${\\bf b}_R$ is a *biased* estimator for $\\bbeta$. -->\n\n<!-- ::: notes -->\n<!-- - $\\sum_{i=1}^k \\var\\left( b_{Ri} \\right) \\le \\sum_{i=1}^k \\var\\left( b_i \\right)$ with equality when $\\delta = 0$. -->\n<!-- ::: -->\n\n<!-- . . . -->\n\n<!-- - As $\\delta$ increases, -->\n<!--   + $\\sum_{i=1}^k \\var\\left( b_{Ri} \\right)$ decreases -->\n<!--   + Bias $E\\left[ {\\bf b}_R \\right] - \\bbeta$ increases -->\n<!--   + $R^2$ decreases (it's OK as we care more about stable estimates and better prediction) -->\n\n<!-- . . . -->\n\n<!-- - Choose $\\delta$ by the **ridge trace** that is a plot of $\\{ b_{Ri} \\}_{i=1}^k$ vs. $\\delta$. -->\n<!--   + Select a small value of $\\delta$ at which the ridge estimates ${\\bf b}_R$ are stable. -->\n\n\n\n\n<!-- ::: notes -->\n<!-- - Cross Validation -->\n<!-- - The ridge estimator ${\\bf b}_R$ is $${\\bf b}_R = ({\\bf X'X + \\delta I})^{-1}{\\bf X'y}$$ -->\n<!-- - ${\\bf b}_R$ is a *biased* estimator of $\\bsbeta$: -->\n<!-- $$E\\left[ {\\bf b}_R \\right] = E\\left[({\\bf X'X + \\delta I})^{-1}{\\bf X'y} \\right]  = E\\left[({\\bf X'X + \\delta I})^{-1}({\\bf X'X}){\\bf b}\\right] = {\\bf Z_{\\delta}}\\bsbeta$$ -->\n<!-- - The covariance matrix of ${\\bf b}_R$ is -->\n<!-- $$\\var\\left({\\bf b}_R \\right) = \\sigma^2 ({\\bf X'X + \\delta I})^{-1}{\\bf X'X }({\\bf X'X + \\delta I})^{-1}$$ -->\n<!-- - For LSE, $\\sum_{i=1}^k \\var\\left( b_i \\right) = \\sigma^2\\sum_{i=1}^k\\frac{1}{\\lambda_i}$ -->\n<!-- - For ridge estimator,  $\\sum_{i=1}^k \\var\\left( b_{Ri} \\right) = \\sigma^2\\sum_{i=1}^k\\frac{\\lambda_i}{(\\lambda_i+\\delta)^2}$ -->\n<!-- - $\\sum_{i=1}^k \\var\\left( b_{Ri} \\right) \\le \\sum_{i=1}^k \\var\\left( b_i \\right)$ with equality when $\\delta = 0$. -->\n<!-- - As $\\delta$ increases, $\\sum_{i=1}^k \\var\\left( b_{Ri} \\right)$ decreases. -->\n<!-- ::: -->\n\n\n<!-- ## [R Lab]{.pink} Hospital Manpower - Ridge Regression {visibility=\"hidden\"} -->\n<!-- :::: {.columns} -->\n\n<!-- ::: {.column width=\"40%\"} -->\n<!-- ::: midi -->\n<!-- ```{r} -->\n<!-- #| echo: true -->\n<!-- #| eval: false -->\n<!-- #| code-line-numbers: false -->\n<!-- #| class-source: my_class800 -->\n\n<!-- manpower_scale <- apply(manpower, 2, scale) -->\n<!-- df <- as.data.frame(manpower_scale) -->\n<!-- delta <- seq(0, 0.5, by = 0.01) -->\n<!-- ridge_fit <- MASS::lm.ridge( -->\n<!--   y ~ . -1, data = df, lambda = delta) -->\n<!-- matplot(coef(ridge_fit), type = \"l\", -->\n<!--         xlab = \"delta\", ylab = \"Coef\",  -->\n<!--         main = \"Ridge Trace\") -->\n<!-- abline(v = which(delta == 0.07),  -->\n<!--        col = \"orange\", lty = 2) -->\n<!-- ``` -->\n<!-- ::: -->\n<!-- ::: -->\n\n\n<!-- ::: {.column width=\"60%\"} -->\n<!-- ```{r} -->\n<!-- #| out-width: 100% -->\n<!-- library(MASS) -->\n<!-- # manpower_scale <- apply(manpower, 2,  -->\n<!-- #                         unit_length_scale) -->\n<!-- manpower_scale <- apply(manpower, 2, scale) -->\n<!-- delta <- seq(0, 0.5, by = 0.01) -->\n<!-- ridge_fit <- lm.ridge(y ~ . -1,  -->\n<!--                       data = as.data.frame(manpower_scale),  -->\n<!--                       lambda = delta) -->\n<!-- par(mar = c(3, 3.5, 2, 0), mgp = c(2, 0.5, 0), las = 1) -->\n<!-- matplot(coef(ridge_fit), type = \"l\", xlab = \"delta\", axes = F, cex.lab = 1.5, -->\n<!--         ylab = \"Coefficients\", main = \"Ridge Trace\", lwd = 2.5, cex.main = 2) -->\n<!-- axis(2, at = seq(-0.5, 1.5, by = 0.5)) -->\n<!-- axis(1, at = c(0, 10, 20, 30, 40, 50), labels = delta[c(0, 10, 20, 30, 40, 50)+1]) -->\n<!-- text(jitter(rep(15, 5), factor = 3), jitter(coef(ridge_fit)[15, ]), colnames(manpower_scale)[-1], cex = 1.5) -->\n<!-- abline(v = which(delta == 0.07), col = \"orange\", lty = 2) -->\n<!-- ``` -->\n<!-- ::: -->\n<!-- :::: -->\n\n\n\n<!-- ::: notes -->\n<!-- ```{r, eval=FALSE} -->\n<!-- # use GCV to select the best delta -->\n<!-- plot(ridge_fit$lambda, ridge_fit$GCV, type = \"l\", col = \"darkorange\",  -->\n<!--      ylab = \"GCV\", xlab = \"delta\", lwd = 3) -->\n<!-- title(\"Hospital Manpower Data: GCV\") -->\n<!-- ridge_fit$lambda[which.min(ridge_fit$GCV)] -->\n<!-- ``` -->\n<!-- ::: -->\n\n\n<!-- ## [R Lab]{.pink} Hospital Manpower - Ridge Regression {visibility=\"hidden\"} -->\n<!-- ```{r} -->\n<!-- #| out-width: 78% -->\n<!-- manpower_scale <- as.data.frame(manpower_scale) -->\n<!-- lm_full_scale <- lm(y ~ ., data = manpower_scale) -->\n<!-- par(mar = c(3, 4, 0, 0), mgp = c(2, 0.5, 0)) -->\n<!-- confidenceEllipse(lm_full_scale, which.coef = c(5, 6), col=\"black\", -->\n<!--                   fill=TRUE, -->\n<!--                   center.cex=1, -->\n<!--                   levels=c(0.8, 0.95), fill.alpha=0.1, -->\n<!--                   xlab=expression(beta[4]),  -->\n<!--                   ylab=expression(beta[5]),  -->\n<!--                   las=1, segments=1000, cex.lab = 2) -->\n<!-- abline(h = 0, v = 0) -->\n<!-- points(0, 0, pch = 15, cex = 1, col = 2) -->\n\n<!-- X_stX_s <- t(X_s) %*% X_s -->\n<!-- A <- X_stX_s + diag(0.07, 5) -->\n<!-- Ainv <- solve(A) -->\n<!-- var_ridge <- summary(lm_full_scale)$sigma^2 * Ainv %*% X_stX_s %*% Ainv / (nrow(manpower) - 1) -->\n<!-- # center <- c(0.10711510, 0.11790894) -->\n<!-- # center <- c(-0.007907946, -0.020534287) -->\n<!-- center <- c(-0.01925995, -0.02575226) -->\n<!-- ellipse(center, shape = var_ridge[c(4, 5), c(4, 5)], center.cex=1, -->\n<!--         radius = sqrt(qchisq(c(0.95), df = 2)), fill=TRUE, fill.alpha=0.1) -->\n<!-- ellipse(center, shape = var_ridge[c(4, 5), c(4, 5)], center.cex=1, -->\n<!--         radius = sqrt(qchisq(c(0.8), df = 2)), fill=TRUE, fill.alpha=0.1) -->\n\n<!-- # ellipse(center, shape = var_ridge[c(1, 3), c(1, 3)], center.cex=1, -->\n<!-- #         radius = sqrt(qchisq(c(0.95), df = 2)), fill=TRUE, fill.alpha=0.1) -->\n<!-- # ellipse(center, shape = var_ridge[c(1, 3), c(1, 3)], center.cex=1, -->\n<!-- #         radius = sqrt(qchisq(c(0.8), df = 2)), fill=TRUE, fill.alpha=0.1) -->\n\n<!-- # confidenceEllipse(ridge_fit, which.coef = c(2, 4), col=\"black\", -->\n<!-- #                   fill=TRUE,  -->\n<!-- #                   center.cex=1, -->\n<!-- #                   levels=c(0.8, 0.95), fill.alpha=0.1, -->\n<!-- #                   xlab=expression(beta[1]),  -->\n<!-- #                   ylab=expression(beta[3]),  -->\n<!-- #                   las=1, segments=1000) -->\n<!-- # abline(h = 0, v = 0) -->\n<!-- # points(0, 0, pch = 15, cex = 1, col = 2) -->\n<!-- ``` -->\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}