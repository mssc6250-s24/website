{
  "hash": "08df47ea15c00a8468427d98b976fa28",
  "result": {
    "markdown": "---\ntitle: \"Regression Diagnostics - Constant Variance ðŸ“–\"\nsubtitle: \"MATH 4780 / MSSC 5780 Regression Analysis\"\nauthor: \"Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University\"\n# date: \"August 02 2023\"\n# macros: _macros.tex # import a list of TeX/LaTeX definitions\nformat: \n  revealjs:\n    # code-line-numbers: false\n    #     - \"macros.tex\"\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n    # include-in-header:\n    highlight-style: arrow\n    code-block-bg: true\n    self-contained: false\n    slide-number: c/t    \n    incremental: false\n    width: 1800\n    height: 1000\n    margin: 0.05\n    logo: \"https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg\"\n    footer: \"[math4780-f23.github.io/website](https://math4780-f23.github.io/website/)\"\n    theme: [\"simple\", \"slides.scss\"]\n    multiplex: true\n    code-link: true\n    fig-cap-location: bottom\n    fig-align: center\n    transition: none ## fade slide convex concave zoom\n    title-slide-attributes:\n      data-background-color: \"#447099\"\n      # data-background-image: images/paper-texture.jpg\n      # data-background-size: cover\n      # data-background-color: \"#698ED5\"\neditor: source\nexecute:\n  freeze: true\n  echo: false\n  purl: true\n---\n\n\n#  {visibility=\"hidden\"}\n\n\\def\\bx{\\mathbf{x}}\n\\def\\bg{\\mathbf{g}}\n\\def\\bw{\\mathbf{w}}\n\\def\\balpha{\\boldsymbol \\alpha}\n\\def\\bbeta{\\boldsymbol \\beta}\n\\def\\beps{\\boldsymbol \\epsilon}\n\\def\\bX{\\mathbf{X}}\n\\def\\by{\\mathbf{y}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bW{\\mathbf{W}}\n\\def\\T{\\text{T}}\n\\def\\cov{\\mathrm{Cov}}\n\\def\\cor{\\mathrm{Corr}}\n\\def\\var{\\mathrm{Var}}\n\\def\\E{\\mathrm{E}}\n\\def\\bmu{\\boldsymbol \\mu}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\def\\Trace{\\text{Trace}}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n# Model Adequacy Checking and Correction\n\n<h2> Non-normality</h2>\n<h2> <span style=\"color:red\"> Non-constant Error Variance </span></h2>\n<h2> Non-linearity and Lack of Fit </h2>\n\n::: notes\n- This week we are going to learn some methods to correct model inadequacy.\n- We can either transform our data, our use a more general method, called Generalized Least Squares or Weighted Least Squares.\n- $\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)$: **mean 0**, **constant variance**, **normally distributed**, and **uncorrelated**.\n- $E(\\epsilon_i) =0$ implies the function form of the model (**linearity**) is correct.\n:::\n\n## Nonconstant Error Variance\n\nWithout constant variance, \n\n- LSEs are still unbiased if linearity and independence of $x$s and errors hold.\n\n- Inference results are not convincing: **distorted $p$-values** and **incorrect coverage** of confidence interval.\n\n- The $b_j$s will have **larger** standard errors (no minimum-variance).\n\n::: question\nIs it harder or easier to reject $H_0: \\beta_j = 0$ when the standard error of $b_j$ is larger?\n:::\n\n- For these consequences to occur, nonconstant error variance must be severe.\n\n## Detecting Nonconstant Error Variance\n- Common pattern: the conditional variation of $y$ to increase with the level of $y$.\n\n- It seems natural to plot $e_i$ against $y$\n\n  + $e_i$ have **unequal** variances $(\\var(e_i) = \\sigma^2(1-h_{ii}))$ even when the errors $\\epsilon_i$ have equal variances $\\var(\\epsilon_i) = \\sigma^2$.\n  + $e_i = y_i - \\hat{y}_i$ are correlated with $y$\n  \n- Use R-student residual plot $t_i$ vs. $\\hat{y}_i$ because R-student residuals have constant variance and they are uncorrelated with $\\hat{y}_i$.\n\n\n::: notes\n$r_{ey} = \\sqrt{1-R^2}$\nhttps://stats.stackexchange.com/questions/5235/what-is-the-expected-correlation-between-residual-and-the-dependent-variable\n:::\n\n\n## Residual Plots\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/09-diag-const-var/unnamed-chunk-4-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/09-diag-const-var/unnamed-chunk-5-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n::::\n\n- If the spread of the residuals **increases** with the level of the fitted values, we might be able to correct the problem by transforming $y$ **down** the ladder of power and roots.\n\n\n## Variance Stablizing Transformation\n- Constant variance assumption is often violated when the response $y$ follows a distribution whose *variance is functionally related to mean*.\n<!-- - The assumption: ${\\bf y} \\sim N\\left({\\bf X\\bsbeta}, \\sigma^2{\\bf I}\\right)$. -->\n\n::: question\nCan you provide a distribution whose variance is functionally related to its mean?\n:::\n\n. . .\n\nRelationship of $\\sigma^2$ to $E(y)$ | Transformation               \n-------------------------------------|------------------------------ \n$\\sigma^2 \\propto$ constant          | $y' = y$ (no transformation)  \n$\\sigma^2 \\propto E(y)$              | $y' = \\sqrt{y}$ (square root; Poisson)\n$\\sigma^2 \\propto E(y)[1-E(y)]$      | $y' = \\sin^{-1}(\\sqrt{y})$ (arcsin; binomial proportions $0 \\le y_i \\le 1$)\n$\\sigma^2 \\propto [E(y)]^2$          | $y' = \\ln(y)$ (natural log)\n$\\sigma^2 \\propto [E(y)]^3$          | $y' = y^{-1/2}$ (reciprocal square root) \n$\\sigma^2 \\propto [E(y)]^4$          | $y' = y^{-1}$ (reciprocal)\n\n::: notes\n- The strength of a transformation depends on the amount of curvature that it induces\n- a mild transformation applied over a relatively narrow range of values (e.g., y max / y min < 2, 3) has little effect. On the other hand, a strong transformation over a wide range of values will have a dramatic effect on the analysis.\n:::\n\n\n\n## [R Lab]{.pink} Nonconstant Variance (CIA Example)\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/09-diag-const-var/unnamed-chunk-6-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n- The residuals increase with fitted values.\n\n- Nonlinear pattern.\n\n- Nonsense negative fitted values.\n:::\n\n\n\n\n::: {.column width=\"50%\"}\n\n::: fragment\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/09-diag-const-var/unnamed-chunk-7-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n- Variation around the central smooth seems approximately constant.\n\n- Nonlinearity is still apparent. (Linearity and lack of fit next week)\n\n:::\n:::\n\n::::\n\n\n::: notes\n- The nonlinear pattern suggests there are serious problems of the linear model.\n\n- The residuals increases with fitted values.\n\n- Several nonsense negative fitted values.\n:::\n\n::: notes\nresidual plot can reveal nonlinearity, but can;t determine where the problem lies and how to correct it.\n:::\n\n\n\n## Tukey's Spread-Level Plot\n\n- Plot log of the absolute R-Student residuals vs. log of the (positive) fitted values.\n- The fitted line slope $b$ suggests a variance-stablizing power transformation $\\lambda = 1 - b$ for $y$.\n\n\n:::: {.columns}\n\n::: midi\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\ncar::spreadLevelPlot(ciafit, smooth = FALSE)\n```\n\n::: {.cell-output-display}\n![](./images/09-diag-const-var/unnamed-chunk-8-1.png){fig-align='center' width=100%}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSuggested power transformation:  0.37 \n```\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nspreadLevelPlot(logciafit, smooth = FALSE)\n```\n\n::: {.cell-output-display}\n![](./images/09-diag-const-var/unnamed-chunk-9-1.png){fig-align='center' width=100%}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSuggested power transformation:  1.1 \n```\n:::\n:::\n\n:::\n:::\n::::\n\n\n::: notes\n- Plot log of the absolute R-Student residuals vs. log of the (positive) fitted values\n\n- Fit by robust regression to generate fitted lines.\n\n- The slope $b$ of the fitted line suggests a variance-stablizing power transformation $\\lambda = 1 - b$ for $y$.\n:::\n\n\n\n## Weighted Least Squares (WLS)\n- Suppose that the other assumptions hold but that the error variances differ: $\\epsilon_i \\stackrel{indep}{\\sim} N(0, \\sigma_i^2)$\n\n- The resulting model isn't estimable because we have more parameters than data points.\n\n- If we **know** the pattern of unequal error variances, so that $\\sigma_i^2 = \\sigma^2/w_i$, for **known** values $w_i$, we can compute the coefficient estimates that minimize the **Weighted Residual Sum of Squares**.\n\n- In simple linear regression, \n$$S(\\beta_0, \\beta_1) = \\sum_{i=1}^nw_i\\epsilon_i^2 = \\sum_{i=1}^n w_i \\left(y_i - \\beta_0 - \\beta_1x_i \\right)^2$$\n\n\n::: alert\nObservations with large variances will have smaller weights.\n:::\n\n\n::: notes\n$$(b_0, b_1, \\dots, b_k) = \\underset{{\\beta_0, \\beta_1, \\dots, \\beta_k}}{\\mathrm{arg \\, min}}  S(\\beta_0, \\beta_1, \\dots, \\beta_k)$$\n\n$$S(\\beta_0, \\beta_1, \\dots, \\beta_k) = \\sum_{i=1}^nw_i\\epsilon_i^2 = \\sum_{i=1}^nw_i\\left(y_i - \\beta_0 - \\sum_{j=1}^k\\beta_j x_{ij}\\right)^2$$\n:::\n\n\n## Weighted Least Squares (WLS)  {visibility=\"hidden\"}\n\n- The resulting least squares normal equations:\n$$\\begin{align} \\beta_0\\sum_{i=1}^n w_i+\\beta_1\\sum_{i=1}^n w_ix_i &= \\sum_{i=1}^n w_iy_i \\\\ \\beta_0 \\sum_{i=1}^n w_ix_i + \\beta_1 \\sum_{i=1}^n w_ix_i^2 &= \\sum_{i=1}^n w_ix_iy_i \\end{align}$$ which can be written as $${\\bf (X'WX)\\bbeta = X'Wy}$$\n\n\n\n## Methods of Choosing Weights\n\n- Prior knowledge or information\n\n- Residual analysis\n  + $\\var(\\epsilon_i) \\propto x_{i}$, suggest $w_i = 1/x_{i}$\n  + When $y_i$ is an *average* of $n_i$ observations at $x_i$, $\\var(y_i) =\\sigma^2 / n_{i}$ and suggest $w_i = n_i$\n  \n- Chosen inversely proportional to variances of measurement error\n\n<!-- - Guess at the weights and iteratively estimate them. -->\n\n\n\n::: notes\n.alert[\nIf we have no idea of $\\bf W$, consider **feasible GLS** estimator $\\hat{\\bsbeta}_{FGLS} = {\\bf(X'\\hat{W}X)^{-1}X'\\hat{W}y}$, where $\\bf \\hat{W}$ is an estimate of $\\bf W$ from some method. \n]\n- The weights, $w_i$, or in general $\\bf V$ must be *known.*\n- Prior knowledge or information\n- Residual analysis\n  + $\\var(\\epsilon_i) \\propto x_{i}$, suggest $w_i = 1/x_{i}$\n  + When $y_i$ is an *average* of $n_i$ observations at $x_i$, $\\var(y_i) =\\sigma^2 / n_{i}$ and suggest $w_i = n_i$\n- Chosen inversely proportional to variances of measurement error\n- Guess at the weights and iteratively estimate them.\n:::\n\n\n## [R Lab]{.pink} Example 5.5 Food Sales in LRA\n- The average monthly revenue vs. annual advertising\nexpenses for 30 restaurants.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nx <- c(3000, 3150, 3085, 5225, 5350, 6090, 8925, 9015, 8885, 8950, 9000, 11345,\n       12275, 12400, 12525, 12310, 13700, 15000, 15175, 14995, 15050, 15200,\n       15150, 16800, 16500, 17830, 19500, 19200, 19000, 19350)\ny <- c(81464, 72661, 72344, 90743, 98588, 96507, 126574, 114133, 115814, 123181,\n       131434, 140564, 151352, 146926, 130963, 144630, 147041, 179021, 166200, 180732,\n       178187, 185304, 155931, 172579, 188851, 192424, 203112, 192482, 218715, 214317)\ngroup <- c(1, 1, 1, 2, 2, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 7, 7, 7, 7, 7, 7, \n           8, 8, 9, 10, 10, 10, 10)\nex5_5 <- data.frame(\"expense\" = x, \"sales\" = y, \"group\" = group)\npar(mar = c(3.3, 3.3, 0, 0), mgp = c(2, 1, 0))\nplot(ex5_5$expense, ex5_5$sales, xlab = \"Advertising expense\", \n     ylab = \"Food sales\", col = group, pch = 16, cex = 2, cex.lab = 1.5)\n```\n\n::: {.cell-output-display}\n![](./images/09-diag-const-var/unnamed-chunk-10-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n::: notes\n- The average monthly income from food sales vs. annual advertising\nexpenses for 30 restaurants.\n- Management is interested in the relationship between these variables.\n<!-- - Management is interested in the relationship between these variables. -->\n:::\n\n\n## [R Lab]{.pink} Non-constant Variance\n- $\\hat{y} = 49443.384 + 8.048x$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/09-diag-const-var/unnamed-chunk-11-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n## [R Lab]{.pink}  Determine Weights\n- There are several sets of $x$ values that are *\"near neighbors\"*.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nex5_5$expense\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  3000  3150  3085  5225  5350  6090  8925  9015  8885  8950  9000 11345\n[13] 12275 12400 12525 12310 13700 15000 15175 14995 15050 15200 15150 16800\n[25] 16500 17830 19500 19200 19000 19350\n```\n:::\n\n```{.r .cell-code}\nex5_5$group\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  1  1  1  2  2  3  4  4  4  4  4  5  5  5  5  5  6  7  7  7  7  7  7  8  8\n[26]  9 10 10 10 10\n```\n:::\n:::\n\n\n- Assume that these near neighbors are close enough to be considered repeat points.\n- Use the variance of the responses at those repeat points to investigate how $\\var(y)$ changes with $x.$\n\n\n## [R Lab]{.pink}  Determine Weights\n\n- The empirical variance of $y$, $s_y^2$, increases approximately linearly with $\\bar{x}$.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/09-diag-const-var/unnamed-chunk-13-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n## [R Lab]{.pink}  Determine Weights\n- $\\hat{s}^2_y = -9226002 + 7781.6\\bar{x}$\n- Substituting each $x_i$ value into this equation will give an estimate of the variance of the corresponding observation $y_i$, $\\hat{s}^2_i$\n- The inverse of these  $\\hat{s}^2_i$ will be reasonable estimates of the weights $w_i$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ns2_fit <- lm(s2_y~x_bar)\n(coef <- s2_fit$coef)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)       x_bar \n   -9226002        7782 \n```\n:::\n\n```{.r .cell-code}\ns2_i <- coef[1] + coef[2] * ex5_5$expense\n(wt <- 1 / s2_i)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 7.1e-08 6.5e-08 6.8e-08 3.2e-08 3.1e-08 2.6e-08 1.7e-08 1.6e-08 1.7e-08\n[10] 1.7e-08 1.6e-08 1.3e-08 1.2e-08 1.1e-08 1.1e-08 1.2e-08 1.0e-08 9.3e-09\n[19] 9.2e-09 9.3e-09 9.3e-09 9.2e-09 9.2e-09 8.2e-09 8.4e-09 7.7e-09 7.0e-09\n[28] 7.1e-09 7.2e-09 7.1e-09\n```\n:::\n:::\n\n\n\n## [R Lab]{.pink} WLS fit\n\n::: {.cell layout-align=\"center\" output.lines='[5,6,7]'}\n\n```{.r .cell-code}\n(wls_fit <- lm(ex5_5$sales ~ ex5_5$expense, \n               weights = wt))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n...\nCoefficients:\n  (Intercept)  ex5_5$expense  \n     51024.81           7.92  \n...\n```\n:::\n:::\n\n\n- $\\hat{y} = 51024.8 + 7.918x$\n- We must examine the residuals to determine if using WLS has improved the fit. \n- Plot the **weighted** residuals $w_i^{1/2}(y_i - \\hat{y}_i^W)$, where $\\hat{y}_i^W$ comes form the WLS fit, against $w_i^{1/2}\\hat{y}_i^W$\n\n\n\n::: notes\n.alert[\nThe WLS residuals and the WLS fitted values when the model assumes $\\var(\\beps) = \\sigma^2 {\\bf W}^{-1}$ should behave as the OLS residuals and OLS fitted values when the model assumes $\\var(\\beps) = \\sigma^2 {\\bf I}$.\n:::\n\n## [R Lab]{.pink} WLS Residual Plot\n- The residual plot has no significant fanning.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/09-diag-const-var/unnamed-chunk-16-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n<!-- ## Other Methods for Dealing with Nonconstant Variance {visibility=\"hidden\"} -->\n\n<!-- - Testing (Breusch and Pagan (1979)) -->\n\n<!-- ```{r, eval=FALSE} -->\n<!-- car::ncvTest(ciafit) -->\n<!-- ``` -->\n\n\n<!-- Nonconstant variance results in unreliable inference results: -->\n\n<!-- - Robust coefficient standard errors (Huber (1967), White (1980), Long and Ervin (2000)) -->\n\n<!-- ```{r, eval=FALSE} -->\n<!-- car::hccm(ciafit)  ## Heteroscedasticity-Corrected Covariance Matrices -->\n<!-- ``` -->\n\n<!-- - Bootstrapping (Efron and Tibshirani (1993)) -->\n\n<!-- ```{r, eval=FALSE} -->\n<!-- car::Boot(ciafit) -->\n<!-- ``` -->\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}