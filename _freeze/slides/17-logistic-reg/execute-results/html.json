{
  "hash": "72cf4cff4ab7a2c016839a76e28e8540",
  "result": {
    "markdown": "---\ntitle: 'Logistic Regression `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 640 512\" style=\"height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M384 96V320H64L64 96H384zM64 32C28.7 32 0 60.7 0 96V320c0 35.3 28.7 64 64 64H181.3l-10.7 32H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H352c17.7 0 32-14.3 32-32s-14.3-32-32-32H277.3l-10.7-32H384c35.3 0 64-28.7 64-64V96c0-35.3-28.7-64-64-64H64zm464 0c-26.5 0-48 21.5-48 48V432c0 26.5 21.5 48 48 48h64c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48H528zm16 64h32c8.8 0 16 7.2 16 16s-7.2 16-16 16H544c-8.8 0-16-7.2-16-16s7.2-16 16-16zm-16 80c0-8.8 7.2-16 16-16h32c8.8 0 16 7.2 16 16s-7.2 16-16 16H544c-8.8 0-16-7.2-16-16zm32 160a32 32 0 1 1 0 64 32 32 0 1 1 0-64z\"/></svg>`{=html}'\nsubtitle: \"MATH/COSC 3570 Introduction to Data Science\"\nauthor: \"Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University\"\n# date: \"December 23 2023\"\n# macros: _macros.tex # import a list of TeX/LaTeX definitions\nformat: \n  revealjs:\n    #     - \"macros.tex\"\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n    # include-in-header:\n    highlight-style: arrow\n    code-block-bg: true\n    self-contained: false\n    slide-number: c/t    \n    incremental: false\n    width: 1800\n    height: 1000\n    margin: 0.05\n    logo: \"https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg\"\n    footer: \"[math3570-s24.github.io/website](https://math3570-s24.github.io/website/)\"\n    theme: [\"simple\", \"slides.scss\"]\n    multiplex: true\n    code-link: true\n    fig-cap-location: bottom\n    fig-align: center\n    transition: none ## fade slide convex concave zoom\n    title-slide-attributes:\n      data-background-color: \"#447099\"\n      # data-background-image: images/paper-texture.jpg\n      # data-background-size: cover\n      # data-background-color: \"#698ED5\"\neditor: source\nexecute:\n  freeze: true\n  echo: true\n---\n\n\n#  {visibility=\"hidden\"}\n\n\\def\\bx{\\mathbf{x}}\n\\def\\bg{\\mathbf{g}}\n\\def\\bw{\\mathbf{w}}\n\\def\\bbeta{\\boldsymbol \\beta}\n\\def\\bX{\\mathbf{X}}\n\\def\\by{\\mathbf{y}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bW{\\mathbf{W}}\n\\def\\T{\\text{T}}\n\\def\\cov{\\mathrm{Cov}}\n\\def\\cor{\\mathrm{Corr}}\n\\def\\var{\\mathrm{Var}}\n\\def\\E{\\mathrm{E}}\n\\def\\bmu{\\boldsymbol \\mu}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\def\\Trace{\\text{Trace}}\n\n\n\n\n\n\n\n\n## Regression vs. Classification\n- Linear regression assumes that the response $Y$ is *numerical*.\n- In many situations, $Y$ is *categorical*!\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n:::{.center}\n**normal email vs. spam email**\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/17-logistic-reg/spam_filter.jpg){fig-align='center' width=70%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n:::{.center}\n**fake news vs. true news**\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/17-logistic-reg/fake_news.jpeg){fig-align='center' width=90%}\n:::\n:::\n\n:::\n::::\n\n- A process of predicting categorical response is known as **classification**.\n\n::: notes\n+ eye color\n+ car brand\n+ true vs. fake news\n:::\n\n\n## Regression Function $f(x)$ vs. Classifier $C(x)$\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/17-logistic-reg/regression.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n. . .\n\n\n:::{.small}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: https://daviddalpiaz.github.io/r4sl/classification-overview.html](./images/17-logistic-reg/classification.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: notes\n- There are many classification tools, or **classifiers** used to predict a categorical response.\n:::\n\n\n## Classification Example\n- Predict whether people will default on their credit card payment $(Y)$ `yes` or `no`, based on monthly credit card balance $(X)$.\n\n- We use the training sample $\\{(x_1, y_1), \\dots, (x_n, y_n)\\}$ to build a classifier.\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/17-logistic-reg/unnamed-chunk-6-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n\n::: {.column width=\"40%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/17-logistic-reg/credit_card.jpeg){fig-align='center' width=100%}\n:::\n:::\n\n:::\n::::\n\n\n## Why Not Linear Regression?\n\n- Most of the time, we code categories using numbers!\n\n$$Y =\\begin{cases}\n    0  & \\quad \\text{if not default}\\\\\n    1  & \\quad \\text{if default}\n     \\end{cases}$$\n     \n\n- $Y = \\beta_0 + \\beta_1X + \\epsilon$, $\\, X =$ credit card balance \n\n:::{.question}\nAny potential issue of this dummy variable approach?\n:::\n\n<!-- - Linear regression assumes that the response is -->\n<!--   + Normally distributed -->\n<!--   + Constant variance -->\n<!--   + Independent -->\n\n::: notes\n- $Y$ is categorical but coded as dummy variable or indicator variable\n- Fit linear regression and treat it as a numerical variable.\n:::\n\n## Why Not Linear Regression? {visibility=\"hidden\"}\n\n- $\\hat{Y} = b_0 + b_1X$ estimates $P(Y = 1 \\mid X) = P(default = yes \\mid balance)$\n<!-- - Brown bars are data points. -->\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/17-logistic-reg/lm_default-1.png){fig-align='center' width=72%}\n:::\n:::\n\n\n\n## Why Not Linear Regression?\n\n- $\\hat{Y} = b_0 + b_1X$\n- **Some estimates might be outside $[0, 1]$.**\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/17-logistic-reg/unnamed-chunk-8-1.png){fig-align='center' width=72%}\n:::\n:::\n\n\n\n\n## Why Not Linear Regression?\n\n- First predict the **probability** of each category of $Y$.\n\n- Predict probability of `default` using a <span style=\"color:blue\">**S-shaped** curve</span>.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/17-logistic-reg/glm_default-1.png){fig-align='center' width=65%}\n:::\n:::\n\n\n\n# Logistic Regression\n\n\n## Framing the Problem: Binary Responses {visibility=\"hidden\"}\n- Treat each outcome (`default` $(y = 1)$ and `not default` $(y = 0)$) as success and failure arising from separate **Bernoulli** trials.\n\n:::{.question}\nWhat is a Bernoulli trial?\n:::\n\n:::{.question}\nWhat is a binomial trial?\n:::\n\n- A Bernoulli trial is a special case of a binomial trial when the number of trials is one:\n  - **exactly two** possible outcomes, \"success\" and \"failure\"\n  - the probability of success $\\pi$ is **constant**\n\n:::{.question}\nIn the default credit card example, \n\n- do we have exactly two outcomes? \n\n- do we have constant probability? $P(y_1 = 1) = P(y_2 = 1) = \\cdots = P(y_n = 1) = \\pi?$\n\n:::\n\n## Framing the Problem: Binary Responses {visibility=\"hidden\"}\n\n- Training data $(x_1, y_1), \\dots, (x_n, y_n)$ where $y_i = 1$ (`default`) or $0$ (`not default`).\n\n- We first predict $P(y_i = 1 \\mid x_i) = \\pi(x_i) = \\pi_i$\n\n- The probability $\\pi$ *changes with* the value of predictor $x$!\n\n. . .\n\n- $X =$ `balance`. $x_1 = 2000$ has a larger $\\pi_1 = \\pi(2000)$ than $\\pi_2 = \\pi(500)$ with $x_2 = 500$.\n\n- Credit cards with a higher balance is more likely to be default.\n\n\n\n## Binary Responses with Nonconstant Probability\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n<!-- - Two outcomes: `default` $(y = 1)$ and `not default` $(y = 0)$ -->\n\n- Training data $(x_1, y_1), \\dots, (x_n, y_n)$ where $y_i = 1$ (`default`) or $0$ (`not default`).\n\n- We first predict $P(y_i = 1 \\mid x_i) = \\pi(x_i) = \\pi_i$\n\n- The probability $\\pi$ *changes with* the value of predictor $x$!\n\n:::\n\n\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/17-logistic-reg/unnamed-chunk-9-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n::::\n\n. . .\n\n<!-- $y_i \\mid x_i \\stackrel{indep}{\\sim} \\text{Bernoulli}(\\pi(x_i)) = \\text{binomial}(m=1,\\pi = \\pi(x_i))$ -->\n\n- $X =$ `balance`. $x_1 = 2000$ has a larger $\\pi_1 = \\pi(2000)$ than $\\pi_2 = \\pi(500)$ with $x_2 = 500$.\n\n- Credit cards with a higher balance is more likely to be default.\n\n\n\n## Logistic Regression\n\n:::{.center}\nInstead of predicting $y_i$ directly, we use predictors to model its *probability* of success, $\\pi_i$.\n:::\n\n:::{.center}\nBut how?\n:::\n\n\n. . .\n\n- **Transform $\\pi \\in (0, 1)$ into another variable $\\eta \\in (-\\infty, \\infty)$. Then construct a linear predictor on on $\\eta$**: $$\\eta_i = \\beta_0 + \\beta_1 x_i$$\n\n\n. . .\n\n- **Logit function:** For $0 < \\pi < 1$\n\n$$\\eta = \\text{logit}(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)$$\n\n- The *logit* function takes a value $\\pi \\in (0, 1)$ and maps it to a value $\\eta \\in (-\\infty, \\infty)$.\n\n\n\n## Logit function $\\eta = \\text{logit}(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)$\n\n\n::: {.cell layout-align=\"center\" hash='17-logistic-reg_cache/revealjs/unnamed-chunk-10_45df7605d624025461b365bd222a5c50'}\n::: {.cell-output-display}\n![](images/17-logistic-reg/unnamed-chunk-10-1.png){fig-align='center' width=72%}\n:::\n:::\n\n\n\n## Logistic Function\n\n- The *logistic* function takes a value $\\eta \\in (-\\infty, \\infty)$ and maps it to a value $\\pi \\in (0, 1)$.\n\n- **Logistic function**:\n$$\\pi = \\text{logistic}(\\eta) = \\frac{1}{1+\\exp(-\\eta)} \\in (0, 1)$$\n\n. . .\n\n- So once $\\eta$ is estimated by the linear predictor, we use the logistic function to transform $\\eta$ back to the probability.\n\n## Logistic Function $\\pi = \\text{logistic}(\\eta) = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)}$\n\n::: {.cell layout-align=\"center\" hash='17-logistic-reg_cache/revealjs/unnamed-chunk-11_b4cd2eda3c322ae08003b61361cd56ae'}\n::: {.cell-output-display}\n![](images/17-logistic-reg/unnamed-chunk-11-1.png){fig-align='center' width=72%}\n:::\n:::\n\n\n\n## Simple Logistic Regression Model\n\nFor $i = 1, \\dots, n$, and with one predictor $X$:\n  $$(Y_i \\mid X = x_i) = \\begin{cases}   1       & \\quad \\text{w/ prob } \\pi(x_i)\\\\\n    0  & \\quad \\text{w/ prob } 1 - \\pi(x_i) \\end{cases}$$\n  $$\\text{logit}(\\pi_i) = \\ln \\left( \\frac{\\pi(x_i)}{1 - \\pi(x_i)} \\right) = \\eta_i = \\beta_0+\\beta_1 x_{i}$$\n<!-- - The $\\text{logit}(\\pi_i)$ is a **link function** that *links* the linear predictor and the mean of $Y_i$. -->\n\n. . .\n\nGoal: Get estimates $\\hat{\\beta}$ and $\\hat{\\beta}_1$, and therefore $\\hat{\\pi}_i$!\n\n\n<!-- $$\\small \\pi_i = \\frac{1}{1+\\exp(-(\\beta_0+\\beta_1 x_{i}))} = \\frac{1}{1 + \\exp(-\\eta_i)}$$ -->\n\n\n$$\\small \\hat{\\pi}_i = \\frac{1}{1+\\exp(-\\hat{\\beta}_0-\\hat{\\beta}_1 x_{i})}$$\n\n\n\n::: notes\n- with sample size $n$ and with $k$ predictors, we have the logistic regression model like this\n- First, we have a probability distribution **Bernoulli** describing how the outcome or response data are generated. \n  - $Y_i \\mid {\\bf x}_i; \\pi_i \\sim \\text{Bern}(p_i)$, ${\\bf x}_i = (x_{1,i}, \\cdots, x_{k,i})$, $i = 1, \\dots, n$\n- Then we have a link function, logit function, that relates the linear regression to the parameter of the outcome distribution, which is the parameter $p$, the probability of success in the Bernoulli distribution.\n  - $\\text{logit}(\\pi_i) = \\eta_i = \\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i}$\n:::\n\n\n\n## Probability Curve\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n- The relationship between $\\pi(x)$ and $x$ is not linear!\n$$\\pi(x) = \\frac{1}{1+\\exp(-\\beta_0-\\beta_1 x)}$$\n- The amount that $\\pi(x)$ changes due to a one-unit change in $x$ depends on the current value of $x$.\n- Regardless of the value of $x$, if $\\beta_1 > 0$, increasing $x$ will be increasing $\\pi(x)$.\n:::\n\n\n\n::: {.column width=\"60%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/17-logistic-reg/unnamed-chunk-12-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n::::\n\n\n::: notes\n\n\n::: {.cell layout-align=\"center\" hash='17-logistic-reg_cache/revealjs/unnamed-chunk-13_96e1f23d90b346ae4130b830b4a74e54'}\n::: {.cell-output-display}\n![](images/17-logistic-reg/unnamed-chunk-13-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n\n\n\n## Interpretation of Coefficients {visibility=\"hidden\"}\n\nThe ratio $\\frac{\\pi}{1-\\pi} \\in (0, \\infty)$ is called the **odds** of some event.\n\n\n- Example: If 1 in 5 people will default, the odds is 1/4 since $\\pi = 0.2$ implies an odds of $0.2/(1−0.2) = 1/4$.\n\n\n\n$$\\ln \\left( \\frac{\\pi(x)}{1 - \\pi(x)} \\right)= \\beta_0 + \\beta_1x$$\n\n- Increasing $x$ by one unit \n  + changes the **log-odds** by $\\beta_1$\n  + multiplies the odds by $e^{\\beta_1}$\n\n\n\n:::{.alert}\n- $\\beta_1$ does *not* correspond to the change in $\\pi(x)$ associated with a one-unit increase in $x$.\n- $\\beta_1$ is the change in **log odds** associated with one-unit increase in $x$.\n:::\n\n\n## Fit Logistic Regression\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbodydata <- read_csv(\"./data/body.csv\")\nbody <- bodydata |> \n    select(GENDER, HEIGHT) |> \n    mutate(GENDER = as.factor(GENDER))\nbody |> slice(1:4) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 2\n  GENDER HEIGHT\n  <fct>   <dbl>\n1 0        172 \n2 1        186 \n3 0        154.\n4 1        160.\n```\n:::\n:::\n\n\n\n- `GENDER = 1` if male\n- `GENDER = 0` if female\n- Use `HEIGHT` (centimeter, 1 cm = 0.39 in) to predict/classify `GENDER`: whether one is male or female.\n:::\n\n::: {.column width=\"40%\"}\n\n:::{.tiny}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: https://www.thetealmango.com/featured/average-male-and-female-height-worldwide/](./images/17-logistic-reg/height.jpeg){fig-align='center' width=100%}\n:::\n:::\n\n:::\n:::\n::::\n\n\n## Logistic Regression - Data Summary\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntable(body$GENDER)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  0   1 \n147 153 \n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbody |> ggplot(aes(x = GENDER, y = HEIGHT)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](images/17-logistic-reg/unnamed-chunk-17-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n<!-- :::: {.columns} -->\n\n<!-- ::: {.column width=\"60%\"} -->\n<!-- ```{r} -->\n<!-- table(body$GENDER) -->\n<!-- tapply(body$HEIGHT, body$GENDER, summary) -->\n<!-- ``` -->\n<!-- ::: -->\n\n\n\n<!-- ::: {.column width=\"40%\"} -->\n<!-- ```{r} -->\n<!-- #| out-width: \"100%\" -->\n<!-- body |> ggplot(aes(x = GENDER,  -->\n<!--                    y = HEIGHT)) + -->\n<!--     geom_boxplot() -->\n<!-- ``` -->\n<!-- ::: -->\n<!-- :::: -->\n\n\n## Logistic Regression - Model Fitting\n\n- Specify the model with `logistic_reg()` [![parsnip](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/parsnip.png){width=60}](https://parsnip.tidymodels.org/reference/logistic_reg.html#:~:text=logistic_reg()%20defines%20a%20generalized,by%20setting%20the%20model%20engine.)\n\n- Use `\"glm\"` instead of `\"lm\"` as the engine\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlogis_mdl <- parsnip::logistic_reg() |> \n    set_engine(\"glm\") \n```\n:::\n\n\n. . .\n\n- Define `family = \"binomial\"`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogis_out <- logis_mdl |> \n    fit(GENDER ~ HEIGHT, \n        data = body, \n        family = \"binomial\")\n```\n:::\n\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogis_out$fit$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)      HEIGHT \n    -40.548       0.242 \n```\n:::\n:::\n\n\n## Logistic Regression Model Result {visibility=\"hidden\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogis_out$fit$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)      HEIGHT \n    -40.548       0.242 \n```\n:::\n:::\n\n\n\n- $\\hat{\\eta} = \\ln \\left( \\frac{\\hat{\\pi}}{1 - \\hat{\\pi}}\\right) = -40.55 + 0.24 \\times \\text{HEIGHT}$\n\n<!-- . . . -->\n\n<!-- - $\\hat{\\eta}(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1x$ -->\n<!-- - $\\hat{\\eta}(x+1) = \\hat{\\beta}_0 + \\hat{\\beta}_1(x+1)$ -->\n<!-- - $\\hat{\\eta}(x+1) - \\hat{\\eta}(x) = \\hat{\\beta}_1 = \\ln(\\text{odds}_{x+1}) - \\ln(\\text{odds}_{x}) = \\ln \\left( \\frac{\\text{odds}_{x+1}}{\\text{odds}_{x}} \\right)$ -->\n\n. . .\n\n- One cm increase in `HEIGHT` increases the *log odds* of being male by 0.24 units.\n- The **odds ratio**, $\\widehat{OR} = \\frac{\\text{odds}_{x+1}}{\\text{odds}_{x}} = e^{\\hat{\\beta}_1} = e^{0.24} = 1.273$.\n- The odds of being male increases by 27.3% with additional one cm of `HEIGHT`.\n\n\n## Pr(GENDER = 1) When HEIGHT is 170 cm\n\n\n- $\\hat{\\eta} = \\ln \\left( \\frac{\\hat{\\pi}}{1 - \\hat{\\pi}}\\right) = -40.55 + 0.24 \\times \\text{HEIGHT}$\n\n\n$$ \\hat{\\pi}(x = 170) = \\frac{1}{1+\\exp(-\\hat{\\beta}_0-\\hat{\\beta}_1 x)} = \\frac{1}{1+\\exp(-(-40.55) - 0.24 \\times 170)} = 63.3\\%$$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(logis_out$fit, newdata = data.frame(HEIGHT = 170), type = \"response\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    1 \n0.633 \n```\n:::\n:::\n\n\n<!-- . . . -->\n\n<!-- :::{.question} -->\n<!-- What is the probability of being male when `HEIGHT` is 160? What about `HEIGHT` 180? -->\n<!-- ::: -->\n\n\n\n\n\n\n## Probability Curve\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npi_hat <- predict(logis_out$fit, type = \"response\")\npi_hat |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     1      2      3      4      5      6 \n0.7369 0.9880 0.0383 0.1480 0.9383 0.4375 \n```\n:::\n\n```{.r .cell-code}\nbody$HEIGHT |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 172 186 154 160 179 167\n```\n:::\n:::\n\n\n\n<!-- ```{r} -->\n<!-- predict(logis_out$fit, newdata = data.frame(HEIGHT = c(160, 170, 180)), type = \"response\") -->\n<!-- ``` -->\n\n:::: {.columns}\n\n::: {.column width=\"70%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/17-logistic-reg/default-predict-viz-1.png){fig-align='center' width=85%}\n:::\n:::\n\n:::\n\n::: {.column width=\"30%\"}\n- **[160 cm, Pr(male) = 0.13]{.pink}**\n- **[170 cm, Pr(male) = 0.63]{.yellow}**\n- **[180 cm, Pr(male) = 0.95]{.green}**\n:::\n::::\n\n\n# {background-color=\"#ffde57\" background-image=\"https://upload.wikimedia.org/wikipedia/commons/0/05/Scikit_learn_logo_small.svg\" background-size=\"40%\" background-position=\"90% 50%\"}\n\n\n::: {.left}\n<h1> sklearn.linear_model </h1>\n\n:::\n\n\n## [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(reticulate); py_install(\"scikit-learn\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n```\n:::\n\n\n. . .\n\n<br>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nbody = pd.read_csv('./data/body.csv')\nx = np.array(body[['HEIGHT']]) ## 2d array with one column\ny = np.array(body['GENDER']) ## 1d array\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nx[0:4]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([[172. ],\n       [186. ],\n       [154.4],\n       [160.5]])\n```\n:::\n\n```{.python .cell-code}\ny[0:4]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([0, 1, 0, 1])\n```\n:::\n:::\n\n\n\n## [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nclf = LogisticRegression().fit(x, y)\nclf.coef_\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([[0.24154889]])\n```\n:::\n\n```{.python .cell-code}\nclf.intercept_\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([-40.51727266])\n```\n:::\n:::\n\n\n. . .\n\n<br>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nnew_height = np.array([160, 170, 180]).reshape(-1, 1)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nclf.predict(new_height)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([0, 1, 1])\n```\n:::\n:::\n\n\n. . .\n\n<br>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nclf.predict_proba(new_height)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([[0.86639467, 0.13360533],\n       [0.36678399, 0.63321601],\n       [0.04919451, 0.95080549]])\n```\n:::\n:::\n\n\n\n# Evaluation\n\n\n\n\n\n\n<!-- # Evaluation Metrics -->\n\n## Evaluation Metrics^[More on [Wiki page](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).]\n\n- **Confusion Matrix**\n\n|                        | True 0               | True 1            |\n|------------------------|-------------------------------|-------------------------------|\n| **Predict 0** |  **True Negative  (TN)** | **False Negative (FN)**|\n| **Predict 1**  |  **False Positive (FP)**|  **True Positive  (TP)**           | \n\n\n\n- **Sensitivity (True Positive Rate)** $= P( \\text{predict 1} \\mid \\text{true 1}) = \\frac{TP}{TP+FN}$\n\n- **Specificity (True Negative Rate)** $= P( \\text{predict 0} \\mid \\text{true 0}) = \\frac{TN}{FP+TN}$ \n\n- **Accuracy** $= \\frac{TP + TN}{TP+FN+FP+TN}$\n\n. . .\n\nA *good* classifier is one which the *test accuracy rate is highest*.\n\n\n\n## Confusion Matrix\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred_prob <- predict(logis_out$fit, type = \"response\")\n\n## true observations\ngender_true <- body$GENDER\n\n## predicted labels\ngender_predict <- (pred_prob > 0.5) * 1\n\n## confusion matrix\ntable(gender_predict, gender_true)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              gender_true\ngender_predict   0   1\n             0 118  29\n             1  29 124\n```\n:::\n:::\n\n\n\n## Receiver Operating Characteristic (ROC) Curve\n|                        | True 0               | True 1            |\n|------------------------|-------------------------------|-------------------------------|\n| **Predict 0** |  **True Negative  (TN)** | **False Negative (FN)**|\n| **Predict 1**  |  **False Positive (FP)**|  **True Positive  (TP)**           |\n\n- **Receiver operating characteristic (ROC) curve** plots True Positive Rate (Sensitivity) vs. False Positive Rate (1 - Specificity)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/17-logistic-reg/unnamed-chunk-33-1.png){fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n## [ROC Curve](https://yardstick.tidymodels.org/reference/roc_curve.html) using [![yardstick of Tidymodels](https://raw.githubusercontent.com/rstudio/hex-stickers/main/thumbs/yardstick.png){width=60}](https://yardstick.tidymodels.org/)  {visibility=\"hidden\"}\n\n\n<!-- ![yardstick of Tidymodels](https://raw.githubusercontent.com/rstudio/hex-stickers/main/thumbs/yardstick.png){width=\"50\"} -->\n<!-- [yardstick of Tidymodels](https://yardstick.tidymodels.org/) -->\n\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nroc_df <- tibble(truth = gender_true,\n                 prob = 1 - pred_prob,\n                 pred = gender_predict)\nroc_df\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 300 × 3\n  truth   prob  pred\n  <fct>  <dbl> <dbl>\n1 0     0.263      1\n2 1     0.0120     1\n3 0     0.962      0\n4 1     0.852      0\n5 1     0.0617     1\n6 1     0.562      0\n# ℹ 294 more rows\n```\n:::\n:::\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n<!-- ```{r} -->\n<!-- #| echo: false -->\n<!-- knitr::include_graphics(\"https://raw.githubusercontent.com/rstudio/hex-stickers/main/thumbs/yardstick.png\") -->\n<!-- ``` -->\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngender_roc <- \n     yardstick::roc_curve(roc_df, truth, prob)\ngender_roc |> slice(1:5, (n()-4):n()) |> \n    print(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 3\n   .threshold specificity sensitivity\n        <dbl>       <dbl>       <dbl>\n 1 -Inf           0           1      \n 2    0.00207     0           1      \n 3    0.00426     0.00654     1      \n 4    0.00492     0.0131      1      \n 5    0.00673     0.0196      1      \n 6    0.994       1           0.0272 \n 7    0.996       1           0.0204 \n 8    0.997       1           0.0136 \n 9    1.00        1           0.00680\n10  Inf           1           0      \n```\n:::\n:::\n\n:::\n::::\n\n## ROC Curve using [![yardstick of Tidymodels](https://raw.githubusercontent.com/rstudio/hex-stickers/main/thumbs/yardstick.png){width=60}](https://yardstick.tidymodels.org/)  {visibility=\"hidden\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngender_roc |> autoplot() +\n    theme(axis.title = element_text(size = 20),\n          axis.text = element_text(size = 16))\n```\n\n::: {.cell-output-display}\n![](images/17-logistic-reg/unnamed-chunk-36-1.png){fig-align='center' width=67%}\n:::\n:::\n\n\n\n::: notes\n + theme(axis.title = element_text(size = 20),\n          axis.text = element_text(size = 16))\n:::\n\n## Comparing Models\n\n:::{.question}\nWhich model performs better?\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/17-logistic-reg/unnamed-chunk-37-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n##\n::: {.lab}\n\n<span style=\"color:blue\"> **21-Logistic Regression** </span>\n\nIn **lab.qmd** `## Lab 21` section,\n\n- Use our fitted logistic regression model to predict whether you are male or female! Change `175` to your height (cm).\n\n- Use [the converter](https://www.thecalculatorsite.com/conversions/common/cm-to-feet-inches.php) to get your height in cm!\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Fit the logistic regression\n\npredict(logis_out$fit, newdata = data.frame(HEIGHT = 175), \n        type = \"response\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    1 \n0.853 \n```\n:::\n:::\n\n\n<!-- - Suppose data are collected with $X =$ hours studied, and $Y =$ receive an A or not in 3570. We fit a logistic regression and the estimated coefficients are $b_0 = −3$, $b_1 = 0.05$. -->\n<!--     + Estimate the probability that a student who studies for 40 hours gets an A in the class. -->\n<!--     + How many hours would the student need to study to have a 50% chance of getting an A in the class? -->\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}