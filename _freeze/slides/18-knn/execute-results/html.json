{
  "hash": "d844b7fd5603982fbaff1085ab88fc53",
  "result": {
    "markdown": "---\ntitle: 'Data Splitting and K-Nearest Neighbors `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 640 512\" style=\"height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M72 88a56 56 0 1 1 112 0A56 56 0 1 1 72 88zM64 245.7C54 256.9 48 271.8 48 288s6 31.1 16 42.3V245.7zm144.4-49.3C178.7 222.7 160 261.2 160 304c0 34.3 12 65.8 32 90.5V416c0 17.7-14.3 32-32 32H96c-17.7 0-32-14.3-32-32V389.2C26.2 371.2 0 332.7 0 288c0-61.9 50.1-112 112-112h32c24 0 46.2 7.5 64.4 20.3zM448 416V394.5c20-24.7 32-56.2 32-90.5c0-42.8-18.7-81.3-48.4-107.7C449.8 183.5 472 176 496 176h32c61.9 0 112 50.1 112 112c0 44.7-26.2 83.2-64 101.2V416c0 17.7-14.3 32-32 32H480c-17.7 0-32-14.3-32-32zm8-328a56 56 0 1 1 112 0A56 56 0 1 1 456 88zM576 245.7v84.7c10-11.3 16-26.1 16-42.3s-6-31.1-16-42.3zM320 32a64 64 0 1 1 0 128 64 64 0 1 1 0-128zM240 304c0 16.2 6 31 16 42.3V261.7c-10 11.3-16 26.1-16 42.3zm144-42.3v84.7c10-11.3 16-26.1 16-42.3s-6-31.1-16-42.3zM448 304c0 44.7-26.2 83.2-64 101.2V448c0 17.7-14.3 32-32 32H288c-17.7 0-32-14.3-32-32V405.2c-37.8-18-64-56.5-64-101.2c0-61.9 50.1-112 112-112h32c61.9 0 112 50.1 112 112z\"/></svg>`{=html}'\nsubtitle: \"MATH/COSC 3570 Introduction to Data Science\"\nauthor: \"Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University\"\n# date: \"December 23 2023\"\n# macros: _macros.tex # import a list of TeX/LaTeX definitions\nformat: \n  revealjs:\n    #     - \"macros.tex\"\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n    # include-in-header:\n    highlight-style: arrow\n    code-block-bg: true\n    self-contained: false\n    slide-number: c/t    \n    incremental: false\n    width: 1800\n    height: 1000\n    margin: 0.05\n    logo: \"https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg\"\n    footer: \"[math3570-s24.github.io/website](https://math3570-s24.github.io/website/)\"\n    theme: [\"simple\", \"slides.scss\"]\n    multiplex: true\n    code-link: true\n    fig-cap-location: bottom\n    fig-align: center\n    transition: none ## fade slide convex concave zoom\n    title-slide-attributes:\n      data-background-color: \"#447099\"\n      # data-background-image: images/paper-texture.jpg\n      # data-background-size: cover\n      # data-background-color: \"#698ED5\"\neditor: source\nexecute:\n  freeze: true\n  echo: true\n---\n\n\n#  {visibility=\"hidden\"}\n\n\\def\\bx{\\mathbf{x}}\n\\def\\bg{\\mathbf{g}}\n\\def\\bw{\\mathbf{w}}\n\\def\\bbeta{\\boldsymbol \\beta}\n\\def\\bX{\\mathbf{X}}\n\\def\\by{\\mathbf{y}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bW{\\mathbf{W}}\n\\def\\T{\\text{T}}\n\\def\\cov{\\mathrm{Cov}}\n\\def\\cor{\\mathrm{Corr}}\n\\def\\var{\\mathrm{Var}}\n\\def\\E{\\mathrm{E}}\n\\def\\bmu{\\boldsymbol \\mu}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\def\\Trace{\\text{Trace}}\n\n\n\n\n\n\n\n\n\n\n# {background-color=\"#A7D5E8\" background-image=\"https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/rsample.png\" background-size=\"30%\" background-position=\"90% 50%\"}\n\n::: {.left}\n<h1> Training and Test Data </h1>\n:::\n\n## Prediction\n- Goal: Build a good regression function or classifier in terms of **prediction accuracy**.\n\n\n. . .\n\n- The mechanics of prediction is **easy**:\n  - Plug in values of predictors to the model equation.\n  - Calculate the predicted value of the response $\\hat{y}$\n\n<!-- ??? -->\n<!-- - So, the mechanics of prediction is **easy**: -->\n<!--   - Once you have your model, you can Plug in values of predictors to the model equation -->\n<!--   - Calculate the predicted value of the response variable, $\\hat{y}$, either numerical or categorical. -->\n\n \n. . .\n\n- Getting it right is **hard**! **No guarantee that**\n  - the model estimates are close to the truth\n  - your model performs as well with new data (**test** data) as it did with your sample data (**training** data)\n  \n  \n<!-- ??? -->\n<!-- - But Getting it right is **hard**! -->\n<!--   - There is no guarantee the model estimates you have are correct -->\n<!--   - Or that your model will perform as well with new data as it did with your sample data -->\n<!-- - Test data are the new data that are not used for training or fitting our model, but the data we are interested in predicting its value. -->\n<!-- - So we care about the prediction performance on the test data much more than the performance on the training data. -->\n\n\n\n## Spending Our Data\n\n- Several steps to create a useful model: \n    + Parameter estimation\n    + Model selection\n    + Performance assessment, etc.\n\n. . .\n\n- Doing all of this on the entire data may lead to **overfitting**: \n\n<br>\n\n> [*The model performs well on the training data, but awfully predicts the response on the new data we are interested.*]{.green}\n\n<br>\n\n\n<!-- - **Allocate specific subsets of data for different tasks**, as opposed to allocating the largest possible amount to the model parameter estimation only (what we've done so far). -->\n\n\n<!-- ??? -->\n<!-- - When we are doing modeling, we are doing several steps to create a useful model,  -->\n<!--     + parameter estimation -->\n<!--     + model selection -->\n<!--     + performance assessment, etc. -->\n<!-- - Doing all of this on the entire data we have available may lead to **overfitting**. In classification, it means that our model labels the training response variable almost perfectly with very high classification accuracy, but the model performs very bad when fitted to the test data or incoming future emails for example. -->\n\n<!-- - What we wanna do is to  **Allocate specific subsets of data for different tasks**, as opposed to allocating the largest possible amount of the data to the model parameter estimation only which is exactly what we've done so far. Remember in linear regression, we also use the entire data set to train our linear regression model, and estimate the regression coefficients. -->\n<!-- - But now, if we wanna make sure that our model is good at predicting things, we probably want to avoid overfitting. And how? -->\n\n\n\n## Overfitting\n\n> [*The model performs well on the training data, but awfully predicts the response on the new data we are interested.*]{.green}\n\n- **Low error rate on observed data, but high prediction error rate on future unobserved data!**\n\n:::{.small}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: modified from https://i.pinimg.com/originals/72/e2/22/72e222c1542539754df1d914cb671bd7.png](./images/18-knn/overfit.png){fig-align='center' width=60%}\n:::\n:::\n\n:::\n\n::: notes\nhttps://i.pinimg.com/originals/72/e2/22/72e222c1542539754df1d914cb671bd7.png\n- Look at this illustration, and let's focus on the overfitting and classification case.\n- the blue and red points are our training data representing two categories, and the green points are the new data to be classified.\n- the black curve is the classification boundary that separates the two categories.\n- Based on the boundary, you can see that the classification performance on the training data set is perfect, because all blue points and red points are perfectly separated.\n- However, such classification rule generated by the training data may not be good for the new data.\n- With this boundary, ...\n- OK so, if we wanna make sure that our model is good at predicting things, we probably want to avoid overfitting. \n- But how?\n:::\n\n\n\n## Splitting Data\n\n- Often, we don't have another unused data to assess the performance of our model.\n\n- Solution: Pretend we have new data by splitting our data into **training set** and **test set (validation set)**!\n\n. . .\n\n- **Training set:**\n    - Sandbox for model building \n    - Spend most of our time using the training set to develop the model\n    - Majority of the original sample data (75% - 80%)\n    \n\n. . .\n\n- **Test set:**\n    - Held in reserve to determine efficacy of one or two chosen models\n    - Critical to look at it *once only*, otherwise it becomes part of the modeling process\n    - Remainder of the data (20% - 25%)\n  \n::: notes\n- **Allocate specific subsets of data for different tasks**, as opposed to allocating the largest possible amount to the model parameter estimation only (what we've done so far).\n- Well we do this by splitting our data. So we split our data into to sets, training set and testing set, or sometimes called validation set.\n- You can think about your training set as your sandbox for model building. You can do whatever you want, like data wrangling, data transformation, data tidying, and data visualization, all of which help you build an appropriate model.\n- So you Spend most of your time using the training set to develop the model\n- And this is the Majority of the original sample data, which is usually about 75% - 80% of your data. So you basically take a random sample from the data that is about 80% of it. \n- And you don't touch the remaining 20% of the data until you are ready to test your model performance.\n- So the test set is held in reserve to determine efficacy of one or two chosen models\n- Critical to look at it once, otherwise it becomes part of the modeling process\n- and that is the Remainder of the data, usually 20% - 25%\n- So ideally, we hope to use our entire data as training data to train our model, right? And to test the model performance, we just collect another data set as test data to be used for testing performance. But in reality, it is not the usual case. In reality, we only have one single data set, and it is hard to collect another sample data as test data.\n- So under this situation, this type of splitting data becomes a must if we want to have both training and test data. \n:::\n\n\n\n## initial_split() in [![rsample](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/rsample.png){width=60}](https://rsample.tidymodels.org/)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nbodydata <- read_csv(\"./data/body.csv\")\nbody <- bodydata |> \n    select(GENDER, HEIGHT, WAIST, BMI) |> \n    mutate(GENDER = as.factor(GENDER))\n```\n:::\n\n\n<br>\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(2023)\ndf_split <- rsample::initial_split(\n    data = body, \n    prop = 0.8)\n\ndf_split\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Training/Testing/Total>\n<240/60/300>\n```\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf_trn <- rsample::training(df_split)\ndf_tst <- rsample::testing(df_split)\n\ndim(df_trn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 240   4\n```\n:::\n\n```{.r .cell-code}\ndim(df_tst)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 60  4\n```\n:::\n:::\n\n:::\n::::\n\n\n::: notes\nnames(df_split)\n\ndf_split$in_id |> head()\n:::\n\n\n## `body` Data\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf_trn\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 240 × 4\n  GENDER HEIGHT WAIST   BMI\n  <fct>   <dbl> <dbl> <dbl>\n1 1        185. 115    30.2\n2 1        171.  78    22.2\n3 0        155. 104.   30.4\n4 0        156. 132.   47.2\n5 0        157.  90.5  24.6\n6 0        170. 134.   36.7\n# ℹ 234 more rows\n```\n:::\n:::\n\n:::\n\n\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf_tst\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 60 × 4\n  GENDER HEIGHT WAIST   BMI\n  <fct>   <dbl> <dbl> <dbl>\n1 0        172  120.   33.3\n2 1        166.  95    25.8\n3 1        181. 119.   37.4\n4 0        164   75.5  19.3\n5 1        181.  92.5  27.4\n6 0        156. 110    33.6\n# ℹ 54 more rows\n```\n:::\n:::\n\n:::\n::::\n\n\n\n## What Makes a Good Classifier: Test Accuracy Rate\n\n- The **test accuracy rate** associated with the *test data* $\\{x_j, y_j\\}_{j=1}^J$:\n$$ \\frac{1}{J}\\sum_{j=1}^JI(y_j = \\hat{y}_j),$$\nwhere $\\hat{y}_j$ is the predicted label resulting from applying the classifier to the test response $y_j$ with predictor $x_j$.\n\n\n:::{.question}\nWhat is the value of $J$ in our example?\n:::\n\n. . .\n\n- The *best* estimated **classifier** $\\hat{C}(x)$ trained from the training data for $C(x)$ is the one producing the *highest test accuracy rate* or *lowest test error rate*.\n\n\n\n## K-Nearest Neighbors (KNN) Classifier\n\nKNN classification uses *majority voting*:\n\n:::{.center}\n[*Look for the most popular class label among its neighbors*.]{.green}\n:::\n\n<!-- - $\\pi_{K}(x) = \\hat{P}(Y = 1 \\mid X = x) = \\frac{1}{K}\\sum_{i \\in \\mathcal{N}_x}I(y_i = 1)$ -->\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n<!-- - *Neighbors in __Euclidean distance__ sense.* -->\n\n\n\n::: {.cell layout-align=\"center\" hash='18-knn_cache/revealjs/unnamed-chunk-8_11936b38ee3dbc8ae2073de276198ad5'}\n::: {.cell-output-display}\n![](images/18-knn/unnamed-chunk-8-1.png){fig-align='center' width=80%}\n:::\n:::\n\n:::\n\n\n\n::: {.column width=\"50%\"}\n\nWhen predicting at $x = (x_1, x_2) = (8, 6)$,\n\n\n\\begin{align}\n\\hat{\\pi}_{3Blue}(x = (8, 6)) &= \\hat{P}(Y = \\text{Blue} \\mid x = (8, 6))\\\\\n&= \\frac{2}{3}\n\\end{align}\n\n\\begin{align}\n\\hat{\\pi}_{3Orange}(x = (8, 6)) &= \\hat{P}(Y = \\text{Orange} \\mid x = (8, 6))\\\\\n&= \\frac{1}{3}\n\\end{align}\n<!-- Thus -->\n<!-- $\\hat{C}_3(x = (8, 6)) = \\text{Blue}$ -->\n\n\n:::\n\n::::\n\n::: notes\n- Here is a graphical example. Suppose K = 3 and we have two predictors, $x_1$ and $x_2$. We want to do classification of $Y$ when $x_1$ is 8 and $x_2$ is 6.\n- Here how do we define neighbors, we use Euclidean distance to decide who are the point (8, 6)'s neighbors. \n- Showing the idea in the figure, we just use the point (8, 6) as the center of a circle, draw a circle with a larger and larger radius until the circle captures 3 other data points that will be treated as neighbors.\n- Here you can see the green circle captures the points, two are blue and one is orange.\n- So when we do classification at (8, 6), we just compute the proportion of blue neighbors and the proportion of the orange neighbors, and assign the category with the highest proportion or probability to the response variable at the value of predictors (8, 6).\n:::\n\n\n## K-Nearest Neighbors {visibility=\"hidden\"}\nTo predict the category of $y$ at $X = x$, with $y = 0, 1$, `0` being Orange; `1` being Blue\n\n- [Logistic regression]{.green}: $$\\hat{\\pi}(x) = \\hat{P}(Y = 1 \\mid X = x) = \\frac{1}{1+e^{-\\hat{\\beta_0}-\\hat{\\beta_1}x}}$$\n\n- [KNN]{.green}: $$\\hat{\\pi}_{K}(x) = \\hat{P}(Y = 1 \\mid X = x) = \\frac{1}{K} \\sum_{i \\in \\mathcal{N}_K(x)}I(y_i = 1),$$ where $\\mathcal{N}_K(x)$ is the collection of indexes for which the training points $x_i$s are the $K$ neighbors of $x$.\n\n::: notes\n- Let's see what K-Nearest Neighbors method or KNN is.\n- For the binary logistic regression, \n$p(x) = \\hat{P}(Y = 1 \\mid X = x) = \\frac{e^{\\hat{\\beta_0}+\\hat{\\beta_1}x_1 + \\dots + \\hat{\\beta_p}x_p}}{1+e^{\\hat{\\beta_0}+\\hat{\\beta_1}x_1 + \\dots + \\hat{\\beta_p}x_p}}$. Model parameters include $\\beta_0, \\dots, \\beta_p$. So linear regression and logistic regression are both parametric approaches. These models have some parameters to be estimated.\n- KNN first identifies the $K$ points that is closest to $x$, or the K nearest neighbors of $x$, then estimates the probability $p_{Km}(x)$ as\n$p_{Km}(x) = \\hat{P}(Y = m \\mid X = x) = \\frac{1}{K}\\sum_{i \\in \\mathcal{N}_x}I(y_i = m),$ where $\\mathcal{N}_x$ is the collection of indexes that $x_i$s are the $K$ neighbors of $x$.\n- So the idea is that we first decide how many neighbors of $x$ or K to be used, and then compute the proportion of those neighbors whose y label is category $m$\n- KNN applies the Bayes rule and classifies the test response to the class with the largest probability. That is, $\\hat{C}_K(x) = \\underset{m}{\\mathrm{argmax}} \\ \\ p_{Km}(x)$\n- In the binary case this becomes\n$$\\hat{C}_K(x) = \\begin{cases}\n    0      & p_{Km}(x) < 0.5\\\\\n    1      & p_{Km}(x) > 0.5\n\\end{cases}.$$\nand if the probability for class $0$ and $1$ are equal, simply assign at random.\n- We typically avoid this equal probability thing happened by using odd number of K.\n:::\n\n\n## KNN Decision Boundary\n\n- Blue grid indicates the region in which a test response is assigned to the blue class.\n\n- *We don't know the true boundary (the true classification rule)!*.\n\n\n::: {.cell layout-align=\"center\" hash='18-knn_cache/revealjs/unnamed-chunk-9_3e2706e2f58ac8fb0ad923e8be449281'}\n::: {.cell-output-display}\n![](images/18-knn/unnamed-chunk-9-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n::: notes\n- OK. Now for every possible value of $x_1$ and $x_2$, we can classify its corresponding response, right?\n- So we can actually create a dense grid of $x_1$ and $x_2$, and label each point on the grid. \n- And so we can have a whole picture of how the classification result looks like. \n- Blue (Orange) grid indicates the region in which a test observation will be assigned to the blue (orange) class.\n- The curves that separates different classes are called decision boundaries.\n- Again, in reality, we don't know the true boundary, the Bayesian decision boundary.\n:::\n\n\n## KNN Training [![recipes](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/recipes.png){width=60}](https://recipes.tidymodels.org/)\n\n- [*Step 1: Create recipe*]{.green}: `recipes::recipe()`\n\n*Standardize predictors before doing KNN!*\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(knn_recipe <- recipes::recipe(GENDER ~ HEIGHT, data = df_trn) |> \n    step_normalize(all_predictors()))\n```\n:::\n\n\n\n::: notes\nA recipe is a description of the steps to be applied to a data set in order to prepare it for data analysis.\n:::\n\n## KNN Training [![parsnip](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/parsnip.png){width=60}](https://parsnip.tidymodels.org/reference/nearest_neighbor.html)\n\n- [*Step 2: Specify Model*]{.green}: `parsnip::nearest_neighbor()`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(knn_mdl <- parsnip::nearest_neighbor(neighbors = 3) |> \n    set_mode(\"classification\") |> \n    set_engine(\"kknn\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = 3\n\nComputational engine: kknn \n```\n:::\n:::\n\n\n\n## KNN Training [![workflows](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/workflows.png){width=60}](https://workflows.tidymodels.org/)\n\n- [*Step 3: Fitting by creating workflow*]{.green}: `workflows::workflow()`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(knn_fit <- workflows::workflow() |> \n    add_recipe(knn_recipe) |> \n    add_model(knn_mdl) |> \n    fit(data = df_trn))\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.my_classfull}\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nkknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(3,     data, 5))\n\nType of response variable: nominal\nMinimal misclassification: 0.217\nBest kernel: optimal\nBest k: 3\n```\n:::\n:::\n\n\n\n## Prediction on Test Data\n\n:::: {.columns}\n::: {.column width=\"58%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbind_cols(\n    predict(knn_fit, df_tst),\n    predict(knn_fit, df_tst, type = \"prob\")) |> \n    dplyr::sample_n(size = 8)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.my_class600}\n# A tibble: 8 × 3\n  .pred_class .pred_0 .pred_1\n  <fct>         <dbl>   <dbl>\n1 1             0       1    \n2 1             0.370   0.630\n3 0             0.519   0.481\n4 1             0.370   0.630\n5 0             0.852   0.148\n6 1             0       1    \n7 0             1       0    \n8 0             0.630   0.370\n```\n:::\n:::\n\n:::\n\n\n\n::: {.column width=\"42%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nknn_pred <- \n    pull(predict(knn_fit, df_tst))\n\n## Confusion matrix\ntable(knn_pred, df_tst$GENDER)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        \nknn_pred  0  1\n       0 25  8\n       1  9 18\n```\n:::\n\n```{.r .cell-code}\n## Test accuracy rate\nmean(knn_pred == df_tst$GENDER)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.717\n```\n:::\n:::\n\n:::\n\n::::\n\n:::  notes\nmean(knn_pred != df_tst$GENDER)\n:::\n\n## Which K Should We Use?\n\n- $K$-nearest neighbors has no model parameters, but a **tuning parameter** $K$.\n\n- This is a parameter which *determines how the model is trained*, not a parameter that is learned through training.\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/18-knn/unnamed-chunk-15-1.png){fig-align='center' width=80%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/18-knn/unnamed-chunk-16-1.png){fig-align='center' width=80%}\n:::\n:::\n\n:::\n\n::::\n\n\n## $v$-fold Cross Validation\n\n- Use **$v$-fold Cross Validation (CV)** to choose tuning parameters. (MATH 4750 Computational Statistics)\n\n- Usually use $v = 5$ or $10$.\n\n- IDEA: \n  + Prepare $v$ CV data sets\n  + Create a sequence of values of $K$\n  + For each value of $K$, run CV, and obtain an accuracy rate\n  + Choose the $K$ with the highest accuracy rate\n  \n  \n  \n## [`rsample::vfold_cv()`](https://rsample.tidymodels.org/reference/vfold_cv.html) {visibility=\"hidden\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## Create folds\n(body_vfold <- rsample::vfold_cv(df_trn, v = 5, strata = GENDER))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#  5-fold cross-validation using stratification \n# A tibble: 5 × 2\n  splits           id   \n  <list>           <chr>\n1 <split [191/49]> Fold1\n2 <split [191/49]> Fold2\n3 <split [192/48]> Fold3\n4 <split [193/47]> Fold4\n5 <split [193/47]> Fold5\n```\n:::\n:::\n\n\n<br>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nk_val <- tibble(neighbors = seq(from = 1, to = 30, by = 2))\n```\n:::\n\n\n\n## [![tune](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/tune.png){width=60}](https://tune.tidymodels.org/) [`tune::tune_grid()`](https://tune.tidymodels.org/reference/tune_grid.html) {visibility=\"hidden\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nknn_mdl <- parsnip::nearest_neighbor(neighbors = tune()) |> \n    set_mode(\"classification\") |> \n    set_engine(\"kknn\")\n```\n:::\n\n\n\n<br>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nknn_cv <- workflows::workflow() |> \n    add_recipe(knn_recipe) |> \n    add_model(knn_mdl) |> \n    tune::tune_grid(resamples = body_vfold, grid = k_val) |> \n    tune::collect_metrics()\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 30 × 7\n  neighbors .metric  .estimator  mean     n std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n1         1 accuracy binary     0.779     5  0.0121 Preprocessor1_Model01\n2         1 roc_auc  binary     0.778     5  0.0134 Preprocessor1_Model01\n3         3 accuracy binary     0.741     5  0.0308 Preprocessor1_Model02\n# ℹ 27 more rows\n```\n:::\n:::\n\n\n<br>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\naccu_cv <- knn_cv |> filter(.metric == \"accuracy\")\n```\n:::\n\n\n::: notes\nknn_recipe <- recipes::recipe(GENDER ~ HEIGHT, data = df_trn) |> \n    step_scale(all_predictors()) |> \n    step_center(all_predictors())\n\nknn_mdl <- parsnip::nearest_neighbor(neighbors = tune()) |> \n    set_mode(\"classification\") |> \n    set_engine(\"kknn\")\n:::\n\n\n## Best $K$ {visibility=\"hidden\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\naccu_cv |> slice_max(mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 7\n  neighbors .metric  .estimator  mean     n std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n1        19 accuracy binary     0.796     5  0.0154 Preprocessor1_Model10\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/18-knn/unnamed-chunk-24-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n## Final Model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nknn_mdl_best <- parsnip::nearest_neighbor(neighbors = 19) |>\n    set_mode(\"classification\") |>\n    set_engine(\"kknn\")\n\nknn_fit_best <- workflows::workflow() |>\n    add_recipe(knn_recipe) |>\n    add_model(knn_mdl_best) |>\n    fit(data = df_trn)\n```\n:::\n\n\n::: notes\nbest_K <- accu_cv |> slice_max(mean) |> pull(neighbors) |> as.integer()\n:::\n\n## Final Model Performance\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nknn_pred_best <- pull(predict(knn_fit_best, df_tst))\n\n## Confusion matrix\ntable(knn_pred_best, df_tst$GENDER)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             \nknn_pred_best  0  1\n            0 25  6\n            1  9 20\n```\n:::\n\n```{.r .cell-code}\n## Test accuracy rate\nmean(knn_pred_best == df_tst$GENDER)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.75\n```\n:::\n:::\n\n\n# {background-color=\"#ffde57\" background-image=\"https://upload.wikimedia.org/wikipedia/commons/0/05/Scikit_learn_logo_small.svg\" background-size=\"40%\" background-position=\"90% 50%\"}\n\n\n::: {.left}\n<h1> sklearn.neighbors </h1>\n<h1> sklearn.model_selection </h1>\n:::\n\n\n## [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(reticulate); py_install(\"scikit-learn\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n```\n:::\n\n\n. . .\n\n<br>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nbody = pd.read_csv('./data/body.csv')\n\nX = body[['HEIGHT']]\ny = body['GENDER']\nX_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.2, random_state=2024)\n```\n:::\n\n\n\n\n## [sklearn.neighbors.KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nneigh = KNeighborsClassifier(n_neighbors = 3)\nX_trn = np.array(X_trn)\nX_tst = np.array(X_tst)\nneigh.fit(X_trn, y_trn)\n```\n\n::: {.cell-output-display}\n```{=html}\n<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\n## Prediction\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\ny_pred = neigh.predict(X_tst)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_tst, y_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([[22,  5],\n       [12, 21]])\n```\n:::\n:::\n\n\n. . .\n\n<br>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nnp.mean(y_tst == y_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.7166666666666667\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nnp.mean(y_tst != y_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.2833333333333333\n```\n:::\n:::\n\n\n\n##\n::: {.lab}\n\n<span style=\"color:blue\"> **22-K Nearest Neighbors** </span>\n\nIn **lab.qmd** `## Lab 22` section, \n\n1. use `HEIGHT`, `WAIST` and `BMI` to predict `GENDER` using KNN with $K = 3$.\n\n2. Generate the (test) confusion matrix.\n\n3. Calculate (test) accuracy rate.\n\n4. Does using more predictors predict better?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n## load data\nbodydata <- read_csv(\"./data/body.csv\")\nbody <- bodydata |> \n    select(GENDER, HEIGHT, WAIST, BMI) |> \n    mutate(GENDER = as.factor(GENDER))\n\n## training and test data\nset.seed(2024)\ndf_split <- rsample::initial_split(data = body, prop = 0.8)\ndf_trn <- rsample::training(df_split)\ndf_tst <- rsample::testing(df_split)\n```\n:::\n\n\n:::\n\n## R Code\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .my_classfull .cell-code}\n## KNN training\nknn_recipe <- recipes::recipe(GENDER ~ HEIGHT + WAIST + BMI, data = df_trn) |> \n    step_normalize(all_predictors())\n\nknn_mdl <- parsnip::nearest_neighbor(neighbors = 3) |> \n    set_mode(\"classification\") |>\n    set_engine(\"kknn\")\n\nknn_out <- workflows::workflow() |> \n    add_recipe(knn_recipe) |> \n    add_model(knn_mdl) |> \n    fit(data = df_trn)\n\n## KNN prediction\nknn_pred <- pull(predict(knn_out, df_tst))\ntable(knn_pred, df_tst$GENDER)\nmean(knn_pred == df_tst$GENDER)\n```\n:::\n\n\n\n## Python Code\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .my_classfull .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\n## load data\nbody = pd.read_csv('./data/body.csv')\n\nX = body[['HEIGHT', 'WAIST', 'BMI']]\ny = body['GENDER']\nX_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.2, random_state=2024)\n\n## KNN training\nneigh = KNeighborsClassifier(n_neighbors = 3)\nX_trn = np.array(X_trn)\nX_tst = np.array(X_tst)\nneigh.fit(X_trn, y_trn)\n\n## KNN prediction\ny_pred = neigh.predict(X_tst)\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_tst, y_pred)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}