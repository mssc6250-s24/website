{
  "hash": "402c99e924b218630281144821a75027",
  "result": {
    "markdown": "---\ntitle: 'K-Means Clustering `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 512 512\" style=\"height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M418.4 157.9c35.3-8.3 61.6-40 61.6-77.9c0-44.2-35.8-80-80-80c-43.4 0-78.7 34.5-80 77.5L136.2 151.1C121.7 136.8 101.9 128 80 128c-44.2 0-80 35.8-80 80s35.8 80 80 80c12.2 0 23.8-2.7 34.1-7.6L259.7 407.8c-2.4 7.6-3.7 15.8-3.7 24.2c0 44.2 35.8 80 80 80s80-35.8 80-80c0-27.7-14-52.1-35.4-66.4l37.8-207.7zM156.3 232.2c2.2-6.9 3.5-14.2 3.7-21.7l183.8-73.5c3.6 3.5 7.4 6.7 11.6 9.5L317.6 354.1c-5.5 1.3-10.8 3.1-15.8 5.5L156.3 232.2z\"/></svg>`{=html}'\nsubtitle: \"MATH/COSC 3570 Introduction to Data Science\"\nauthor: \"Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University\"\n# date: \"December 23 2023\"\n# macros: _macros.tex # import a list of TeX/LaTeX definitions\nformat: \n  revealjs:\n    #     - \"macros.tex\"\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n    # include-in-header:\n    highlight-style: arrow\n    code-block-bg: true\n    self-contained: false\n    slide-number: c/t    \n    incremental: false\n    width: 1800\n    height: 1000\n    margin: 0.05\n    logo: \"https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg\"\n    footer: \"[math3570-s24.github.io/website](https://math3570-s24.github.io/website/)\"\n    theme: [\"simple\", \"slides.scss\"]\n    multiplex: true\n    code-link: true\n    fig-cap-location: bottom\n    fig-align: center\n    transition: none ## fade slide convex concave zoom\n    title-slide-attributes:\n      data-background-color: \"#447099\"\n      # data-background-image: images/paper-texture.jpg\n      # data-background-size: cover\n      # data-background-color: \"#698ED5\"\neditor: source\nexecute:\n  freeze: true\n  echo: true\n---\n\n\n#  {visibility=\"hidden\"}\n\n\\def\\bx{\\mathbf{x}}\n\\def\\bg{\\mathbf{g}}\n\\def\\bw{\\mathbf{w}}\n\\def\\bbeta{\\boldsymbol \\beta}\n\\def\\bX{\\mathbf{X}}\n\\def\\by{\\mathbf{y}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bW{\\mathbf{W}}\n\\def\\T{\\text{T}}\n\\def\\cov{\\mathrm{Cov}}\n\\def\\cor{\\mathrm{Corr}}\n\\def\\var{\\mathrm{Var}}\n\\def\\E{\\mathrm{E}}\n\\def\\bmu{\\boldsymbol \\mu}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\def\\Trace{\\text{Trace}}\n\n\n\n\n\n\n\n\n<!-- # Clustering -->\n\n## Clustering Methods\n\n- **Clustering**: *unsupervised learning* technique for finding *subgroups* or *clusters* in a data set.\n\n- GOAL: [**Homogeneous within groups; heterogeneous between groups**]{.blue}\n\n\n::: notes\n- When we group data points together, we hope the points in the same group look very much like each other, and the points in the different groups or clusters look very different.\n- When I say they look like each other, I mean the points share similar characteristics. In other words, their variables' values are pretty similar.\n:::\n\n. . .\n\n- [Customer/Marketing Segmentation]{.green}\n    + Divide customers into clusters on age, income, etc.\n    + Each subgroup might be more receptive to a particular form of advertising, or more likely to purchase a particular product.\n    \n:::{.small}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: https://www.datacamp.com/community/tutorials/introduction-customer-segmentation-python](./images/20-kmeans/clustering.png){fig-align='center' width=30%}\n:::\n:::\n\n:::\n\n\n::: notes\n- Divide customers into clusters on the basis of common characteristics.\n- Students: the price shouldn't be that high, and the item should be good-looking and maybe colorful, so it looks young.\n- If we target this group, the low price is not that important, but the item should be beautiful, high-class and high quality.\n:::\n\n\n\n## K-Means Clustering\n- Partition observations into $K$ **distinct, non-overlapping** clusters: assign each to **exactly one** of the $K$ clusters.\n\n- Must pre-specify the number of clusters $K \\ll n$.\n\n::: small\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: Introduction to Statistical Learning Fig 12.7](./images/20-kmeans/kmeans.png){fig-align='center' width=75%}\n:::\n:::\n\n\n:::\n\n\n::: notes\n- A data point cannot belong to two clusters at the same time.\n- As KNN, we have to decide how many clusters the data are partitioned into.\n:::\n\n\n\n## K-Means Algorithm Illustration (K = 3) {visibility=\"hidden\"}\n\n\n:::: {.columns}\n\n::: {.column width=\"52%\"}\n:::{.small}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: Introduction to Statistical Learning Fig 12.8](./images/20-kmeans/kmeans_algo.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n:::\n\n\n\n::: {.column width=\"48%\"}\n:::{.instructions}\n\n**K-Means Algorithm**\n\n- Choose a value of $K$.\n- _Randomly_ assign a number, from 1 to $K$, to each of the observations.\n- Iterate until the cluster assignments stop changing:\n    + **[1]** For each of the $K$ clusters, compute its cluster *centroid*.\n    + **[2]** Assign each observation to the cluster _whose centroid is closest_.\n:::\n:::\n::::\n\n\n::: notes\n- Choose a value of $K$.\n- _Randomly_ assign a number, from 1 to $K$, here the color, to each of the observations.\n- Then we find the mean of each cluster. Because we start from random assignment, it's not surprising that the three means are close. But still, they are not exactly equal.\n- Now, with the means, Assign each observation to the cluster _whose centroid is closest_. Clearly, all points on the top should be colored in brown, because they are close to the brown mean the most. Right? And the points right here are pink, and data right here are green.\n- Now based on this new assignment, we can re-compute new group means because now we have new group assignment.\n- And the new means are shown here.\n- Based on the new means, we reassign data to a group.\n- Then we compute new means, then do new assignment. \n- We keep iterating the two steps until all the data points are stick with a group without changing.\n:::\n\n\n\n\n## K-Means Illustration\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n**Data** (Let's choose $K=2$)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/20-kmeans/unnamed-chunk-5-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n:::{.instructions}\n\n**K-Means Algorithm**\n\n- [Choose a value of $K$.]{.blue}\n- _Randomly_ assign a number, from 1 to $K$, to each of the observations.\n- Iterate until the cluster assignments stop changing:\n    + **[1]** For each of the $K$ clusters, compute its cluster *centroid*.\n    + **[2]** Assign each observation to the cluster _whose centroid is closest_.\n:::\n:::\n::::\n\n\n\n\n## K-Means Illustration\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n**Random assignment**\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/20-kmeans/unnamed-chunk-6-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n:::{.instructions}\n\n**K-Means Algorithm**\n\n- Choose a value of $K$.\n- [_Randomly_ assign a number, from 1 to $K$, to each of the observations.]{.blue}\n- Iterate until the cluster assignments stop changing:\n    + **[1]** For each of the $K$ clusters, compute its cluster *centroid*.\n    + **[2]** Assign each observation to the cluster _whose centroid is closest_.\n:::\n:::\n\n::::\n\n## K-Means Illustration\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n**Compute the cluster centroid**\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/20-kmeans/unnamed-chunk-7-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n:::{.instructions}\n\n**K-Means Algorithm**\n\n- Choose a value of $K$.\n- _Randomly_ assign a number, from 1 to $K$, to each of the observations.\n- Iterate until the cluster assignments stop changing:\n    + **[1]** [For each of the $K$ clusters, compute its cluster *centroid*.]{.blue}\n    + **[2]** Assign each observation to the cluster _whose centroid is closest_.\n:::\n:::\n\n::::\n\n\n\n## K-Means Illustration\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n**Do new assignment**\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/20-kmeans/unnamed-chunk-8-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n:::{.instructions}\n\n**K-Means Algorithm**\n\n- Choose a value of $K$.\n- _Randomly_ assign a number, from 1 to $K$, to each of the observations.\n- Iterate until the cluster assignments stop changing:\n    + **[1]** For each of the $K$ clusters, compute its cluster *centroid*.\n    + **[2]** [Assign each observation to the cluster _whose centroid is closest_.]{.blue}\n:::\n:::\n::::\n\n\n\n\n## K-Means Illustration\n\n\n:::: {.columns}\n\n\n\n::: {.column width=\"50%\"}\n**Do new assignment**\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/20-kmeans/unnamed-chunk-9-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n\n\n::: {.column width=\"50%\"}\n\n**Compute the cluster centroid** ...\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/20-kmeans/unnamed-chunk-10-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n::::\n\n## \n\n::: small\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: Introduction to Statistical Learning Fig 12.8](./images/20-kmeans/kmeans_algo.png){fig-align='center' width=53%}\n:::\n:::\n\n\n:::\n\n\n## K-Means Algorithm\n\n<!-- :::{.tip} -->\n<!-- - The K-means algorithm finds a _local_ rather than a global optimum. -->\n<!-- - The results depend on the initial cluster assignment of each observation. So run the algorithm multiple times, then select the one producing the smallest **within-cluster variation**. -->\n<!-- - Standardize the data so that distance is not affected by variable unit. -->\n<!-- ::: -->\n\n:::{.callout-note}\n\n:::{style=\"font-size: 1.2em;\"}\n- The K-means algorithm finds a _local_ rather than global optimum.\n- The results depend on the initial cluster assignment of each observation. \n  + Run the algorithm multiple times, then select the one producing the smallest **within-cluster variation**.\n- Standardize the data so that distance is not affected by variable unit.\n:::\n\n:::\n\n\n::: notes\n- What is **within-cluster variation**? I avoid using mathematical formula. But the idea is if the data points in the same group are very close to their group mean, their within-cluster variation will be small.\n- We will have K within-cluster variation, one for each group.\n- So we hope the sum of K within-cluster variations or the total within-cluster variation is as small as possible.\n:::\n\n\n\n##\n\n::: small\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: Introduction to Statistical Learning Fig 12.8](./images/20-kmeans/kmeans_local.png){fig-align='center' width=53%}\n:::\n:::\n\n\n:::\n\n## Data for K-Means\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf <- read_csv(\"./data/clus_data.csv\")\ndf  ## income in thousands\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 240 × 2\n    age income\n  <dbl>  <dbl>\n1  32.1  167. \n2  59.1   56.9\n3  54.0   52.4\n4  26.3  -13.9\n5  61.3   41.1\n6  40.7   39.8\n# ℹ 234 more rows\n```\n:::\n:::\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf_clust <- as_tibble(scale(df))\ndf_clust\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 240 × 2\n     age income\n   <dbl>  <dbl>\n1 -0.878  1.00 \n2  1.44  -0.601\n3  1.00  -0.668\n4 -1.38  -1.63 \n5  1.64  -0.831\n6 -0.137 -0.851\n# ℹ 234 more rows\n```\n:::\n:::\n\n:::\n\n::::\n\n\n::: notes\nlibrary(factoextra)\nhttps://bookdown.org/tpinto_home/Unsupervised-learning/k-means-clustering.html#KM1\nhttps://towardsdatascience.com/k-means-clustering-concepts-and-implementation-in-r-for-data-science-32cae6a3ceba\nhttps://www.datanovia.com/en/lessons/k-means-clustering-in-r-algorith-and-practical-examples/\nhttps://uc-r.github.io/kmeans_clustering#optimal\nhttps://www.statology.org/k-means-clustering-in-r/\n:::\n\n## Data for K-Means\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf_clust |> ggplot(aes(x = age, \n                       y = income)) + \n    geom_point()\n```\n\n::: {.cell-output-display}\n![](images/20-kmeans/unnamed-chunk-16-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n## `kmeans()`\n\n::: midi\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(kclust <- kmeans(x = df_clust, centers = 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.my_classfull}\nK-means clustering with 3 clusters of sizes 67, 88, 85\n\nCluster means:\n    age income\n1 -0.29   1.33\n2  1.04  -0.32\n3 -0.85  -0.71\n\nClustering vector:\n  [1] 1 2 2 3 2 3 2 3 1 2 2 3 3 2 3 3 1 3 1 1 3 3 3 2 3 3 2 2 3 1 3 2 1 1 2 1 3\n [38] 3 3 1 3 3 2 1 3 1 2 3 2 2 2 3 1 3 2 3 3 2 1 2 2 3 2 2 2 3 3 2 3 2 2 3 1 1\n [75] 3 2 3 2 2 3 2 2 3 2 1 1 1 2 2 2 3 3 3 3 1 2 1 3 3 2 3 3 2 3 1 2 2 3 1 1 1\n[112] 2 1 2 2 2 2 3 2 2 2 1 3 1 3 2 2 2 1 1 2 1 3 2 3 1 1 1 1 3 2 1 1 3 3 3 2 2\n[149] 3 2 1 2 1 3 1 3 2 3 1 2 2 1 3 3 3 2 2 1 3 3 2 3 1 1 2 3 2 3 1 3 1 3 1 2 3\n[186] 2 2 1 2 2 2 1 3 2 2 2 2 1 1 1 2 1 1 3 1 3 3 1 2 1 3 3 2 2 1 3 3 3 2 1 2 1\n[223] 2 3 3 3 2 1 3 1 1 1 3 2 2 2 1 3 3 1\n\nWithin cluster sum of squares by cluster:\n[1] 66 41 39\n (between_SS / total_SS =  69.5 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n```\n:::\n:::\n\n:::\n\n## `kmeans()`\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkclust$centers\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    age income\n1 -0.29   1.33\n2  1.04  -0.32\n3 -0.85  -0.71\n```\n:::\n\n```{.r .cell-code}\nkclust$size\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 67 88 85\n```\n:::\n\n```{.r .cell-code}\nhead(kclust$cluster, 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 1 2 2 3 2 3 2 3 1 2 2 3 3 2 3 3 1 3 1 1\n```\n:::\n:::\n\n\n## Cluster Info\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(df_clust_k <- augment(kclust, df_clust))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 240 × 3\n     age income .cluster\n   <dbl>  <dbl> <fct>   \n1 -0.878  1.00  1       \n2  1.44  -0.601 2       \n3  1.00  -0.668 2       \n4 -1.38  -1.63  3       \n5  1.64  -0.831 2       \n6 -0.137 -0.851 3       \n# ℹ 234 more rows\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(tidy_kclust <- tidy(kclust))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 5\n     age income  size withinss cluster\n   <dbl>  <dbl> <int>    <dbl> <fct>  \n1 -0.286  1.33     67     65.9 1      \n2  1.04  -0.325    88     40.8 2      \n3 -0.848 -0.714    85     38.9 3      \n```\n:::\n:::\n\n\n\n::: notes\naugment adds the point classifications to the original data set:\n:::\n\n## K-Means in R\n::: {.panel-tabset}\n\n## Clustering Result\n\n:::: {.columns}\n\n::: {.column width=\"80%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/20-kmeans/unnamed-chunk-21-1.png){fig-align='center' width=90%}\n:::\n:::\n\n:::\n\n::: {.column width=\"20%\"}\n\n- [Steady-income family]{.green}\n- [New college graduates/mid-class young family]{.blue}\n- [High socioeconomic class]{.red}\n\n:::\n\n::::\n\n\n## Code\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf_clust_k |>  \n    ggplot(aes(x = age, \n               y = income)) + \n    geom_point(aes(color = .cluster), \n               alpha = 0.8) + \n    geom_point(data = tidy_kclust |>  \n                   select(1:2),\n               size = 8,\n               fill = \"black\",\n               shape = \"o\") +\n    theme_minimal() +\n    theme(legend.position = \"bottom\")\n```\n:::\n\n:::\n\n\n## K-Means in R: [factoextra](https://rpkgs.datanovia.com/factoextra/index.html)\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(factoextra)\nfviz_cluster(object = kclust, data = df_clust, label = NA) + \n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](images/20-kmeans/unnamed-chunk-23-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n## Choose K: Total Withing Sum of Squares\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## wss = total within sum of squares\nfviz_nbclust(x = df_clust, FUNcluster = kmeans, method = \"wss\",  \n             k.max = 10)\n```\n\n::: {.cell-output-display}\n![](images/20-kmeans/unnamed-chunk-24-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n## Choose K: Average Silhouette Method {visibility=\"hidden\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## silhouette = Average Silhouette Width Method\nfviz_nbclust(df_clust, kmeans, method = \"silhouette\",  k.max = 10)\n```\n\n::: {.cell-output-display}\n![](images/20-kmeans/unnamed-chunk-25-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n::: notes\nmeasuring how well each data point lies within its cluster\n:::\n\n\n\n## Choose K: Gap Statistics {visibility=\"hidden\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## gap_stat = Gap Statistics\nfviz_nbclust(df_clust, kmeans, method = \"gap_stat\",  k.max = 10)\n```\n\n::: {.cell-output-display}\n![](images/20-kmeans/unnamed-chunk-26-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n::: notes\ncompares the total intracluster variation for different number of cluster k with their expected values under a data with no clustering (these data generated using Monte Carlo simulations). The higher the gap between the observed and expected, the better the clustering.\n:::\n\n\n## Practical Issues\n\n- Try several different $K$s, and look for the one with the most useful or interpretable solution.\n\n:::{.alert}\nClustering is not beneficial for decision making or strategic plan if the clusters found are not meaningful based on their features.\n:::\n\n- The clusters found may be heavily distorted due to outliers that do not belong to any cluster. \n\n- Clustering methods are not very robust to perturbations of the data.\n\n\n##\n\n::: {.lab}\n\n<span style=\"color:blue\"> **23-K means Clustering** </span>\n\nIn **lab.qmd** `## Lab 24` section, \n\n1. Install R package `palmerpenguins` at <https://allisonhorst.github.io/palmerpenguins/>\n\n2. Perform K-Means to with $K = 3$ to cluster penguins based on `bill_length_mm` and `flipper_length_mm` of data `peng`.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\npeng <- penguins[complete.cases(penguins), ] |> \n    select(flipper_length_mm, bill_length_mm)\n```\n:::\n\n\n:::\n\n\n##\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/20-kmeans/unnamed-chunk-28-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/20-kmeans/unnamed-chunk-29-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::::\n\n\n\n# {background-color=\"#ffde57\" background-image=\"https://upload.wikimedia.org/wikipedia/commons/0/05/Scikit_learn_logo_small.svg\" background-size=\"40%\" background-position=\"90% 50%\"}\n\n\n::: {.left}\n<h1> sklearn.cluster </h1>\n<!-- <h1> sklearn.preprocessing </h1> -->\n\n:::\n\n\n## [sklearn.cluster.KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\ndf_clus = pd.read_csv('./data/clus_data.csv')\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nscaler = StandardScaler()\nX = scaler.fit_transform(df_clus.values)\n```\n:::\n\n\n\n<br>\n\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nkmeans = KMeans(n_clusters=3,  n_init=10).fit(X)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nkmeans.labels_[0:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([0, 1, 1, 2, 1, 2, 1, 2, 0, 1], dtype=int32)\n```\n:::\n:::\n\n\n<br>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nnp.round(kmeans.cluster_centers_, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([[-0.29,  1.33],\n       [ 1.04, -0.33],\n       [-0.85, -0.72]])\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}