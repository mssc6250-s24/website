{
  "hash": "4924f65e8f5053247010bcee2420d478",
  "result": {
    "markdown": "---\ntitle: 'Multiple Linear Regression - Matrix Approach `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 512 512\" style=\"height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M64 256V160H224v96H64zm0 64H224v96H64V320zm224 96V320H448v96H288zM448 256H288V160H448v96zM64 32C28.7 32 0 60.7 0 96V416c0 35.3 28.7 64 64 64H448c35.3 0 64-28.7 64-64V96c0-35.3-28.7-64-64-64H64z\"/></svg>`{=html}'\nsubtitle: \"MATH 4780 / MSSC 5780 Regression Analysis\"\nauthor: \"Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University\"\n# date: \"August 02 2023\"\n# macros: _macros.tex # import a list of TeX/LaTeX definitions\nformat: \n  revealjs:\n    # code-line-numbers: false\n    #     - \"macros.tex\"\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n    # include-in-header:\n    highlight-style: arrow\n    code-block-bg: true\n    self-contained: false\n    slide-number: c/t    \n    incremental: false\n    width: 1800\n    height: 1000\n    margin: 0.05\n    logo: \"https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg\"\n    footer: \"[math4780-f23.github.io/website](https://math4780-f23.github.io/website/)\"\n    theme: [\"simple\", \"slides.scss\"]\n    multiplex: true\n    code-link: true\n    fig-cap-location: bottom\n    fig-align: center\n    transition: none ## fade slide convex concave zoom\n    title-slide-attributes:\n      data-background-color: \"#447099\"\n      # data-background-image: images/paper-texture.jpg\n      # data-background-size: cover\n      # data-background-color: \"#698ED5\"\neditor: source\nexecute:\n  freeze: true\n  echo: false\n  purl: true\n---\n\n\n#  {visibility=\"hidden\"}\n\n\\def\\bx{\\mathbf{x}}\n\\def\\bg{\\mathbf{g}}\n\\def\\bw{\\mathbf{w}}\n\\def\\balpha{\\boldsymbol \\alpha}\n\\def\\bbeta{\\boldsymbol \\beta}\n\\def\\beps{\\boldsymbol \\epsilon}\n\\def\\bX{\\mathbf{X}}\n\\def\\by{\\mathbf{y}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bW{\\mathbf{W}}\n\\def\\T{\\text{T}}\n\\def\\cov{\\mathrm{Cov}}\n\\def\\cor{\\mathrm{Corr}}\n\\def\\var{\\mathrm{Var}}\n\\def\\E{\\mathrm{E}}\n\\def\\bmu{\\boldsymbol \\mu}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\def\\Trace{\\text{Trace}}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n## Regression Model in Matrix Form\n\n$$y_i= \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_kx_{ik} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2), \\quad i = 1, 2, \\dots, n$$\n\n<!-- - $\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)$ -->\n\n. . .\n\n\n$$\\bf y = \\bf X \\bbeta + \\beps$$ where\n\n$$\\begin{align}\n\\bf y = \\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n \\end{bmatrix},\\quad\n\\bf X = \\begin{bmatrix}\n  1 & x_{11} & x_{12} & \\cdots & x_{1k} \\\\\n  1 & x_{21} & x_{22} & \\cdots & x_{2k} \\\\\n  \\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  1 & x_{n1} & x_{n2} & \\cdots & x_{nk} \\end{bmatrix}, \\quad\n\\bbeta = \\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_k \\end{bmatrix} , \\quad\n\\beps = \\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\vdots \\\\\n\\epsilon_n\\end{bmatrix}\n\\end{align}$$\n\n- ${\\bf X}_{n \\times p}$: Design matrix\n- $\\beps \\sim MVN_n({\\bf 0}, \\sigma^2 {\\bf I}_n)$ [^1]\n\n\n[^1]: For simplicity and convenience, $N({\\bf a}, {\\bf B})$ represents a multivariate normal distribution with mean vector ${\\bf a}$ and covariance matrix ${\\bf B}$.\n\n\n\n::: notes\n- Let's first write our model in a matrix form.\n- $\\bf X$ is usually called the design matrix.\n:::\n\n\n\n\n## Regression Model in Matrix Form\n\n::: fact\nFor a random vector ${\\bf Z} = (Z_1, \\dots, Z_n)'$, its mean vector is\n$$E({\\bf Z}) = E\\begin{pmatrix}\nZ_1 \\\\\nZ_2 \\\\\n\\vdots \\\\\nZ_n\\end{pmatrix} = \\begin{bmatrix}\nE(Z_1) \\\\\nE(Z_2) \\\\\n\\vdots \\\\\nE(Z_n)\\end{bmatrix}$$\n\nIts (symmetric) variance-covariance matrix is \n $$\\scriptsize \\begin{align} \\cov({\\bf Z}) = {\\bf \\Sigma} &= \\begin{bmatrix} \\var(Z_1) & \\cov(Z_1, Z_2) & \\cdots & \\cov(Z_1, Z_n) \\\\ \\cov(Z_2, Z_1) & \\var(Z_2) & \\cdots & \\cov(Z_2, Z_n) \\\\ \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\ \\cov(Z_n, Z_1) & \\cov(Z_n, Z_2) & \\cdots & \\var(Z_n) \\end{bmatrix} \\end{align}$$\n:::\n\n\n## Least Squares Estimation in Matrix Form\n$$S(\\beta_0, \\beta_1, \\dots, \\beta_k) = \\sum_{i=1}^n\\epsilon_i^2 = \\sum_{i=1}^n\\left(y_i - \\beta_0 - \\sum_{j=1}^k\\beta_j x_{ij}\\right)^2$$\n\n$$\\begin{align}\n\\left.\\frac{\\partial S}{\\partial\\beta_0}\\right\\vert_{b_0, b_1, \\dots, b_k} &= -2 \\sum_{i=1}^n\\left(y_i - b_0 - \\sum_{j=1}^k b_j x_{ij}\\right) = 0\\\\\n\\left.\\frac{\\partial S}{\\partial\\beta_j}\\right\\vert_{b_0, b_1, \\dots, b_k} &= -2 \\sum_{i=1}^n\\left(y_i - b_0 - \\sum_{j=1}^k b_j x_{ij}\\right)x_{ij} = 0, \\quad j = 1, 2, \\dots, k\n\\end{align}$$\n\n. . .\n\n$$\\begin{align}\nS(\\bbeta) = \\sum_{i=1}^n\\epsilon_i^2 = \\beps'\\beps &= (\\bf y - \\bf X \\bbeta)'(\\bf y - \\bf X \\bbeta) \\\\\n&= \\bf y'\\bf y - \\bbeta'\\bf X'\\bf y - \\bf y'\\bf X \\bbeta + \\bbeta' \\bf X' \\bf X \\bbeta \\\\\n&={\\bf y}'{\\bf y} - 2\\bbeta'{\\bf X}'{\\bf y} + \\bbeta' {\\bf X}' {\\bf X} \\bbeta\n\\end{align}$$\n\n## Least Squares Estimation in Matrix Form\n$$\\begin{align}\nS(\\bbeta) = \\sum_{i=1}^n\\epsilon_i^2 = \\beps'\\beps &= (\\bf y - \\bf X \\bbeta)'(\\bf y - \\bf X \\bbeta) \\\\\n&= \\bf y'\\bf y - \\bbeta'\\bf X'\\bf y - \\bf y'\\bf X \\bbeta + \\bbeta' \\bf X' \\bf X \\bbeta \\\\\n&={\\bf y}'{\\bf y} - 2\\bbeta'{\\bf X}'{\\bf y} + \\bbeta' {\\bf X}' {\\bf X} \\bbeta\n\\end{align}$$\n\n::: fact\n\nLet ${\\bf t}$ and ${\\bf a}$ be $n \\times 1$ column vectors, and ${\\bf A}_{n \\times n}$ is a symmetric matrix.\n\n- $\\frac{\\partial {\\bf t}'{\\bf a} }{\\partial {\\bf t}} = \\frac{\\partial {\\bf a}'{\\bf t} }{\\partial {\\bf t}} = {\\bf a}$\n\n- $\\frac{\\partial {\\bf t}'{\\bf A} {\\bf t}}{\\partial {\\bf t}} = 2{\\bf A} {\\bf t}$\n\n:::\n\n- Normal equations:\n$\\begin{align} \\left.\\frac{\\partial S}{\\partial \\bbeta}\\right \\vert_{\\bf b} = -2 {\\bf X}' {\\bf y} + 2 {\\bf X}' {\\bf X} {\\bf b} = \\boldsymbol{0} \\end{align}$\n\n\n::: notes\n- To get the LSE in a matrix form, we first write the sum of squares using matrices.\n:::\n\n. . .\n\n- LSE for $\\bbeta$: <span style=\"color:blue\"> $\\begin{align} \\boxed{{\\bf b} = ({\\bf X}' {\\bf X}) ^{-1} {\\bf X}' \\bf y} \\end{align}$ </span>\n\n\n\n\n\n## Normal Equations\n$(\\bf X' \\bf X) \\bf b = \\bf X' \\bf y$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/06-mlr-mat/normal_eqn.png){fig-align='center' width=100%}\n:::\n:::\n\n\n::: notes\n- Simple linear regression case.\n:::\n\n\n## [R Lab]{.pink} Design Matrix\n\n:::: {.columns}\n::: midi\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nX <- cbind(1, delivery_data[, c(2, 3)])\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.my_classfull}\n   1    7  560\n   1    3  220\n   1    3  340\n   1    4   80\n   1    6  150\n   1    7  330\n   1    2  110\n   1    7  210\n   1   30 1460\n   1    5  605\n   1   16  688\n   1   10  215\n   1    4  255\n   1    6  462\n   1    9  448\n   1   10  776\n   1    6  200\n   1    7  132\n   1    3   36\n   1   17  770\n   1   10  140\n   1   26  810\n   1    9  450\n   1    8  635\n   1    4  150\n```\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ny <- as.matrix(delivery_data$time)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.my_classfull}\n16.7\n11.5\n12.0\n14.9\n13.8\n18.1\n 8.0\n17.8\n79.2\n21.5\n40.3\n21.0\n13.5\n19.8\n24.0\n29.0\n15.3\n19.0\n 9.5\n35.1\n17.9\n52.3\n18.8\n19.8\n10.8\n```\n:::\n:::\n\n:::\n:::\n::::\n\n\n## [R Lab]{.pink} Regression Coefficients\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/06-mlr-mat/xtx.png){fig-align='center' width=70%}\n:::\n\n::: {.cell-output-display}\n![](./images/06-mlr-mat/xty.png){fig-align='center' width=70%}\n:::\n\n::: {.cell-output-display}\n![](./images/06-mlr-mat/b.png){fig-align='center' width=70%}\n:::\n:::\n\n<!-- # ```{r} -->\n<!-- # #| out-width: 90% -->\n<!-- # knitr::include_graphics(\"./images/06-mlr-mat/xty.png\") -->\n<!-- # ``` -->\n<!-- # ```{r} -->\n<!-- # #| out-width: 90% -->\n<!-- # knitr::include_graphics(\"./images/06-mlr-mat/b.png\") -->\n<!-- # ``` -->\n:::\n\n\n::: {.column width=\"50%\"}\n${\\bf b} = ({\\bf X}' {\\bf X}) ^{-1} \\bf X' \\bf y$\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nt(X) %*% X\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             1  cases distance\n1           25    219    10232\ncases      219   3055   133899\ndistance 10232 133899  6725688\n```\n:::\n\n```{.r .cell-code}\nt(X) %*% y\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]\n1           560\ncases      7375\ndistance 337072\n```\n:::\n\n```{.r .cell-code}\n(b <- solve(t(X) %*% X) %*% t(X) %*% y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]\n1        2.341\ncases    1.616\ndistance 0.014\n```\n:::\n:::\n\n:::\n::::\n\n\n## Hat Matrix\n- In SLR, $\\hat{y}_i = h_{i1}y_1 + h_{i2}y_2 + \\cdots + h_{in}y_n = {\\bf h}_i'{\\bf y}$ where $h_{ij} = \\frac{1}{n} + \\frac{(x_i-\\overline{x})(x_j-\\overline{x})}{S_{xx}}$ and ${\\bf h}_i' = (h_{i1}, h_{i2}, \\dots, h_{in})$. The **hat matrix** is ${\\bf H} = (h_{ij})_{n \\times n}$.\n\n. . .\n\n$$\\hat{\\bf y} = {\\bf X} {\\bf b} = {\\bf X} ({\\bf X}' {\\bf X}) ^{-1} \\bf X' \\bf y = \\bf H \\bf y$$\n\n- ${\\bf H} = {\\bf X} ({\\bf X}' {\\bf X}) ^{-1} \\bf X'$\n\n- The vector of residuals $e_i = y_i - \\hat{y}_i$ is\n$$\\bf e = \\bf y - \\hat{\\bf y} = \\bf y - \\bf X \\bf b = \\bf y -\\bf H \\bf y = (\\bf I - \\bf H) \\bf y$$\n\n. . .\n\n::: alert\n- Both $\\bf H$ and $\\bf I-H$ are *symmetric* and *idempotent*. They are **projection** matrices.\n- $\\bf H$ projects $\\bf y$ to $\\hat{\\bf y}$ on the $p$-dimensional space spanned by columns of $\\bf X$, or the column space of $\\bf X$, $Col(\\bf X)$.\n- $\\bf I - H$ projects $\\bf y$ to $\\bf e$ on the space **perpendicular** to $Col(\\bf X)$, or $Col(\\bf X)^{\\bot}$.\n:::\n\n\n\n::: notes\n- The vector of fitted values $\\hat{y}_i$ corresponding to $y_i$ is\n$$\\hat{\\bf y} = \\bf X \\bf b = \\bf X (\\bf X' \\bf X) ^{-1} \\bf X' \\bf y = \\bf H \\bf y$$\n- $\\bf H$ plays an important role in regression analysis.\n- And it will be shown up many times later in many topics of this course.\n- OK. It's a little abstract. Let's look into the geometrical interpretation of least squares little by little.\n:::\n\n## Geometrical Interpretation of Least Squares\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n- $Col({\\bf X}) = \\{ {\\bf Xb}: {\\bf b} \\in {\\bf R}^p \\}$\n- ${\\bf y} \\notin Col({\\bf X})$\n- $\\hat{{\\bf y}} = {\\bf Xb} = {\\bf H} {\\bf y} \\in Col({\\bf X})$\n- Minimize the distance of $\\color{red}{A}$ to $Col(\\bf X)$: Find the point in $Col(\\bf X)$ that is closest to $A$.\n:::\n\n::: {.column width=\"70%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![https://commons.wikimedia.org/wiki/File:OLS_geometric_interpretation.svg](./images/06-mlr-mat/ols_geom.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n::::\n\n\n::: notes\n- (1) what is column space of $X$?\n  + The col space of $X$ is the set or the collection of all linear combinations of columns of $X$. In other words, column space of $X$ is the space spanned by the columns of $X$.\n  + Example\n  + For visualization purpose, suppose $X$ has 2 columns, $x_1$ and $x_2$. The column space of $X$ will be the plane spanned by the 2 columns, shown in green color.\n- (2) observation $y$.\n  + Basically 99% of the time, $Y$ will not be in the column space of $X$.\n  + $y_i = b_0 + b_1x_{i1} + ... +  b_kx_{ik} + e_i$\n  + $y_i$ is not the linear combo of columns of $X$, and $y$ is not a point in Col(X).\n  + So $y$ is point in the $n$-dimensional space, and not on the plane spanned by columns of $X$.\n- (3) H (Let's see what the projection matrix $H$ is doing)\n  + When we multiply $H$ by $y$, we are finding the projection of $y$ onto the Col(X).\n  + The projection of $y$ on the Col(X) is actually the fitted value $\\hat{y}$.\n  + $\\hat{y}_i = b_0 + b_1x_{i1} + ... +  b_kx_{ik}$\n  + There are so many (actually infinitely many) linear combo of columns of $X$, why this particular vector is our $\\hat{y}$?\n  + The distance between $y$ and Col(X) is actually the shortest when we do the projection of $y$ onto the Col(X).\n  + In other words the distance between $y$ and $\\hat{y}$ is the minimal distance among all the distance between $y$ and any point in the Col(X).\n\n<!-- - Any point in the column space of $\\bf X$ (estimation space) is of the form $\\bf X \\bf \\bbeta$. -->\n<!-- - Minimizing the distance of point $A$ defined by $\\bf y$ to the column space $(S(\\bbeta) = \\|\\bf y - \\bf X \\bf \\bbeta\\|^2)$ requires finding the point in the space that is close to $A$.  -->\n<!-- - $\\bf y - \\hat{\\bf y} = \\bf {y - X b} \\perp Col(X)$ -->\n<!-- - $\\bf X'(y - Xb) = 0$. -->\n:::\n\n\n## Geometrical Interpretation of Least Squares\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n- The distance is minimized when the point in the space is the foot of the line from $A$ **normal** to the space. This is point $C$.\n$\\small {\\bf e} = ({\\bf y - \\hat{\\bf y}}) = ({\\bf y - X b}) = ({\\bf I} - {\\bf H}) {\\bf y} \\perp Col({\\bf X})$\n- $\\bf X'(y - Xb) = 0$\n\n::: alert\nSearching for the LS solution $\\bf b$ that minimizes $SS_{res}$ is the same as locating the point ${\\bf Xb} \\in Col({\\bf X})$ that is as close to $\\bf y$ as possible!\n:::\n:::\n\n\n::: {.column width=\"70%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![https://commons.wikimedia.org/wiki/File:OLS_geometric_interpretation.svg](./images/06-mlr-mat/ols_geom.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n::::\n\n\n\n::: notes\n- (4) (I - H) Now let's see what (I - H) is doing.\n  + The residual vector is $y - \\hat{y}$. Geometrically, the residual vector is this dash line vector that is perpendicular to the column space of $X$. In other words,the residual vector is the normal vector of Col(X).\n  + This is the result of projection. The distance between $y$ and the column space is minimized when the point in the space is the foot of the line from $A$ normal to the space. This is point $C$.\n  + We know that e = (I - H)y. So (I - H) project $Y$ onto the space that is perpendicular to Col(X). And the vector in that space is the residual vector.\n- (5) Calculus connects to Linear Algebra\n\n:::\n\n## [R Lab]{.pink} Verify Identity\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n${\\bf H} = {\\bf X} ({\\bf X}' {\\bf X}) ^{-1} \\bf X'$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nH <- X %*% solve(t(X) %*% X) %*% t(X)\n```\n:::\n\n\n$\\hat{\\bf y} = \\bf H \\bf y$\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfitted_y <- H %*% y\nfitted_y[1:4]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 22 10 12 10\n```\n:::\n\n```{.r .cell-code}\ndelivery_lm$fitted.values[1:4]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 1  2  3  4 \n22 10 12 10 \n```\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n$\\bf e = (\\bf I - \\bf H) \\bf y$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn <- length(y)\nI <- diag(n)\nres <- (I - H) %*% y\nres[1:4]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -5.03  1.15 -0.05  4.92\n```\n:::\n\n```{.r .cell-code}\ndelivery_lm$residuals[1:4]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    1     2     3     4 \n-5.03  1.15 -0.05  4.92 \n```\n:::\n\n```{.r .cell-code}\n## residual vector in the left null space of X\nt(X) %*% res\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             [,1]\n1         3.9e-13\ncases    -1.4e-12\ndistance -6.0e-11\n```\n:::\n:::\n\n:::\n::::\n\n## Multivariate Normal Distribution\n\n::: fact\n\n${\\bf y} \\sim N(\\bmu, {\\bf \\Sigma})$, and ${\\bf Z = By + c}$ with a constant matrix ${\\bf B}$ and vector $\\bf c$, then $${\\bf Z} \\sim N({\\bf B\\bmu}, {\\bf B \\Sigma B}')$$\n:::\n\n${\\bf b} = ({\\bf X}' {\\bf X}) ^{-1} \\bf X' \\bf y$\n\n$$\\textbf{b} \\sim N\\left(\\bbeta, \\sigma^2 {\\bf (X'X)}^{-1} \\right)$$\n$$E(\\textbf{b}) = E\\left[ {\\bf (X'X)}^{-1} {\\bf X}' {\\bf y}\\right] = \\bbeta$$\n$$\\cov(\\textbf{b}) = \\cov\\left[{\\bf (X'X)}^{-1} {\\bf X}' {\\bf y} \\right] = \\sigma^2 {\\bf (X'X)}^{-1}$$\n\n\n::: alert\nThe standard error of $b_j$ is ${\\sqrt{s^2C_{jj}}}$, where $C_{jj}$ is the diagonal element of $({\\bf X'X})^{-1}$ corresponding to $b_j$.\n:::\n\n\n::: notes\nIf ${\\bf y} \\sim N_n(\\bsmu, {\\bf \\Sigma})$, and ${\\bf Z = By + c}$ with a constant vector $\\bf c$, then $${\\bf Z} \\sim N({\\bf B\\bsmu}, {\\bf B \\Sigma B}')$$.\n$$\\var(\\textbf{b}) = E\\left[(\\textbf{b} - E(\\textbf{b}))(\\textbf{b}-E(\\textbf{b}))'\\right] = \\var\\left[\\bf{(X'X)^{-1}X'y}\\right] = \\sigma^2 \\bf (X'X)^{-1}$$\n\n- The standard error of $b_j$ is ${\\sqrt{\\hat{\\sigma}^2C_{jj}}}$, where $C_{jj}$ is the diagonal element of $({\\bf X'X})^{-1}$ corresponding to $b_j$.\n:::\n\n# Hypothesis Testing\n<h2> Tests on Subsets of Coefficients </h2>\n<!-- <h2> Testing the General Linear Hypothesis </h2> -->\n\n## Reduced Model vs. Full Model\n- Overall test of significance: *all* predictors vs. Marginal $t$-test: *one single* predictor\n- How to test **any subset** of predictors?\n\n. . .\n\n- **Full Model**: $y = \\beta_0 + \\beta_1x_1+\\beta_2x_2 + \\beta_3x_3 + \\beta_4x_4 + \\epsilon$\n- $H_0: \\beta_{2} = \\beta_{4} = 0$\n\n. . .\n\n- **Reduced Model** (under $H_0$): $y = \\beta_0 + \\beta_1x_1 + \\beta_3x_3 + \\epsilon$\n- Like to see if $x_2$ and $x_4$ can contribute significantly to the regression model when $x_1$ and $x_3$ are in the model.\n  + If yes, $\\beta_{2} \\ne 0$ and/or $\\beta_{4} \\ne 0$. (Reject $H_0$)\n  + Otherwise, $\\beta_{2} = \\beta_{4} = 0$. (Do not reject $H_0$)\n\n. . .\n\n::: alert\nGiven $x_1$ and $x_3$ in the model, we examine **how much extra $SS_R$ is increased ( $SS_{res}$ is reduced) if $x_2$ and $x_4$ are added to the model**.\n:::\n\n\n::: notes\n- If much more extra $SS_R$ is increased after $x_2$ and $x_4$ are included in the model, meaning that lots of variation in $y$ that were treated as unexplained variation or variation that cannot be explained by the $x_1$ and $x_3$, now are absorbed or explained by $x_2$ and $x_4$.\n- $x_2$ and $x_4$ provide some important and valuable information that $x_1$ and $x_3$ cannot provide, and that information helps us better predict response values and explain the variation of $Y$.\n- So if we observe a significant increase in SS_R, it means that $x_2$ and $x_4$ are valuable and their coefficient is significantly non-zero. Therefore, we should keep the full model that includes all predictors, or reject $H_0$ because the reduced model is too simple, and lose lots of useful information for explaining variation in $Y$.\n:::\n\n\n## Extra Sum-of-sqaures\n- Full Model: $\\bf y = X\\bbeta+\\beps$\n\n- Partition coefficient vector:\n\n$$\\bbeta_{p \\times 1} = \\left[\n\\begin{array}{c}\n  \\bbeta_1 \\\\\n  \\hline\n  \\bbeta_2\n\\end{array}\n\\right]$$\n\nwhere $\\bbeta_1$ is $(p-r) \\times 1$ and $\\bbeta_2$ is $r \\times 1$\n\n- Test $H_0: \\bbeta_2 = {\\bf 0}$, $H_1: \\bbeta_2 \\ne {\\bf 0}$\n\n- Example: <span style=\"color:blue\"> $p=5$, $r=2$, $\\bbeta_1 = (\\beta_0, \\beta_1, \\beta_3)'$, $\\bbeta_2 = (\\beta_2, \\beta_4)'$.</span>\n\n\n::: notes\n- For simplicity, we can partition the coefficient vector to beta_1 and beta_2, where beta_2 is the subset we'd like to test.\n- $1 \\le r \\le k$\n:::\n\n. . .\n\n$${\\bf y} = {\\bf X} \\bbeta+\\beps = {\\bf X}_1\\bbeta_1 + {\\bf X}_2\\bbeta_2 + \\beps$$\n\n+ $n \\times (p-r)$ matrix ${\\bf X}_1$: the columns of $\\bf X$ associated with $\\bbeta_1$\n\n+ $n \\times r$ matrix ${\\bf X}_2$: the columns of $\\bf X$ associated with $\\bbeta_2$\n\n## Extra Sum-of-sqaures\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n**Full Model**: $\\small \\color{red}{{\\bf y} = {\\bf X}\\bbeta+\\beps = {\\bf X}_1\\bbeta_1 + {\\bf X}_2\\bbeta_2 + \\beps}$\n\n- ${\\bf b} = ({\\bf X}' {\\bf X}) ^{-1} {\\bf X}' {\\bf y}$\n<!-- - $SS_R(\\bbeta) = {\\bf b}{\\bf X}' {\\bf y}$ with $p$ dfs -->\n:::\n\n\n::: {.column width=\"50%\"}\n**Reduced Model**: $\\small \\color{red}{{\\bf y} = {\\bf X}_1\\bbeta_1+\\beps}$ $\\color{red}{(\\bbeta_2 = {\\bf 0})}$\n\n- ${\\bf b}_1 = ({\\bf X}_1' {\\bf X}_1) ^{-1} {\\bf X}_1' {\\bf y}$\n<!-- - $SS_R(\\bbeta_1) = {\\bf b}_1{\\bf X}_1'{\\bf y}$ with $p-r$ dfs -->\n:::\n::::\n\n::: notes\n- To find the contribution of the terms in $\\bbeta_2$ to the regression, fit the model assuming that $H_0: \\bf \\bbeta_2 = 0$ is true.\n<!-- - To find the contribution of the terms in $\\bbeta_2$, fit the model assuming $H_0: \\bf \\bbeta_2 = 0$ is true. -->\n:::\n\n. . .\n\n- The $SS_R$ due to $\\bbeta_2$ given that $\\bbeta_1$ is in the model is\n$$\\begin{align} SS_R(\\bbeta_2|\\bbeta_1) &= SS_R(\\bbeta) - SS_R(\\bbeta_1)\\\\ &= SS_R(\\bbeta_1, \\bbeta_2) - SS_R(\\bbeta_1) \\end{align}$$ with $r$ dfs.\n- This is the **extra sum of squares** due to $\\bbeta_2$.\n- It measures the increase in the $SS_R$ that results from adding regressors in ${\\bf X}_2$ to the model that already contains regressors in ${\\bf X}_1$.\n\n\n::: notes\n- The regression sum of squares due to $\\bbeta_2$ given that $\\bbeta_1$ is in the model is\n$$SS_R(\\bbeta_2|\\bbeta_1) = SS_R(\\bbeta) - SS_R(\\bbeta_1)$$ with $p - (p-r) = r$ dfs.\n- This is **extra sum of squares** due to $\\bbeta_2$.\n- It measures the increase in the $SS_R$ that results from adding $x_{k-r+1}, x_{k-r+2}, \\dots, x_k$ to the model that already contains $x_{1}, x_{2}, \\dots, x_{k-r}$.\n- To find the contribution of the terms in $\\bbeta_2$ to the regression, fit the model assuming that $H_0: \\bf \\bbeta_2 = 0$ is true.\n:::\n\n\n## Partial $F$-test\n- $F_{test} = \\frac{SS_R(\\bbeta_2|\\bbeta_1)/r}{SS_{res}(\\bbeta)/(n-p)} = \\frac{MS_R(\\bbeta_2|\\bbeta_1)}{MS_{res}}$\n- Under $H_0$ that $\\bbeta_2 = \\bf 0$, $F_{test} \\sim F_{r, n-p}$. $(p = k+1)$.\n- Reject $H_0$ if $F_{test} > F_{\\alpha, r, n-p}$.\n\n. . .\n\n::: alert\nGiven the regressors of ${\\bf X}_1$ are in the model,\n\n- If the regressors of ${\\bf X}_2$ contribute much, $SS_R(\\bbeta_2|\\bbeta_1)$ will be large.\n\n- A large $SS_R(\\bbeta_2|\\bbeta_1)$ implies a large $F_{test}$.\n\n- A large $F_{test}$ tends to reject $H_0$, and conclude that $\\bbeta_2 \\ne \\bf 0$.\n\n- $\\bbeta_2 \\ne \\bf 0$ means the regressors of ${\\bf X}_2$ provide additional explanatory and prediction power that ${\\bf X}_1$ cannot provide.\n\n:::\n\n\n## Example: Delivery Data\n- $H_0: \\beta_2 = 0 \\qquad H_1: \\beta_2 \\ne 0$\n- Full model: $y = \\beta_0 + \\beta_1x_1+\\beta_2x_2+\\epsilon$\n\n. . .\n\n::: question\nWhat is the reduced model?\n:::\n\n. . .\n\n- Reduced model: $y = \\beta_0 + \\beta_1x_1 +\\epsilon$\n\n. . .\n\n- $SS_R(\\beta_2|\\beta_1, \\beta_0) = SS_R(\\beta_2, \\beta_1, \\beta_0) -SS_R(\\beta_1, \\beta_0)$\n- $F_{test} = \\frac{SS_R(\\beta_2|\\beta_1, \\beta_0)/1}{SS_{res}(\\bbeta)/(25-3)}$\n- Reject $H_0$ if $F_{test} > F_{\\alpha, 1, 22}$\n\n\n. . .\n\n::: alert\nWhen $r=1$, partial $F$-test is equivalent to marginal $t$-test.\n:::\n\n\n## [R Lab]{.pink} Extra Sum-of-sqaures Approach\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfull_lm <- delivery_lm\nreduced_lm <- lm(time ~ cases, data = delivery_data)\nanova(reduced_lm, full_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: time ~ cases\nModel 2: time ~ cases + distance\n  Res.Df RSS Df Sum of Sq    F  Pr(>F)    \n1     23 402                              \n2     22 234  1       168 15.8 0.00063 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsumm_delivery$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)    2.341     1.0967     2.1  4.4e-02\ncases          1.616     0.1707     9.5  3.3e-09\ndistance       0.014     0.0036     4.0  6.3e-04\n```\n:::\n:::\n\n\n## General Linear Hypothesis {visibility=\"hidden\"}\n- Previous testing procedures are special cases of the **general linear hypothesis testing**.\n- $H_0: {\\bf T\\bbeta = c}$, where ${\\bf T}$ is an $m \\times p$ constant matrix and $\\bf c$ is a $m \\times 1$ constant vector.\n- There are $m$ linear hypotheses to be tested. [^2]\n- **Full Model**: $y = \\beta_0 + \\beta_1x_1+\\beta_2x_2 + \\beta_3x_3 + \\epsilon$\n- The model under $H_0$ is the reduced model or **restricted model**.\n$$H_0: \\begin{align}&\\beta_1 + 2 \\beta_2 = 0 \\\\\n&\\beta_3 = 2 \\end{align}$$\n\n$$\\begin{bmatrix}\n0 & 1 & 2 & 0  \\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\beta_3\n\\end{bmatrix}=\n\\begin{bmatrix}\n0 \\\\\n2\n\\end{bmatrix} \\iff {\\bf T}_{2 \\times 4} \\bbeta_{4 \\times 1} = {\\bf c}_{2 \\times 1}$$\n\n\n[^2]: Assume only $r$ of the $m$ equations are independent.\n\n\n\n## General $F$-test {visibility=\"hidden\"}\n\n<!-- - $F_{test} = \\frac{(\\bf Tb - c)'\\left[T(X'X)^{-1}T'\\right]^{-1}(\\bf Tb - c)}{rMS_{res}}$ where $r$ is the rank of $\\bf T$ -->\n- Reject $H_0$ if $F_{test} > F_{\\alpha, r, n-p}$, where $r$ is the rank of $\\bf T$. $(F_{test}$ is shown in Eq. (3.43) in LRA)\n\n- Back to Delivery example. Suppose we test $H_0: \\beta_{1} = \\beta_{2} = 0 \\quad H_1: \\beta_j \\ne 0 \\text{ for at least one } j$\n\n. . .\n\n::: question\nWhat are $\\bf T$, $\\bbeta$ and $\\bf c$?\n:::\n\n. . .\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n$$H_0: \\begin{align} &\\beta_1 = 0 \\\\\n&\\beta_2 = 0 \\end{align}$$\n\n$$\\begin{bmatrix}\n0 & 1 & 0  \\\\\n0 & 0 & 1\n\\end{bmatrix}\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\beta_2\n\\end{bmatrix}=\n\\begin{bmatrix}\n0 \\\\\n0\n\\end{bmatrix}$$\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nTT <- matrix(0, 2, 3)\nTT[1, 2] <- 1; TT[2, 3] <- 1\nTT\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    0    1    0\n[2,]    0    0    1\n```\n:::\n\n```{.r .cell-code}\nc0 <- c(0, 0)\n```\n:::\n\n:::\n::::\n\n\n## [R Lab]{.pink} General Linear Hypothesis {visibility=\"hidden\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(car)  ## Companion to Applied Regression\ncar::lht(model = delivery_lm,\n         hypothesis.matrix = TT,\n         rhs = c0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.my_class600}\nLinear hypothesis test\n\nHypothesis:\ncases = 0\ndistance = 0\n\nModel 1: restricted model\nModel 2: time ~ cases + distance\n\n  Res.Df  RSS Df Sum of Sq   F  Pr(>F)    \n1     24 5785                             \n2     22  234  2      5551 261 4.7e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n\n- This is (of course) exactly the same as the test for significance!\n\n\n## Standardized Regression Coefficients\n::: question\n$\\hat{y} = 5 + x_1 + 1000x_2$. Can we say the effect of $x_2$ on $y$ is 1000 times larger than the effect of $x_1$?\n:::\n\n. . .\n\n- Nope! If $x_1$ is measured in litres and $x_2$ in millilitres, although $b_2$ is 1000 times larger than $b_1$, *both effects on $\\hat{y}$ are identical.*\n\n. . .\n\n- The units of $\\beta_j$ are $\\frac{\\text{units of } y}{\\text{units of } x_j}$\n  + <span style=\"color:blue\"> $\\beta_2$: delivery time (min) $(y)$ per distance (ft) walked by the driver $(x_2)$. </span>\n- It is helpful to work with **dimensionless or standardized** coefficients.\n  + comparison\n  + get rid of round-off errors in ${\\bf (X'X)}^{-1}$.\n\n<!-- - Two methods of scaling: -->\n<!--   + Unit normal scaling -->\n<!--   + Unit length scaling -->\n\n::: notes\n- interpretation\n- It is difficult to compare coefficients because the magnitude of $b_j$ reflects the units of measurement of $x_j$.\n:::\n\n. . .\n\n::: question\nIn Intro Stats, how do we standardize a variable?\n:::\n\n\n## Unit Normal Scaling\n\n- $z_{ij} = \\frac{x_{ij}-\\overline{x}_j}{s_j}, \\, i = 1, \\dots, n, \\, j = 1, \\dots, k$, where $s_j$ is the sample SD of $x_j$.\n- $y^*_{i} = \\frac{y_{i}-\\overline{y}}{s_y}, \\, i = 1, \\dots, n$, where $s_y$ is the sample SD of $y$.\n- The scaled predictors and response have mean 0 and variance 1.\n- The new model:\n$$y_i^* = \\alpha_1z_{i1} + \\alpha_2z_{i2} + \\cdots + \\alpha_kz_{ik} + \\epsilon_i$$\n\n::: question\nWhy no intercept term $\\alpha_0$?\n:::\n\n. . .\n\n- The least-squares estimator for $\\balpha$:\n$${\\bf a} = {\\bf (Z'Z)}^{-1} {\\bf Z'y}^*$$\n\n\n## Unit Length Scaling  {visibility=\"hidden\"}\n- $w_{ij} = \\frac{x_{ij}-\\overline{x}_j}{S_{jj}^{1/2}}, \\, i = 1, \\dots, n, \\, j = 1, \\dots, k$, where $S_{jj} = \\sum_{i=1}^n(x_{ij}-\\overline{x}_j)^2$ is the (corrected) sum of squares for $x_j$.\n- $y^0_{i} = \\frac{y_{i}-\\overline{y}}{SS_T^{1/2}}, \\, i = 1, \\dots, n$.\n- The new regressor and response have mean 0 and length 1 $(\\sqrt{\\sum_{i=1}^nw_{ij}^2} = 1)$.\n- The new model:\n$$y_i^0 = \\alpha_1w_{i1} + \\alpha_2w_{i2} + \\cdots + \\alpha_kw_{ik} + \\epsilon_i$$\n- The least-squares estimator for $\\bsalpha$:\n$${\\bf a} = {\\bf (W'W)}^{-1} {\\bf W'y}^0$$\n\n::: notes\n- $S_{jj} = \\sum_{i=1}^n(x_{ij}-\\overline{x}_j)^2$ is the (corrected) sum of squares for $x_j$.\n:::\n\n\n## Comparison  {visibility=\"hidden\"}\n- ${\\bf W'W}$ is the correlation matrix formed by $x_1, \\dots, x_k$.\n- $\\bf W'y^0$ is the correlation between $x_j$ and $y$.\n- ${\\bf Z'Z} = (n-1) {\\bf W'W}$\n- ${\\bf a} = {\\bf (Z'Z)}^{-1} {\\bf Z'y}^* =  {\\bf (W'W)}^{-1} {\\bf W'y}^0$\n$$b_j = a_j \\left( \\frac{SS_T}{S_{jj}}\\right)^{1/2}, \\quad j = 1, 2, \\dots, k$$\n$$b_0 = \\overline{y} - \\sum_{j=1}^kb_j\\overline{x}_j$$\n\n\n## [R Lab]{.pink} Standardized Coefficients\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## unit normal scaling\nscale_data <- scale(delivery_data, center = TRUE, scale = TRUE)  ## becomes a matrix\napply(scale_data, 2, mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    time    cases distance \n-6.2e-17  8.9e-18  9.8e-17 \n```\n:::\n\n```{.r .cell-code}\napply(scale_data, 2, var)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    time    cases distance \n       1        1        1 \n```\n:::\n:::\n\n\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## No-intercept\nscale_lm <- lm(time ~ cases + distance - 1, data = as.data.frame(scale_data)) \nscale_lm$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   cases distance \n    0.72     0.30 \n```\n:::\n\n```{.r .cell-code}\n## With intercept\nscale_lm_0 <- lm(time ~ cases + distance, data = as.data.frame(scale_data))  \nscale_lm_0$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)       cases    distance \n   -3.3e-17     7.2e-01     3.0e-01 \n```\n:::\n:::\n\n\n\n::: notes\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n:::\n\n\n\n<!-- ## Other Topics -->\n<!-- - Justification of distributional properties -->\n<!-- - Simultaneous confidence intervals (region) -->\n<!-- - Extrapolation problem -->\n<!-- - Maximum likelihood estimation -->\n<!-- - Colliearity -->\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}