{
  "hash": "39fcb72a2f66e372b171c1bde58844c2",
  "result": {
    "markdown": "---\ntitle: \"Model Building, Selection and Validation\"\nsubtitle: \"MATH 4780 / MSSC 5780 Regression Analysis\"\nauthor: \"Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University\"\n# date: \"August 15 2023\"\n# macros: _macros.tex # import a list of TeX/LaTeX definitions\nformat: \n  revealjs:\n    # code-line-numbers: false\n    #     - \"macros.tex\"\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n    # include-in-header:\n    highlight-style: arrow\n    code-block-bg: true\n    self-contained: false\n    slide-number: c/t    \n    incremental: false\n    width: 1800\n    height: 1000\n    margin: 0.05\n    logo: \"https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg\"\n    footer: \"[math4780-f23.github.io/website](https://math4780-f23.github.io/website/)\"\n    theme: [\"simple\", \"slides.scss\"]\n    multiplex: true\n    code-link: true\n    fig-cap-location: bottom\n    fig-align: center\n    transition: none ## fade slide convex concave zoom\n    title-slide-attributes:\n      data-background-color: \"#447099\"\n      # data-background-image: images/paper-texture.jpg\n      # data-background-size: cover\n      # data-background-color: \"#698ED5\"\neditor: source\nexecute:\n  freeze: true\n  echo: false\n  purl: true\n---\n\n\n#  {visibility=\"hidden\"}\n\n\\def\\bx{\\mathbf{x}}\n\\def\\bg{\\mathbf{g}}\n\\def\\bw{\\mathbf{w}}\n\\def\\balpha{\\boldsymbol \\alpha}\n\\def\\bbeta{\\boldsymbol \\beta}\n\\def\\beps{\\boldsymbol \\epsilon}\n\\def\\bLambda{\\boldsymbol \\Lambda}\n\\def\\bX{\\mathbf{X}}\n\\def\\by{\\mathbf{y}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bW{\\mathbf{W}}\n\\def\\T{\\text{T}}\n\\def\\cov{\\mathrm{Cov}}\n\\def\\cor{\\mathrm{Corr}}\n\\def\\var{\\mathrm{Var}}\n\\def\\E{\\mathrm{E}}\n\\def\\bmu{\\boldsymbol \\mu}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\def\\Trace{\\text{Trace}}\n\n\n\n\n\n\n\n# Model Building, Selection and Validation\n<h2> Model Building Process </h2>\n<h2> Model Selection Criteria </h2>\n<h2> Selection Methods </h2>\n\n## Model Building\n\nSo far, we assume that we\n\n- have a very good idea of the basic form of the model (linear form after transformation)\n- know (nearly) all of the regressors that are important and should be used.\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n**Model Adequacy**\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/15-var-select/model_building.png){fig-align='center' width=75%}\n:::\n:::\n\n:::\n\n\n\n::: {.column width=\"50%\"}\n\n**Model Selection**\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/15-var-select/model_selection.png){fig-align='center' width=26%}\n:::\n:::\n\n:::\n::::\n\n\n::: notes\nOur strategy is \n- Fit the full model\n- Perform a thorough analysis (residual analysis, outliers, collinearity, etc)\n- Transformation of response/regressors\n- Test regressor significance\n- Perform a thorough analysis\n:::\n\n\n\n## Variable Selection\n\n- We have a *large pool of __candidate regressors__*, of which only a few are likely to be important. \n- Finding an appropriate subset of regressors for the model is called **model/variable selection**.\n\n. . .\n\nTwo \"conflicting\" goals in model building:\n\n- as many regressors as possible for better *predictive performance on new data* (**smaller bias**).\n\n- as few regressors as possible because as the number of regressors increases, \n  + $\\var(\\hat{y})$ will increase (**larger variance**)\n  + cost more in data collecting and maintaining\n  + more model complexity\n\nA compromise between the two hopefully leads to the *\"best\" regression equation*.\n\n::: question\nWhat does **best** mean?\n:::\n\n::: notes\n- In most practical problems, we have a *large pool of possible __candidate regressors__*, of which only a few are likely to be important. \n- Finding an appropriate subset of regressors for the model is called **variable selection**.\n- Two \"conflicting\" goals in model building:\n  + as many regressors as possible for more information for prediction\n  + as few regressors as possible (1) the variance of $\\hat{y}$ will increase as the number of regressors increases (2) cost more in data collection (3) more model complexity\n\nA compromise between the two hopefully leads to the *\"best\" regression equation*.\n:::\n\n. . .\n\nThere is **no unique definition of \"best\"**, and different methods specify **different subsets of the candidate regressors as best**.\n\n\n<!-- --- -->\n<!-- ## Model Misspecification -->\n<!-- - *Exclusion of relevant variables (underspecified)* -->\n<!--   + **True model**: ${\\bf y = X_1\\bsbeta_1 + X_2\\bsbeta_2 + \\bsep}$ -->\n<!--   + **Underspecified model**: ${\\bf y = X_1\\bsbeta_1 + \\bsdel}$ -->\n<!-- - *Inclusion of irrelevant variables (overspecified)* -->\n<!--   + **True model**: ${\\bf y = X\\bsbeta + \\bsep}$ -->\n<!--   + **Overspecified model**: ${\\bf y = X\\bsbeta + Z\\bsgamma + \\bsdel}$ -->\n\n\n<!-- |Property         | Underfit | Overfit | -->\n<!-- |:-------------------|:-------|:-------| -->\n<!-- | Bias of $\\small {\\bf b}$        | $E[\\small {\\bf b_1}_{under}] \\ne \\bsbeta_1$ unless $\\small {\\bf X_1'X_2=0}$| $E[{\\bf b}_{over}] = \\bsbeta$ -->\n<!-- | MSE                | $\\small MSE({\\bf b_1}_{under}) > MSE({\\bf b_1})$ unless $\\small {\\bf X_1'X_2=0}$| $\\small MSE({\\bf b}_{over}) > MSE({\\bf b})$ unless $\\small {\\bf X'Z=0}$ -->\n<!-- | Bias of $\\small \\hat{\\sigma}^2$ | $\\small E[\\hat{\\sigma}^2_{under}] > \\sigma^2$ even if $\\small {\\bf X_1'X_2=0}$| $\\small E[\\hat{\\sigma}^2_{over}] = \\sigma^2$ -->\n\n## Predictive Performance\n- A selected model that fits the observed sample data well may not predict well on new observations.\n- Build a good regression function/model in terms of **prediction accuracy**. (Model validation/assessment)\n- Want: The selected model minimizes the mean square prediction error (MSPE) on the *new data*:\n$$\\small MSPE(\\hat{y}) = E\\left[ (\\hat{y} - y)^2\\right] = E\\left[ (\\hat{y} - E(\\hat{y}))^2\\right] + [E(\\hat{y}) - y]^2 = \\var(\\hat{y}) + \\text{Bias}^2(\\hat{y})$$\n\n\n. . .\n\n- The mechanics of prediction is **easy**:\n  - Plug in values of predictors to the model equation.\n  - Calculate the predicted value of the response $\\hat{y}$\n\n::: notes\n- So, the mechanics of prediction is **easy**:\n  - Once you have your model, you can Plug in values of predictors to the model equation\n  - Calculate the predicted value of the response variable, $\\hat{y}$, either numerical or categorical.\n  \n:::\n\n\n \n. . .\n\n- Getting it right is **hard**! **No guarantee that**\n  - the model estimates are close to the truth\n  - your model performs as well with new data as it did with your sample data\n  \n  \n::: notes\n- But Getting it right is **hard**!\n  - There is no guarantee the model estimates you have are correct\n  - Or that your model will perform as well with new data as it did with your sample data\n- Test data are the new data that are not used for training or fitting our model, but the data we are interested in predicting its value.\n- So we care about the prediction performance on the test data much more than the performance on the training data.\n\n:::\n  \n\n## Spending Our Data\n\n- Several steps to create a useful model: \n    + Parameter estimation\n    + Model building and selection\n    + Performance assessment, etc.\n\n\n. . .\n\n- Doing all of this on the entire data may lead to **overfitting**: \n\n<br>\n\n> *The model performs well on the current sample data, but awfully predicts the response on the new data we are interested.*\n\n<br>\n\n\n<!-- - **Allocate specific subsets of data for different tasks**, as opposed to allocating the largest possible amount to the model parameter estimation only (what we've done so far). -->\n\n\n::: notes\n- When we are doing modeling, we are doing several steps to create a useful model, \n    + parameter estimation\n    + model selection\n    + performance assessment, etc.\n- Doing all of this on the entire data we have available may lead to **overfitting**. In classification, it means that our model labels the training response variable almost perfectly with very high classification accuracy, but the model performs very bad when fitted to the test data or incoming future emails for example.\n\n<!-- - What we wanna do is to  **Allocate specific subsets of data for different tasks**, as opposed to allocating the largest possible amount of the data to the model parameter estimation only which is exactly what we've done so far. Remember in linear regression, we also use the entire data set to train our linear regression model, and estimate the regression coefficients. -->\n<!-- - But now, if we wanna make sure that our model is good at predicting things, we probably want to avoid overfitting. And how? -->\n:::\n\n\n## Overfitting\n\n> *The model performs well on the current sample data, but awfully predicts the response on the new data we are interested.*\n\n- **Low error rate on observed data, but high prediction error rate on future unobserved data!**\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: https://i.pinimg.com/originals/72/e2/22/72e222c1542539754df1d914cb671bd7.png](./images/15-var-select/overfit_reg.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n::: notes\nhttps://i.pinimg.com/originals/72/e2/22/72e222c1542539754df1d914cb671bd7.png\n- Look at this illustration, and let's focus on the overfitting and classification case.\n- the blue and red points are our training data representing two categories, and the green points are the new data to be classified.\n- the black curve is the classification boundary that separates the two categories.\n- Based on the boundary, you can see that the classification performance on the training data set is perfect, because all blue points and red points are perfectly separated.\n- However, such classification rule generated by the training data may not be good for the new data.\n- With this boundary, ...\n- OK so, if we wanna make sure that our model is good at predicting things, we probably want to avoid overfitting. \n- But how?\n:::\n\n## Splitting Data\n\n- Often, we don't have another unused data to assess the performance of our model.\n\n- Solution: Pretend we have new data by splitting our data into **training set** and **test set (validation set)**!\n\n. . .\n\n- **Training set:**\n    - Sandbox for model building/selection\n    - Spend most of your time using the training set to develop the model\n    - Majority of the original sample data (usually ~ 80%)\n- **Test set:**\n    - Held in reserve to determine efficacy of one or two chosen models\n    - Critical to look at it *once only*, otherwise it becomes part of the modeling process\n    - Remainder of the data (usually ~ 20%)\n  \n  \n::: notes\n- **Allocate specific subsets of data for different tasks**, as opposed to allocating the largest possible amount to the model parameter estimation only (what we've done so far).\n- Well we do this by splitting our data. So we split our data into to sets, training set and testing set, or sometimes called validation set.\n- You can think about your training set as your sandbox for model building. You can do whatever you want, like data wrangling, data transformation, data tidying, and data visualization, all of which help you build an appropriate model.\n- So you Spend most of your time using the training set to develop the model\n- And this is the Majority of the original sample data, which is usually about 75% - 80% of your data. So you basically take a random sample from the data that is about 80% of it. \n- And you don't touch the remaining 20% of the data until you are ready to test your model performance.\n- So the test set is held in reserve to determine efficacy of one or two chosen models\n- Critical to look at it once, otherwise it becomes part of the modeling process\n- and that is the Remainder of the data, usually 20% - 25%\n- So ideally, we hope to use our entire data as training data to train our model, right? And to test the model performance, we just collect another data set as test data to be used for testing performance. But in reality, it is not the usual case. In reality, we only have one single data set, and it is hard to collect another sample data as test data.\n- So under this situation, this type of splitting data becomes a must if we want to have both training and test data. \n:::\n\n\n## Model Selection Criteria\n- The full (largest) model has $M$ candidate regressors.\n- There are $M \\choose p-1$ possible subset models of size $p$.\n- There are totally $2^M$ possible subset models.\n\n- An evaluation metric should consider **Goodness of Fit** and **Model Complexity**:\n\n> **Goodness of Fit**: The more regressors, the better\n\n> **Complexity Penalty**: The less regressors, the better\n\n. . .\n\n- Evaluate subset models:\n  + $R_{adj}^2$ $\\uparrow$\n  + Mallow's $C_p$  $\\downarrow$\n  + Information Criterion (AIC, BIC)  $\\downarrow$\n  + **PRE**diction **S**um of **S**quares (PRESS) $\\downarrow$ (Allen, D.M. (1974))\n\n\n::: notes\n- $R^2$ $\\uparrow$\n- $MS_{res}$ $\\downarrow$\n- The idea of model selection is to apply some penalty on the number of parameters used in the model. On one hand, we want to model fitting to be as good as possible, i.e., the mean squared error is small. On the other hand, we also want to restrict on the number of variables. We know that as we keep adding variables into a linear regression, the R2 would usually increase. Hence, there is a trade-off between the two. In general, we consider a criterion in the form of\n:::\n\n## Selection Criteria: Mallow's $C_p$ Statistic $\\downarrow$\nFor a model with $p$ coefficients ( $k$ predictors ),\n\n$$\\begin{align} C_p &= \\frac{SS_{res}(p)}{\\hat{\\sigma}^2} - n + 2p \\\\ &= p + \\frac{(s^2 - \\hat{\\sigma}^2)(n-p)}{\\hat{\\sigma}^2} \\end{align}$$\n\n- $\\hat{\\sigma}^2$ is the variance estimate from the *full* model, i.e., $\\hat{\\sigma}^2 = MS_{res}(M)$.\n\n- $s^2$ is the variance estimate from the model with $p$ coefficients, i.e., $s^2 = MS_{res}(p)$.\n\n- Favors the candidate model with the **smallest $C_p$**.\n\n- For unbiased models that $E[\\hat{y}_i] = E[y_i]$, $C_p = p$.\n  + All of the errors in $\\hat{y}_i$ is variance, and the model is not underfitted. \n\n::: notes\n- $C_p = M + 1$ for the full model. \n:::\n\n\n## Mallow's $C_p$ Plot\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/15-var-select/unnamed-chunk-5-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n- Model A is a heavily biased model.\n- Model D is the poorest performer.\n- Model B and C are reasonable.\n- Model C has $C_p < 3$ which implies $MS_{res}(3) < MS_{res}(M)$\n:::\n::::\n\n\n## Selection Criteria: Information Criterion $\\downarrow$\nFor a model with $p$ coefficients ( $k$ predictors ),\n\n- Akaike information criterion (AIC) is $$\\text{AIC} = n \\ln \\left( \\frac{SS_{res}(p)}{n} \\right) + 2p$$\n- Bayesian information criterion (BIC) is $$\\text{BIC} = n \\ln \\left( \\frac{SS_{res}(p)}{n} \\right) + p \\ln (n)$$\n- BIC penalizes more when adding more variables as the sample size increases. \n- BIC tends to choose models with less regressors.\n\n\n## Selection Criteria: PRESS $\\downarrow$\n- Predicted Residual Error Sum of Squares (PRESS)\n\n- $\\text{PRESS}_p = \\sum_{i=1}^n[y_i - \\hat{y}_{(i)}]^2 = \\sum_{i=1}^n\\left( \\frac{e_i}{1-h_{ii}}\\right)^2$ where $e_i = y_i - \\hat{y}_i$.\n\n- $R_{pred, p}^2 = 1 - \\frac{PRESS_p}{SS_T}$\n\n- $\\text{Absolute PRESS}_p = \\sum_{i=1}^n|y_i - \\hat{y}_{(i)}|$ can also be considered when some large prediction errors are too influential.\n\n\n## [R Lab]{.pink} Criteria Computation\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmanpower <- read.csv(file = \"./data/manpower.csv\", header = TRUE)\nlm_full <- lm(y ~ x1 + x2 + x3 + x4 + x5, \n              data = manpower)\nsumm_full <- summary(lm_full)\n\n## Adjusted R sq.\nsumm_full$adj.r.squared  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.987\n```\n:::\n\n```{.r .cell-code}\n# PRESS\nsum((lm_full$residual / \n       (1 - hatvalues(lm_full))) ^ 2) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 32195222\n```\n:::\n:::\n\n\n<!-- - `AIC()` calculating AIC using -2 log-likelihood. -->\n- `ols_mallows_cp()` for Mallow's $C_p$ in [`olsrr`](https://olsrr.rsquaredacademy.com/) package\n:::\n\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## AIC\nextractAIC(lm_full, k = 2)  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]   6 224\n```\n:::\n\n```{.r .cell-code}\n## BIC\nn <- length(manpower$y)\nextractAIC(lm_full, k = log(n)) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]   6 229\n```\n:::\n:::\n\n\n<!-- # ```{r} -->\n<!-- # #| out-width: 50% -->\n<!-- # knitr::include_graphics(\"./images/15-var-select/hex_olsrr.png\") -->\n<!-- # ``` -->\n[![](./images/15-var-select/hex_olsrr.png){width=50% height=50%}](https://olsrr.rsquaredacademy.com/)\n:::\n::::\n\n\n\n\n::: notes\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n'log Lik.' -130 (df=7)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 224\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 229\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.43\n```\n:::\n:::\n\n:::\n\n\n## Selection Methods: Best Subset (All Possible) Selection\n- Assume the intercept is in all models.\n- If there are $M$ possible regressors, we investigate all $2^M - 1$ possible regression equations.\n- Use the selection criteria to determine some candidate models and complete regression analysis on them.\n- If the estimates of a particular coefficient tends to \"jump around\", this could be an indication of collinearity.\n\n\n## [R Lab]{.pink} Best Subset Selection  `ols_step_all_possible()`\n\n<!-- - The `ols_step_all_possible()` function in [`olsrr`](https://olsrr.rsquaredacademy.com/) package. -->\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nolsrr_all <- olsrr::ols_step_all_possible(lm_full)\nnames(olsrr_all)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"mindex\"     \"n\"          \"predictors\" \"rsquare\"    \"adjr\"      \n [6] \"predrsq\"    \"cp\"         \"aic\"        \"sbic\"       \"sbc\"       \n[11] \"msep\"       \"fpe\"        \"apc\"        \"hsp\"       \n```\n:::\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n- `n`: number of predictors\n- `predictors`: predictors in the model\n- `rsquare`: R-square of the model\n- `adjr`: adjusted R-square of the model\n- `predrsq`: predicted R-square of the model\n:::\n\n\n\n::: {.column width=\"50%\"}\n- `cp`: Mallowâ€™s Cp\n- `aic`: AIC\n- `sbic`: Sawa BIC\n- `sbc`: Schwarz BIC (the one we defined)\n:::\n::::\n\n##\n\n\n::: small\nModel (x2 x3 x5)\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```{.my_classfull}\n   Index N     Predictors R-Square Adj. R-Square Mallow's Cp\n3      1 1             x3    0.972         0.970       20.38\n1      2 1             x1    0.971         0.970       21.20\n2      3 1             x2    0.893         0.886      114.97\n4      4 1             x4    0.884         0.877      125.87\n5      5 1             x5    0.335         0.290      785.26\n10     6 2          x2 x3    0.987         0.985        4.94\n6      7 2          x1 x2    0.986         0.984        5.66\n14     8 2          x3 x5    0.985         0.983        7.29\n9      9 2          x1 x5    0.984         0.982        8.16\n13    10 2          x3 x4    0.975         0.972       18.57\n8     11 2          x1 x4    0.974         0.970       20.04\n7     12 2          x1 x3    0.973         0.969       21.99\n11    13 2          x2 x4    0.931         0.921       72.29\n12    14 2          x2 x5    0.924         0.913       80.30\n15    15 2          x4 x5    0.910         0.898       96.50\n23    16 3       x2 x3 x5    0.990         0.988        2.92\n18    17 3       x1 x2 x5    0.989         0.987        3.71\n16    18 3       x1 x2 x3    0.987         0.984        6.21\n22    19 3       x2 x3 x4    0.987         0.984        6.90\n17    20 3       x1 x2 x4    0.986         0.983        7.66\n20    21 3       x1 x3 x5    0.985         0.982        8.97\n25    22 3       x3 x4 x5    0.985         0.982        9.00\n21    23 3       x1 x4 x5    0.985         0.981        9.41\n19    24 3       x1 x3 x4    0.978         0.974       16.80\n24    25 3       x2 x4 x5    0.952         0.941       48.28\n30    26 4    x2 x3 x4 x5    0.991         0.988        4.03\n28    27 4    x1 x2 x4 x5    0.991         0.987        4.26\n27    28 4    x1 x2 x3 x5    0.991         0.987        4.35\n26    29 4    x1 x2 x3 x4    0.988         0.984        7.54\n29    30 4    x1 x3 x4 x5    0.985         0.980       10.92\n31    31 5 x1 x2 x3 x4 x5    0.991         0.987        6.00\n```\n:::\n:::\n\n:::\n\n\n\n::: notes\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n[1] 275\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 273\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 224\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 280\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 278\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 229\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 278\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 280\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 232\n```\n:::\n:::\n\n:::\n\n## [R Lab]{.pink} `ols_step_best_subset()`\n\n::: small\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# metric = c(\"rsquare\", \"adjr\", \"predrsq\", \"cp\", \"aic\", \"sbic\", \"sbc\", \"msep\", \"fpe\", \"apc\", \"hsp\")\nolsrr::ols_step_best_subset(lm_full, metric = \"predrsq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.my_classfull}\n   Best Subsets Regression   \n-----------------------------\nModel Index    Predictors\n-----------------------------\n     1         x3             \n     2         x2 x3          \n     3         x2 x3 x5       \n     4         x2 x3 x4 x5    \n     5         x1 x2 x3 x4 x5 \n-----------------------------\n\n                                                           Subsets Regression Summary                                                            \n-------------------------------------------------------------------------------------------------------------------------------------------------\n                       Adj.        Pred                                                                                                           \nModel    R-Square    R-Square    R-Square     C(p)        AIC         SBIC        SBC           MSEP             FPE            HSP         APC  \n-------------------------------------------------------------------------------------------------------------------------------------------------\n  1        0.9722      0.9703       0.956    20.3812    285.5158    234.8274    288.0155    15612703.3319    1025426.9349    65534.8041    0.0352 \n  2        0.9867      0.9848       0.964     4.9416    274.9519    227.0975    278.2847     8029607.6257     552301.0536    36111.9920    0.0190 \n  3        0.9901      0.9878       0.964     2.9177    272.0064    226.8104    276.1724     6503027.4301     466884.0206    31496.1442    0.0160 \n  4        0.9908      0.9877       0.942     4.0263    272.6849    229.2716    277.6841     6563621.8068     490245.8263    34438.7564    0.0168 \n  5        0.9908      0.9867       0.935     6.0000    274.6442    232.3507    280.4767     7202730.2305     557787.1896    41227.7488    0.0192 \n-------------------------------------------------------------------------------------------------------------------------------------------------\nAIC: Akaike Information Criteria \n SBIC: Sawa's Bayesian Information Criteria \n SBC: Schwarz Bayesian Criteria \n MSEP: Estimated error of prediction, assuming multivariate normality \n FPE: Final Prediction Error \n HSP: Hocking's Sp \n APC: Amemiya Prediction Criteria \n```\n:::\n:::\n\n:::\n\n\n::: notes\n- `ols_step_best_subset()` identifies the best model of each size using $R^2$ by default.\n:::\n\n\n## [R Lab]{.pink} Best Subset Selection\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"font-size: 21px; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> k </th>\n   <th style=\"text-align:right;\"> 1 </th>\n   <th style=\"text-align:right;\"> 2 </th>\n   <th style=\"text-align:right;\"> 3 </th>\n   <th style=\"text-align:right;\"> 4 </th>\n   <th style=\"text-align:right;\"> 5 </th>\n   <th style=\"text-align:right;\"> r2 </th>\n   <th style=\"text-align:right;\"> adj_r2 </th>\n   <th style=\"text-align:right;\"> cp </th>\n   <th style=\"text-align:right;\"> press </th>\n   <th style=\"text-align:right;\"> abs_press </th>\n   <th style=\"text-align:right;\"> r2_pred </th>\n   <th style=\"text-align:right;\"> aic </th>\n   <th style=\"text-align:right;\"> bic </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.972 </td>\n   <td style=\"text-align:right;\"> 0.970 </td>\n   <td style=\"text-align:right;\"> 20.38 </td>\n   <td style=\"text-align:right;\"> 2.18e+07 </td>\n   <td style=\"text-align:right;\"> 13431 </td>\n   <td style=\"text-align:right;\"> 0.956 </td>\n   <td style=\"text-align:right;\"> 235 </td>\n   <td style=\"text-align:right;\"> 237 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.971 </td>\n   <td style=\"text-align:right;\"> 0.970 </td>\n   <td style=\"text-align:right;\"> 21.20 </td>\n   <td style=\"text-align:right;\"> 2.22e+07 </td>\n   <td style=\"text-align:right;\"> 13666 </td>\n   <td style=\"text-align:right;\"> 0.955 </td>\n   <td style=\"text-align:right;\"> 236 </td>\n   <td style=\"text-align:right;\"> 237 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.893 </td>\n   <td style=\"text-align:right;\"> 0.886 </td>\n   <td style=\"text-align:right;\"> 114.97 </td>\n   <td style=\"text-align:right;\"> 1.58e+08 </td>\n   <td style=\"text-align:right;\"> 28031 </td>\n   <td style=\"text-align:right;\"> 0.680 </td>\n   <td style=\"text-align:right;\"> 258 </td>\n   <td style=\"text-align:right;\"> 260 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.884 </td>\n   <td style=\"text-align:right;\"> 0.877 </td>\n   <td style=\"text-align:right;\"> 125.87 </td>\n   <td style=\"text-align:right;\"> 7.01e+07 </td>\n   <td style=\"text-align:right;\"> 22890 </td>\n   <td style=\"text-align:right;\"> 0.858 </td>\n   <td style=\"text-align:right;\"> 260 </td>\n   <td style=\"text-align:right;\"> 261 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.335 </td>\n   <td style=\"text-align:right;\"> 0.290 </td>\n   <td style=\"text-align:right;\"> 785.26 </td>\n   <td style=\"text-align:right;\"> 4.56e+08 </td>\n   <td style=\"text-align:right;\"> 61324 </td>\n   <td style=\"text-align:right;\"> 0.079 </td>\n   <td style=\"text-align:right;\"> 289 </td>\n   <td style=\"text-align:right;\"> 291 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.987 </td>\n   <td style=\"text-align:right;\"> 0.985 </td>\n   <td style=\"text-align:right;\"> 4.94 </td>\n   <td style=\"text-align:right;\"> 1.79e+07 </td>\n   <td style=\"text-align:right;\"> 12742 </td>\n   <td style=\"text-align:right;\"> 0.964 </td>\n   <td style=\"text-align:right;\"> 225 </td>\n   <td style=\"text-align:right;\"> 227 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.986 </td>\n   <td style=\"text-align:right;\"> 0.984 </td>\n   <td style=\"text-align:right;\"> 5.66 </td>\n   <td style=\"text-align:right;\"> 1.80e+07 </td>\n   <td style=\"text-align:right;\"> 12873 </td>\n   <td style=\"text-align:right;\"> 0.964 </td>\n   <td style=\"text-align:right;\"> 225 </td>\n   <td style=\"text-align:right;\"> 228 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.985 </td>\n   <td style=\"text-align:right;\"> 0.983 </td>\n   <td style=\"text-align:right;\"> 7.29 </td>\n   <td style=\"text-align:right;\"> 1.26e+07 </td>\n   <td style=\"text-align:right;\"> 10915 </td>\n   <td style=\"text-align:right;\"> 0.974 </td>\n   <td style=\"text-align:right;\"> 227 </td>\n   <td style=\"text-align:right;\"> 230 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.984 </td>\n   <td style=\"text-align:right;\"> 0.982 </td>\n   <td style=\"text-align:right;\"> 8.16 </td>\n   <td style=\"text-align:right;\"> 1.30e+07 </td>\n   <td style=\"text-align:right;\"> 11074 </td>\n   <td style=\"text-align:right;\"> 0.974 </td>\n   <td style=\"text-align:right;\"> 228 </td>\n   <td style=\"text-align:right;\"> 230 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.975 </td>\n   <td style=\"text-align:right;\"> 0.972 </td>\n   <td style=\"text-align:right;\"> 18.57 </td>\n   <td style=\"text-align:right;\"> 3.25e+07 </td>\n   <td style=\"text-align:right;\"> 16825 </td>\n   <td style=\"text-align:right;\"> 0.934 </td>\n   <td style=\"text-align:right;\"> 235 </td>\n   <td style=\"text-align:right;\"> 238 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.974 </td>\n   <td style=\"text-align:right;\"> 0.970 </td>\n   <td style=\"text-align:right;\"> 20.04 </td>\n   <td style=\"text-align:right;\"> 3.58e+07 </td>\n   <td style=\"text-align:right;\"> 17328 </td>\n   <td style=\"text-align:right;\"> 0.928 </td>\n   <td style=\"text-align:right;\"> 236 </td>\n   <td style=\"text-align:right;\"> 239 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.973 </td>\n   <td style=\"text-align:right;\"> 0.969 </td>\n   <td style=\"text-align:right;\"> 21.99 </td>\n   <td style=\"text-align:right;\"> 2.26e+07 </td>\n   <td style=\"text-align:right;\"> 13957 </td>\n   <td style=\"text-align:right;\"> 0.954 </td>\n   <td style=\"text-align:right;\"> 237 </td>\n   <td style=\"text-align:right;\"> 240 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.931 </td>\n   <td style=\"text-align:right;\"> 0.921 </td>\n   <td style=\"text-align:right;\"> 72.29 </td>\n   <td style=\"text-align:right;\"> 1.41e+08 </td>\n   <td style=\"text-align:right;\"> 28166 </td>\n   <td style=\"text-align:right;\"> 0.716 </td>\n   <td style=\"text-align:right;\"> 253 </td>\n   <td style=\"text-align:right;\"> 255 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.924 </td>\n   <td style=\"text-align:right;\"> 0.913 </td>\n   <td style=\"text-align:right;\"> 80.30 </td>\n   <td style=\"text-align:right;\"> 1.29e+08 </td>\n   <td style=\"text-align:right;\"> 25960 </td>\n   <td style=\"text-align:right;\"> 0.740 </td>\n   <td style=\"text-align:right;\"> 254 </td>\n   <td style=\"text-align:right;\"> 257 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.910 </td>\n   <td style=\"text-align:right;\"> 0.898 </td>\n   <td style=\"text-align:right;\"> 96.50 </td>\n   <td style=\"text-align:right;\"> 7.12e+07 </td>\n   <td style=\"text-align:right;\"> 25251 </td>\n   <td style=\"text-align:right;\"> 0.856 </td>\n   <td style=\"text-align:right;\"> 257 </td>\n   <td style=\"text-align:right;\"> 260 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"font-size: 21px; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> k </th>\n   <th style=\"text-align:right;\"> 1 </th>\n   <th style=\"text-align:right;\"> 2 </th>\n   <th style=\"text-align:right;\"> 3 </th>\n   <th style=\"text-align:right;\"> 4 </th>\n   <th style=\"text-align:right;\"> 5 </th>\n   <th style=\"text-align:right;\"> r2 </th>\n   <th style=\"text-align:right;\"> adj_r2 </th>\n   <th style=\"text-align:right;\"> cp </th>\n   <th style=\"text-align:right;\"> press </th>\n   <th style=\"text-align:right;\"> abs_press </th>\n   <th style=\"text-align:right;\"> r2_pred </th>\n   <th style=\"text-align:right;\"> aic </th>\n   <th style=\"text-align:right;\"> bic </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.990 </td>\n   <td style=\"text-align:right;\"> 0.988 </td>\n   <td style=\"text-align:right;\"> 2.92 </td>\n   <td style=\"text-align:right;\"> 1.78e+07 </td>\n   <td style=\"text-align:right;\"> 12019 </td>\n   <td style=\"text-align:right;\"> 0.964 </td>\n   <td style=\"text-align:right;\"> 222 </td>\n   <td style=\"text-align:right;\"> 225 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.989 </td>\n   <td style=\"text-align:right;\"> 0.987 </td>\n   <td style=\"text-align:right;\"> 3.71 </td>\n   <td style=\"text-align:right;\"> 1.81e+07 </td>\n   <td style=\"text-align:right;\"> 12163 </td>\n   <td style=\"text-align:right;\"> 0.964 </td>\n   <td style=\"text-align:right;\"> 223 </td>\n   <td style=\"text-align:right;\"> 226 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.987 </td>\n   <td style=\"text-align:right;\"> 0.984 </td>\n   <td style=\"text-align:right;\"> 6.21 </td>\n   <td style=\"text-align:right;\"> 2.28e+07 </td>\n   <td style=\"text-align:right;\"> 14243 </td>\n   <td style=\"text-align:right;\"> 0.954 </td>\n   <td style=\"text-align:right;\"> 226 </td>\n   <td style=\"text-align:right;\"> 229 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.987 </td>\n   <td style=\"text-align:right;\"> 0.984 </td>\n   <td style=\"text-align:right;\"> 6.90 </td>\n   <td style=\"text-align:right;\"> 3.01e+07 </td>\n   <td style=\"text-align:right;\"> 15780 </td>\n   <td style=\"text-align:right;\"> 0.939 </td>\n   <td style=\"text-align:right;\"> 227 </td>\n   <td style=\"text-align:right;\"> 230 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.986 </td>\n   <td style=\"text-align:right;\"> 0.983 </td>\n   <td style=\"text-align:right;\"> 7.66 </td>\n   <td style=\"text-align:right;\"> 3.28e+07 </td>\n   <td style=\"text-align:right;\"> 16227 </td>\n   <td style=\"text-align:right;\"> 0.934 </td>\n   <td style=\"text-align:right;\"> 227 </td>\n   <td style=\"text-align:right;\"> 231 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.985 </td>\n   <td style=\"text-align:right;\"> 0.982 </td>\n   <td style=\"text-align:right;\"> 8.97 </td>\n   <td style=\"text-align:right;\"> 1.30e+07 </td>\n   <td style=\"text-align:right;\"> 11104 </td>\n   <td style=\"text-align:right;\"> 0.974 </td>\n   <td style=\"text-align:right;\"> 229 </td>\n   <td style=\"text-align:right;\"> 232 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.985 </td>\n   <td style=\"text-align:right;\"> 0.982 </td>\n   <td style=\"text-align:right;\"> 9.00 </td>\n   <td style=\"text-align:right;\"> 1.63e+07 </td>\n   <td style=\"text-align:right;\"> 11801 </td>\n   <td style=\"text-align:right;\"> 0.967 </td>\n   <td style=\"text-align:right;\"> 229 </td>\n   <td style=\"text-align:right;\"> 232 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.985 </td>\n   <td style=\"text-align:right;\"> 0.981 </td>\n   <td style=\"text-align:right;\"> 9.41 </td>\n   <td style=\"text-align:right;\"> 1.78e+07 </td>\n   <td style=\"text-align:right;\"> 12296 </td>\n   <td style=\"text-align:right;\"> 0.964 </td>\n   <td style=\"text-align:right;\"> 229 </td>\n   <td style=\"text-align:right;\"> 232 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.978 </td>\n   <td style=\"text-align:right;\"> 0.974 </td>\n   <td style=\"text-align:right;\"> 16.80 </td>\n   <td style=\"text-align:right;\"> 3.44e+07 </td>\n   <td style=\"text-align:right;\"> 18046 </td>\n   <td style=\"text-align:right;\"> 0.930 </td>\n   <td style=\"text-align:right;\"> 235 </td>\n   <td style=\"text-align:right;\"> 238 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.952 </td>\n   <td style=\"text-align:right;\"> 0.941 </td>\n   <td style=\"text-align:right;\"> 48.28 </td>\n   <td style=\"text-align:right;\"> 1.07e+08 </td>\n   <td style=\"text-align:right;\"> 26242 </td>\n   <td style=\"text-align:right;\"> 0.784 </td>\n   <td style=\"text-align:right;\"> 248 </td>\n   <td style=\"text-align:right;\"> 252 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.991 </td>\n   <td style=\"text-align:right;\"> 0.988 </td>\n   <td style=\"text-align:right;\"> 4.03 </td>\n   <td style=\"text-align:right;\"> 2.86e+07 </td>\n   <td style=\"text-align:right;\"> 14606 </td>\n   <td style=\"text-align:right;\"> 0.942 </td>\n   <td style=\"text-align:right;\"> 222 </td>\n   <td style=\"text-align:right;\"> 227 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.991 </td>\n   <td style=\"text-align:right;\"> 0.987 </td>\n   <td style=\"text-align:right;\"> 4.26 </td>\n   <td style=\"text-align:right;\"> 3.03e+07 </td>\n   <td style=\"text-align:right;\"> 14992 </td>\n   <td style=\"text-align:right;\"> 0.939 </td>\n   <td style=\"text-align:right;\"> 223 </td>\n   <td style=\"text-align:right;\"> 227 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.991 </td>\n   <td style=\"text-align:right;\"> 0.987 </td>\n   <td style=\"text-align:right;\"> 4.35 </td>\n   <td style=\"text-align:right;\"> 2.25e+07 </td>\n   <td style=\"text-align:right;\"> 13364 </td>\n   <td style=\"text-align:right;\"> 0.955 </td>\n   <td style=\"text-align:right;\"> 223 </td>\n   <td style=\"text-align:right;\"> 227 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.988 </td>\n   <td style=\"text-align:right;\"> 0.984 </td>\n   <td style=\"text-align:right;\"> 7.54 </td>\n   <td style=\"text-align:right;\"> 3.77e+07 </td>\n   <td style=\"text-align:right;\"> 17952 </td>\n   <td style=\"text-align:right;\"> 0.924 </td>\n   <td style=\"text-align:right;\"> 227 </td>\n   <td style=\"text-align:right;\"> 231 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.985 </td>\n   <td style=\"text-align:right;\"> 0.980 </td>\n   <td style=\"text-align:right;\"> 10.92 </td>\n   <td style=\"text-align:right;\"> 1.86e+07 </td>\n   <td style=\"text-align:right;\"> 12773 </td>\n   <td style=\"text-align:right;\"> 0.962 </td>\n   <td style=\"text-align:right;\"> 231 </td>\n   <td style=\"text-align:right;\"> 235 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.991 </td>\n   <td style=\"text-align:right;\"> 0.987 </td>\n   <td style=\"text-align:right;\"> 6.00 </td>\n   <td style=\"text-align:right;\"> 3.22e+07 </td>\n   <td style=\"text-align:right;\"> 16025 </td>\n   <td style=\"text-align:right;\"> 0.935 </td>\n   <td style=\"text-align:right;\"> 224 </td>\n   <td style=\"text-align:right;\"> 229 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n:::\n::::\n\n## [R Lab]{.pink} Best Subset Selection\n\nScale Cp, AIC, BIC to $[0, 1]$.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/15-var-select/unnamed-chunk-17-1.png){fig-align='center' width=72%}\n:::\n:::\n\n\n\n## Selection Methods: Forward Selection\n\n- Begins with **no regressors**.\n- Insert regressors into the model **one at a time**.\n\n. . .\n\n- The first regressor selected, $x_1$, is the one producing the **largest $R^2$** of any single regressor. It is the one with\n  + the **highest correlation** with the response. \n  + the **largest $F_{test}$** and $F_{test} > F_{IN}(\\alpha, 1, n-2)$, where $F_{IN}$ is the pre-specified $F$ threshold.\n\n. . .\n\n- The second regressor $x_2$ produces the **largest increase in $R^2$ in the presence of $x_1$**. It is the one with\n  + the **largest partial correlation** with the response. \n  + the **largest partial $F_{test}  = \\frac{SS_R(x_2|x_1)}{MS_{res}(x_1, x_2)}$** and $F_{test} > F_{IN}(\\alpha, 1, n-3)$\n\n. . .\n\n- The process terminates when \n  + partial $F_{test} <F_{IN}(\\alpha, 1, n-p)$, or\n  + the last candidate regressor is added to the model.\n\n::: notes\n$F_{test} > F_{\\alpha, 1, n-2}$\n$F_{test}  = \\frac{SS_R(x_2|x_1)}{MS_{res}(x_1, x_2)}> F_{\\alpha, 1, n-2}$\nOnce a regressor has been added, it cannot be removed at a later step.\n- Begins with **no regressors** in the model other than the intercept.\n- Insert regressors into the model **one at a time**.\n- The first regressor selected to be entered into the model, say $x_1$, is the one that produces the **largest $R^2$** of any single regressor.\n  + the one with the **highest correlation** with the response. \n  + the one with the **largest $F_{test}$** and $F_{test} > F_{IN}$\n- The second regressor examined is the one, say $x_2$, that produces the **largest increase in $R^2$ in the presence of $x_1$**.\n  + the one with the **largest partial correlation** with the response. \n  + the one with the **largest $F_{test}  = \\frac{SS_R(x_2|x_1)}{MS_{res}(x_1, x_2)}$** and $F_{test} > F_{IN}$\n- The process terminates either when the partial $F$ statistic at a particular step does not exceed $F_{IN}$ or when the last candidate regressor is added to the model.\n\n:::\n\n\n\n\n## [R Lab]{.pink} Forward Selection `ols_step_forward_p()`\n\n- The default threshold compared with the $p$-value $=P(F_{1, n-p} > F_{test})$ is 0.3.\n- If $p$-value < 0.3, the regressor is entered.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(olsrr_for <- olsrr::ols_step_forward_p(lm_full, penter = 0.3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n                             Selection Summary                              \n---------------------------------------------------------------------------\n        Variable                  Adj.                                         \nStep    Entered     R-Square    R-Square     C(p)        AIC         RMSE      \n---------------------------------------------------------------------------\n   1    x3            0.9722      0.9703    20.3812    285.5158    957.8556    \n   2    x2            0.9867      0.9848     4.9416    274.9519    685.1685    \n   3    x5            0.9901      0.9878     2.9177    272.0064    614.7794    \n---------------------------------------------------------------------------\n```\n:::\n\n```{.r .cell-code}\n# names(olsrr_for)\n```\n:::\n\n\n\n## [R Lab]{.pink} Forward Selection `ols_step_forward_p()`\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nolsrr_for$model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = paste(response, \"~\", paste(preds, collapse = \" + \")), \n    data = l)\n\nCoefficients:\n(Intercept)           x3           x2           x5  \n   1523.389        0.978        0.053     -320.951  \n```\n:::\n:::\n\n\n- `progress = TRUE`, `details = TRUE` for detailed selection progress.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nolsrr::ols_step_forward_p(lm_full, penter = 0.3, progress = TRUE, details = TRUE)\n```\n:::\n\n\n## .pink[R Lab:] Forward Selection `ols_step_forward_aic()` {visibility=\"hidden\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(olsrr_for_aic <- olsrr::ols_step_forward_aic(lm_full))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n                           Selection Summary                             \n------------------------------------------------------------------------\nVariable       AIC       Sum Sq         RSS          R-Sq      Adj. R-Sq \n------------------------------------------------------------------------\nx3           285.516    4.81e+08    13762308.863    0.97218      0.97033 \nx2           274.952    4.88e+08     6572382.538    0.98671      0.98482 \nx5           272.006     4.9e+08     4913398.503    0.99007      0.98778 \n------------------------------------------------------------------------\n```\n:::\n\n```{.r .cell-code}\nnames(olsrr_for_aic)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"predictors\" \"steps\"      \"arsq\"       \"aics\"       \"ess\"       \n[6] \"rss\"        \"rsq\"        \"model\"     \n```\n:::\n:::\n\n\n\n\n## Selection Methods: Stepwise Regression\n\n- This procedure is a *modification of forward selection.*\n- At each step, all regressors put into the model are reassessed via their partial $F$ statistic.\n- A regressor added at an earlier step may now be redundant because of the relationships between it and regressors now in the equation. \n- If the partial $F_{test} < F_{OUT}$, the variable will be removed.\n- The method requires both an $F_{IN}$ and $F_{OUT}$.\n\n\n::: notes\nit becomes insignificant with the addition of other variables to the model.\n:::\n\n## [R Lab]{.pink} Stepwise Regression `ols_step_both_p()`\n- If $p$-value < pent = 0.1, the regressor is entered.\n- After refitting, the regressor is removed if $p$-value > prem = 0.3.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(olsrr_both <- olsrr::ols_step_both_p(lm_full, pent = 0.1, prem = 0.3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n                              Stepwise Selection Summary                                \n---------------------------------------------------------------------------------------\n                     Added/                   Adj.                                         \nStep    Variable    Removed     R-Square    R-Square     C(p)        AIC         RMSE      \n---------------------------------------------------------------------------------------\n   1       x3       addition       0.972       0.970    20.3810    285.5158    957.8556    \n   2       x2       addition       0.987       0.985     4.9420    274.9519    685.1685    \n   3       x5       addition       0.990       0.988     2.9180    272.0064    614.7794    \n---------------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n## [R Lab]{.pink} Stepwise Regression `step()` {visibility=\"hidden\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x2 + x3 + x5, data = manpower)\n\nCoefficients:\n(Intercept)           x2           x3           x5  \n   1523.389        0.053        0.978     -320.951  \n```\n:::\n:::\n\n\n::: notes\n- Choose a model by AIC in a Stepwise Algorithm\n:::\n\n\n\n\n## Comments on Stepwise-Type Procedures\n\n- There is a **backward elimination** stepwise-type procedure\n  + Begin with the model with all $M$ candidate regressors.\n  + Remove regressors from the model **one at a time**.\n  + `olsrr::ols_step_backward_p()`\n  + Check [Variable Selection Methods](https://olsrr.rsquaredacademy.com/articles/variable_selection.html#best-subset-regression)\n\n. . .\n\n- One can select models/variables based on other metrics such as AIC\n  + `olsrr::ols_step_forward_aic()`\n  + `olsrr::ols_step_both_aic()`\n  + `olsrr::ols_step_backward_aic()`\n  \n. . .\n\n- The order in which the regressors enter or leave the model does *not* imply an order of importance to the regressors.\n\n- *No one model may be the \"best\".*\n\n- Different stepwise techniques could result in different models.\n<!-- - Inexperienced analysts may use the final model simply because the procedure spit it out. -->\n\n\n::: notes\n- The order in which the regressors enter or leave the model does not imply an order of importance to the regressors.\n- This is in fact a general problem with the forward selection procedure. Once a regressor has been added, it cannot be  removed at a later step.\n- forward selection tends to agree with all possible regressions for small subset sizes but not for large ones\n- backward elimination tends to agree with all possible regressions for large subset sizes but not for small ones.\n- the most common being that none of the procedures generally guarantees that the best subset regression model of any size will be identified.\n- there is one best subset model, but that there are several equally good ones.\n\n\nset F IN = F OUT = 4, as this corresponds roughly to the upper 5% point of the F distribution\nBendel and Afi fi [ 1974 ] recommend Î± = 0.25 for\nforward selection. F IN of between 1.3 and 2.\n\nKennedy and Bancroft [ 1971] suggest Î± = 0.25 for forward selection and recommend Î± = 0.10 for backward elimi\n\n- STRATEGY FOR VARIABLE SELECTION AND MODEL BUILDING\nthe PRESS statistic tends to recommend smaller models than Mallow's Cp, which in turn tends to recommend smaller models than the adjusted R2 .\n:::\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "15-var-select_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint/kePrint.js\"></script>\n<link href=\"../site_libs/lightable/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}