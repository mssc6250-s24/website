{
  "hash": "41df1303726a6756a07e81d358fa52a6",
  "result": {
    "markdown": "---\ntitle: 'Tree-based Methods `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 448 512\" style=\"height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M210.6 5.9L62 169.4c-3.9 4.2-6 9.8-6 15.5C56 197.7 66.3 208 79.1 208H104L30.6 281.4c-4.2 4.2-6.6 10-6.6 16C24 309.9 34.1 320 46.6 320H80L5.4 409.5C1.9 413.7 0 419 0 424.5c0 13 10.5 23.5 23.5 23.5H192v32c0 17.7 14.3 32 32 32s32-14.3 32-32V448H424.5c13 0 23.5-10.5 23.5-23.5c0-5.5-1.9-10.8-5.4-15L368 320h33.4c12.5 0 22.6-10.1 22.6-22.6c0-6-2.4-11.8-6.6-16L344 208h24.9c12.7 0 23.1-10.3 23.1-23.1c0-5.7-2.1-11.3-6-15.5L237.4 5.9C234 2.1 229.1 0 224 0s-10 2.1-13.4 5.9z\"/></svg>`{=html}'\nsubtitle: \"MSSC 6250 Statistical Machine Learning\"\nauthor: \"Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University\"\n# date: \"January 08 2024\"\n# macros: _macros.tex # import a list of TeX/LaTeX definitions\nformat: \n  revealjs:\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n    # include-in-header:\n    #     - \"macros.tex\"\n    highlight-style: github\n    code-block-bg: true\n    self-contained: false\n    slide-number: c/t\n    incremental: false\n    width: 1800\n    height: 1000\n    margin: 0.05\n    logo: \"https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg\"\n    footer: \"[mssc6250-s24.github.io/website](https://mssc6250-s24.github.io/website/)\"\n    theme: [\"simple\", \"styles.scss\"]\n    echo: false\n    multiplex: true\n    code-link: true\n    fig-cap-location: bottom\n    fig-align: center\n    transition: none ## fade slide convex concave zoom\n    title-slide-attributes:\n      data-background-color: \"#447099\"\n      # data-background-image: images/paper-texture.jpg\n      # data-background-size: cover\n      # data-background-color: \"#698ED5\"\neditor: source\nexecute:\n  freeze: true\n  echo: false\n---\n\n\n\n# {visibility=\"hidden\"}\n\n\\def\\cD{{\\cal D}}\n\\def\\cL{{\\cal L}}\n\\def\\cX{{\\cal X}}\n\\def\\cF{{\\cal F}}\n\\def\\cH{{\\cal H}}\n\\def\\bA{\\mathbf{A}}\n\\def\\bB{\\mathbf{B}}\n\\def\\bX{\\mathbf{X}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bU{\\mathbf{U}}\n\\def\\bD{\\mathbf{D}}\n\\def\\bV{\\mathbf{V}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bW{\\mathbf{W}}\n\\def\\bK{\\mathbf{K}}\n\\def\\bx{\\mathbf{x}}\n\\def\\by{\\mathbf{y}}\n\\def\\bs{\\mathbf{s}}\n\\def\\br{\\mathbf{r}}\n\\def\\bu{\\mathbf{u}}\n\\def\\be{\\mathbf{e}}\n\\def\\bv{\\mathbf{v}}\n\\def\\bp{\\mathbf{p}}\n\\def\\bw{\\mathbf{w}}\n\\def\\bz{\\mathbf{z}}\n\\def\\bzero{\\mathbf{0}}\n\\def\\balpha{\\boldsymbol \\alpha}\n\\def\\bbeta{\\boldsymbol \\beta}\n\\def\\btheta{\\boldsymbol \\theta}\n\\def\\bxi{\\boldsymbol \\xi}\n\\def\\bmu{\\boldsymbol \\mu}\n\\def\\bepsilon{\\boldsymbol \\epsilon}\n\\def\\T{\\text{T}}\n\\def\\Trace{\\text{Trace}}\n\\def\\Cov{\\text{Cov}}\n\\def\\Var{\\text{Var}}\n\\def\\E{\\text{E}}\n\\def\\pr{\\text{pr}}\n\\def\\Prob{\\text{P}}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n<!-- ## Tree-based Methods -->\n\n\n## Tree-based Methods\n\n- Can be used for regression and classification.\n\n- IDEA: Segmenting the predictor space into many simple regions.\n\n- Simple, useful for interpretation, and has nice graphical representation.\n\n- Not competitive with the best supervised learning approaches in terms of prediction accuracy. (Large bias)\n\n- Combining a large number of trees (**ensembles**) often results in improvements in prediction accuracy, at the expense of some loss interpretation.\n\n<!-- ## Adaptive  -->\n\n\n<!-- # Classification and Regression Trees (CART) -->\n\n## Decision Trees: Classification and Regression Trees (CART)\n\n- CART is a nonparametric method that *recursively partitions* the feature space into *hyper-rectangular* subsets (boxes), and make prediction on each subset.\n\n- Divide the predictor space — the set of possible values\nfor $X_1, X_2, \\dots, X_p$ — into $J$ distinct and non-overlapping regions, $R_1, R_2, \\dots, R_J$.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/unnamed-chunk-4-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n. . .\n\n- For every test point that falls into the region $R_j$ , we make the same\nprediction:\n  + *Regression*: the [*mean of the response values*]{.green} for the training points in $R_j$.\n  + *Classification*: the [*most commonly occurring class*]{.green} of training points in $R_j$.\n\n\n\n## Recursive Binary Splitting\n\n- Computationally infeasible to consider every possible partition of the feature space into $J$ boxes.\n\n. . .\n\n:::: {.columns}\n\n::: {.column width=\"80%\"}\n- The **recursive binary splitting** is *top-down* and *greedy*:\n  + [Top-down]{.green}: begins at the top of the tree\n  + [Greedy]{.green}: at each step, the *best* split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.\n  \n:::\n\n::: {.column width=\"20%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/14-tree/8_1-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n::::\n\n\n. . .\n\n- Select $X_j$ and a cutoff $s$ so that splitting the predictor space into $\\{\\bX \\mid X_j < s \\}$ and $\\{\\bX \\mid X_j \\ge s \\}$ leads to the greatest possible\nreduction in \n  + $SS_{res}$ for regression\n  + **Gini index**, **entropy** or misclassification rate for classification\n   \n- Repeatedly split one of the two previously identified regions until a stopping criterion is reached.\n   \n  \n  \n::: notes\nat each step of the tree-building process\n\n:::\n   \n## Classification Tree\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n- KNN requires K and a distance measure.\n\n- SVM requires kernels.\n\n- Tree solves this by *recursively partitioning* the feature space using a *binary splitting* rule $\\mathbf{1}\\{x \\le c \\}$\n\n- 0: [Red]{.red}; 1: [Blue]{.blue}\n:::\n\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/unnamed-chunk-6-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::::\n\n## Classification Tree\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\nIf $x_2 < -0.64$, $y = 0$.\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/rpart-plot-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/unnamed-chunk-9-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::::\n\n## Classification Tree\n\nIf $x_2 \\ge -0.64$ and $x_1 \\ge 0.69$, $y = 0$.\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/rpart-plot-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/unnamed-chunk-11-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::::\n\n## Classification Tree\n\n\nIf $x_2 \\ge -0.64$, $x_1 < 0.69$, and $x_2 \\ge 0.75$, $y = 0$.\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/rpart-plot-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/unnamed-chunk-13-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::::\n\n\n\n## Classification Tree\n\n\nIf $x_2 \\ge -0.64$, $x_1 < 0.69$, $x_2 < 0.75$, and $x_1 < -0.69$, $y = 0$.\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/rpart-plot-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/unnamed-chunk-15-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::::\n\n\n## Classification Tree\n\nStep 5 may not be beneficial.\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/rpart-plot-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/unnamed-chunk-17-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::::\n\n\n## Classification Tree\n\nStep 6 may not be beneficial. (Could overfit)\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/rpart-plot-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/unnamed-chunk-19-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::::\n\n## Classification Tree\n\nStep 7 may not be beneficial. (Could overfit)\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/rpart-plot-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/unnamed-chunk-21-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::::\n\n\n## Misclassification Rate\n\n- The classification error rate is the fraction of the training observations in that region that do not belong to the most common class: $$1 - \\max_{k} (\\hat{p}_{mk})$$ where $\\hat{p}_{mk}$ is the proportion of training observations in the $m$th region that are from the $k$th class.\n\n- Classification error is not sensitive for tree-growing.\n\n- Ideally hope to have nodes (regions) including training points that belong to only one class.\n\n\n\n## Gini Index (Impurity)\nThe Gini index is defined by\n\n$$\\sum_{k=1}^K \\hat{p}_{mk}(1 - \\hat{p}_{mk})$$ which is a measure of total variance across the K classes.\n\n- Gini is small if all of the $\\hat{p}_{mk}$s are close to zero or one. \n\n- **Node purity**: a small value indicates that a node contains predominantly\nobservations from a single class.\n\n\n\n\n## Shannon Entropy\n\nThe **Shannon entropy** is defined as \n\n$$- \\sum_{k=1}^K \\hat{p}_{mk} \\log(\\hat{p}_{mk}).$$\n- The entropy is near zero if the $\\hat{p}_{mk}$s are all near zero or one.\n\n- Gini index and the entropy are similar numerically.\n\n\n::: notes\nAny of these three approaches might be used when pruning the\ntree, but the classification error rate is preferable if prediction accuracy of\nthe final pruned tree is the goal.\n:::\n\n\n## Comparing Measures\n\n- Misclassification can be used for evaluating a tree, but may not be sensitive enough for building a tree.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/unnamed-chunk-22-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n::: notes\nFor each quantity, smaller value means that the node is more “pure”, hence, there is a higher certainty when we predict a new value. The idea of splitting a node is that, we want the two resulting child node to contain less variation. In other words, we want each child node to be as “pure” as possible.\n:::\n\n## Regression Tree\n\nThe goal is to find boxes $R_1, \\dots ,R_J$ that minimize the $SS_{res}$, given by $$\\sum_{j=1}^J\\sum_{i \\in R_j}\\left( y_i - \\hat{y}_{R_j}\\right)^2$$ where $\\hat{y}_{R_j}$ is the mean response for the training observations within $R_j$.\n\n\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/14-tree/partition.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n::: {.column width=\"70%\"}\n\n::: small\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: ISL Fig 8.3](./images/14-tree/reg-tree.png){fig-align='center' width=80%}\n:::\n:::\n\n:::\n\n:::\n\n::::\n\n\n## Tree Pruning\n\n- Using regression and classification performance measures to grow trees with no penalty on the tree size leads to overfitting.\n\n. . .\n\n- **Cost complexity pruning**:\n\nGiven the largest tree $T_{max}$,\n\n\\begin{align}\n\\min_{T \\subset T_{max}} \\sum_{m=1}^{|T|}\\sum_{i:x_i\\in R_m} \\left( y_i - \\hat{y}_{R_m}\\right)^2 +  \\alpha|T|\n\\end{align}\nwhere $|T|$ indicates the number of terminal nodes of the tree $T$.\n\n- Large $\\alpha$ results in small trees\n\n- Choose $\\alpha$ using CV\n\n- Algorithm 8.1 in ISL for building a regression tree.\n\n- Replace $SS_{res}$ with misclassification rate for classification. \n\n\n## Implementation\n\n- [`rpart::rpart()`](/https://cran.r-project.org/web/packages/rpart/rpart.pdf) \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(rpart)\nrpart::rpart(formula = y ~ x1 + x2, data)\n```\n:::\n\n\n\n<br>\n\n\n- [`tree::tree()`](https://cran.r-project.org/web/packages/tree/tree.pdf)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tree)\ntree::tree(formula = y ~ x1 + x2, data)\n```\n:::\n\n\n\n<br>\n\n\n- [`sklearn tree`](https://scikit-learn.org/stable/modules/tree.html#)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nfrom sklearn import tree\ndtc = tree.DecisionTreeClassifier()\ndtc = dtc.fit(X, y)\ndtr = tree.DecisionTreeRegressor()\ndtr = dtr.fit(X, y)\n```\n:::\n\n\n\n## Demo `rpart()`^[Read ISL Sec. 8.3 for `tree()` demo] {visibility=\"hidden\"}\n\n- `rpart()` uses the 10-fold CV (`xval` in `rpart.control()`)\n- `cp` is the complexity parameter\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrpart.control(minsplit = 20, minbucket = round(minsplit/3), cp = 0.01, \n              maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, xval = 10,\n              surrogatestyle = 0, maxdepth = 30, ...)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrpart.fit <- rpart::rpart(as.factor(y) ~ x1 + x2, \n                          data = data.frame(x1, x2, y),\n                          control = rpart.control(xval = 10, cp = 0))\nrpart.fit$cptable\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       CP nsplit rel error xerror   xstd\n1 0.17040      0     1.000  1.000 0.0498\n2 0.14798      3     0.484  0.637 0.0452\n3 0.01121      4     0.336  0.399 0.0384\n4 0.00224      7     0.300  0.377 0.0375\n5 0.00000      9     0.296  0.381 0.0377\n```\n:::\n:::\n\n\n\n## Demo `rpart()` {visibility=\"hidden\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplotcp(rpart.fit)\n```\n\n::: {.cell-output-display}\n![](images/14-tree/unnamed-chunk-30-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Demo `rpart()` {visibility=\"hidden\"}\n\n:::: {.columns}\n\n::: {.column width=\"55%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprunedtree <- prune(rpart.fit, cp = 0.012)\nprunedtree\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.my_class600}\nn= 500 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 500 223 0 (0.5540 0.4460)  \n   2) x2< -0.644 90   6 0 (0.9333 0.0667) *\n   3) x2>=-0.644 410 193 1 (0.4707 0.5293)  \n     6) x1>=0.694 68   8 0 (0.8824 0.1176) *\n     7) x1< 0.694 342 133 1 (0.3889 0.6111)  \n      14) x2>=0.748 53   7 0 (0.8679 0.1321) *\n      15) x2< 0.748 289  87 1 (0.3010 0.6990)  \n        30) x1< -0.69 51   9 0 (0.8235 0.1765) *\n        31) x1>=-0.69 238  45 1 (0.1891 0.8109) *\n```\n:::\n:::\n\n:::\n\n:::{.column width=\"45%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrpart.plot::rpart.plot(prunedtree)\n```\n\n::: {.cell-output-display}\n![](images/14-tree/unnamed-chunk-32-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n::::\n\n\n\n## Trees v.s. Linear Regression\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n[**Linear regression**]{.green}\n\n$$f(X) = \\beta_0 + \\sum_{j=1}^pX_j\\beta_j$$\n\n- Performs better when the relationship between $y$ and $x$ is approximately linear.\n\n\n:::\n\n\n::: {.column width=\"50%\"}\n[**Regression tree**]{.green}\n\n$$f(X) = \\sum_{j=1}^J \\hat{y}_{R_j}\\mathbf{1}(\\bX \\in R_j)$$\n\n\n- Performs better when there is a highly nonlinear and complex relationship between $y$ and $x$.\n\n- Preferred for interpretability and visualization.\n\n:::\n\n::::\n\n## Trees v.s. Linear Models\n\n::: small\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: ISL Fig 8.7](./images/14-tree/8_7-1.png){fig-align='center' width=80%}\n:::\n:::\n\n:::\n\n<!-- # Bagging {background-color=\"#447099\"} -->\n\n\n# Ensemble Learning: Bagging, Random Forests, Boosting\n\n> **Two heads are better than one**, not because either is infallible, but because they are unlikely to go wrong in the same direction. -- C.S. Lewis, British Writer (1898 - 1963)\n\n> 『三個臭皮匠，勝過一個諸葛亮』\n\n\n\n\n## Ensemble Methods\n\n- An **ensemble** method combines many **weak learners** (unstable, less accurate) to obtain a single and powerful model.\n\n- The CARTs suffer from *high variance*.\n\n- If independent $Z_1, \\dots, Z_n$ have variance $\\sigma^2$, then $\\bar{Z}$ has variance $\\sigma^2/n$.\n\n- *Averaging a set of observations reduces variance!*\n\n. . .\n\nWith $B$ separate training sets, \n\n$$\\hat{f}_{avg}(x) = \\frac{1}{B}\\sum_{b=1}^B\\hat{f}_{b}(x)$$\n\n## Bagging\n\n- **Bootstrap aggregation**, or **bagging** is a procedure for reducing variance.\n\n- Generate $B$ bootstrap samples by repeatedly \nsampling with replacement from the training set $B$ times.\n\n::: {.midi}\n$$\\hat{f}_{bag}(x) = \\frac{1}{B}\\sum_{b=1}^B\\hat{f}^*_{b}(x)$$\n:::\n\n::: small\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: Wiki page of bootstrap aggregating](https://upload.wikimedia.org/wikipedia/commons/c/c8/Ensemble_Bagging.svg){fig-align='center' width=50%}\n:::\n:::\n\n:::\n\n::: notes\nBagging works for many regression methods, but it is particularly useful for decision trees.\n:::\n\n## Bagging on Decision Trees\n\n::: small\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: Wiki page of ensemble learning](https://upload.wikimedia.org/wikipedia/commons/4/4a/Ensemble_Aggregation.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n## CART v.s. Bagging\n\n- For CART, the decision line has to be aligned to axis.\n\n- For Bagging, $B = 200$ each having 400 training points. Boundaries are smoother.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"33%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/unnamed-chunk-37-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n\n\n::: {.column width=\"33%\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/cart-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n::: {.column width=\"33%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/bagging-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n::::\n<!-- # Random Forests {background-color=\"#447099\"} -->\n\n## Notes of Bagging\n- Using a very large $B$ will not lead to overfitting.\n\n- Use $B$ sufficiently large that the error has settled down.\n\n- Bagging improves prediction accuracy at the expense of interpretability.\n\n. . .\n\n- When different trees are highly correlated, simply averaging is not very effective.\n\n- If there is one very strong predictor in the data set, in the collection of bagged trees, most or all of the trees will use this strong predictor in the top split. Therefore, all of the bagged trees will look quite similar to each other.\n\n- The predictions from the bagged trees will be highly correlated, and hence averaging does not lead to as large reduction in variance.\n\n\n::: notes\n- Use many trees and averaging. It is no longer clear which variables are most important to the procedure.\n\n- If there is one very strong predictor in the data set, in the collection of bagged trees, most or all of the trees will use this strong predictor in the top split. Therefore, all of the bagged trees will look quite similar to each other.\n\n- The predictions from the bagged trees will be highly correlated, and hence averaging does not lead to as large reduction in variance.\n\n:::\n\n## Random Forests\n- **Random forests** improve bagged trees by *decorrelating* the trees.\n\n- *$m$ predictors are randomly sampled as split candidates from the $p$ predictors.*\n\n<!-- - The split is allowed to use only one of those $m$ predictors. -->\n\n::: small\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: Misra and Li in Machine Learning for Subsurface Characterization (2020)](./images/14-tree/rf.jpg){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: notes\n- When building decision trees, each time a split in a tree is considered, *a random sample of $m$ predictors is chosen as split candidates from the full set of $p$ predictors.*\n\n- The split is allowed to use only one of those $m$ predictors.\n:::\n\n\n\n## Random Forests\n\n- $m \\approx \\sqrt{p}$ for classification; $m \\approx p/3$ for regression.\n\n- *Decorrelating*: on average $(p − m)/p$ of the splits will not even consider the strong predictor, and so other predictors will have more of a chance.\n\n- If $m = p$, random forests = bagging.\n\n- The improvement is significant when $p$ is large.\n\n\n## CART vs. Bagging vs. Random Forests\n\n- `randomForest::randomForest(x, y, mtry, ntree, nodesize, sampsize)`\n  <!-- + `mtry`: number of randomly sampled variable to consider at each internal node ($m$) -->\n  <!-- + `ntree`: number of trees ($B$) -->\n  <!-- + `nodesize`: stop splitting when the size of terminal nodes is no larger than nodesize -->\n  <!-- + `sampsize`: how many samples to use when fitting each tree -->\n\n:::: {.columns}\n\n::: {.column width=\"33%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/unnamed-chunk-41-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n\n\n::: {.column width=\"33%\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/unnamed-chunk-42-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n::: {.column width=\"33%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/unnamed-chunk-43-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n::::\n\n\n\n## Boosting\n\n- Bagging trees are built on bootstrap data sets, independent with each other.\n\n- **Boosting**^[Boosting can also be applied to many models, but most effectively applied to decision trees.] trees are grown *sequentially*: each tree is grown using information from previously grown trees.\n\n- Boosting does not involve bootstrap sampling; instead each\ntree is fit on a *modified version of the original data set*, the residuals! \n\n\n::: small\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: Fig. 12.1 of Hands-on Machine Learning with R](./images/14-tree/boosting.jpg){fig-align='center' width=70%}\n:::\n:::\n\n:::\n\n\n<!-- - Consider producing a sequence of learners (trees), $\\hat{f}^1, \\dots, \\hat{f}^B$. -->\n\n<!-- - Boosting learns slowly  -->\n\n\n## Boosting\n\n- *Base (weak) learners*: Could any simple model (high-bias low-variance). Usually a decision tree.\n\n- *Training weak models*: Fit a (shallow) tree $\\hat{f}^b$ with relatively few $d$ splits\n\n- *Sequential Training w.r.t. residuals*: Fitting each tree in the sequence to the previous tree's residuals. \n\nSuppose our final tree is $\\hat{f}(x)$ that starts with $0$.\n\n1. Fit a decision tree $\\hat{f}^1$ to $\\{ y_i \\}$\n2. Grow the tree: $\\hat{f}(x) = \\lambda\\hat{f}^1$\n3. Fit the next decision tree $\\hat{f}^2$ to the residuals of the previous fit $\\{ e^1_i\\} = \\{  y_i - \\lambda\\hat{f}^1(x_i)\\}$\n4. Grow the tree: $\\hat{f}(x) = \\lambda\\hat{f}^1 + \\lambda\\hat{f}^2$\n5. Fit the next decision tree $\\hat{f}^3$ to the residuals of the previous fit $\\{ e^2_i \\} = \\{  e^1_i - \\lambda\\hat{f}^2(x_i)\\}$\n6. Grow the tree: $\\hat{f}(x) = \\lambda\\hat{f}^1 + \\lambda\\hat{f}^2 + \\lambda\\hat{f}^3$\n\n<!-- Continue growing trees until the residuals are small enough. -->\n\n## Boosting\n- The final boosting tree is \n$$\\hat{f}(x) = \\sum_{b=1}^B\\lambda\\hat{f}^b(x)$$\n- Tuning parameters\n  + Number of base trees $B$: Large $B$ can overfit. Use cross-validation to choose $B$.\n  + Number of base tree splits $d$: Often $d=1$ works well. The growth of a tree takes into account the other grown trees, so small trees are sufficient.\n  + Shrinkage $\\lambda > 0$: Controls the learning rate of boosting. Usual values are 0.01 or 0.001. Small $\\lambda$ needs large $B$.\n\n\n## Boosting for Classification\n\n- `distribution = \"bernoulli\"`: **LogitBoost**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngbm.fit = gbm(y ~ ., data = data.frame(x1, x2, y), \n              distribution = \"bernoulli\", \n              n.trees = 10000, shrinkage = 0.01, bag.fraction = 0.6, \n              interaction.depth = 2, cv.folds = 10)\n```\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"33%\"}\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/unnamed-chunk-47-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n\n::: {.column width=\"33%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/unnamed-chunk-48-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n:::\n\n\n::: {.column width=\"33%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/unnamed-chunk-49-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n:::\n::::\n\n## Boosting Cross Validation\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngbm.perf(gbm.fit, method = \"cv\")\n```\n\n::: {.cell-output-display}\n![](images/14-tree/unnamed-chunk-50-1.png){fig-align='center' width=100%}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1181\n```\n:::\n:::\n\n\n\n\n## Boosting for Regression\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/unnamed-chunk-51-1.png){fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngbm.fit <- gbm::gbm(y ~ x, data = data.frame(x, y), \n                    distribution = \"gaussian\", n.trees = 300,\n                    shrinkage = 0.5, bag.fraction = 0.8,\n                    cv.folds = 10)\n```\n:::\n\n\n\n## Boosting for Regression\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/14-tree/unnamed-chunk-53-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n<!-- ## Summary of Tree Ensemble Methods -->\n\n<!-- - **Bagging** the trees are grown independently on random samples of -->\n<!-- the observations. Consequently, the trees tend to be quite similar to -->\n<!-- each other. Thus, bagging can get caught in local optima and can fail -->\n<!-- to thoroughly explore the model space. -->\n\n<!-- - **Random forests** the trees are once again grown independently on -->\n<!-- random samples of the observations. However, each split on each tree -->\n<!-- is performed using a random subset of the features, thereby decorrelating -->\n<!-- the trees, and leading to a more thorough exploration of model -->\n<!-- space relative to bagging. -->\n\n<!-- - **Boosting**, we only use the original data, and do not draw any random -->\n<!-- samples. The trees are grown successively, using a “slow” learning -->\n<!-- approach: each new tree is fit to the signal that is left over from -->\n<!-- the earlier trees, and shrunken down before it is used. -->\n\n\n## Other Topics\n\n<!-- - Boosting -->\n- AdaBoost (Adaptive Boosting) `gbm(y ~ ., distribution = \"adaboost\")`\n\n- Gradient Boosting/Extreme Gradient Boosting (XGBoost) `xgboost` \n  + <http://uc-r.github.io/gbm_regression>\n  + <https://xgboost.readthedocs.io/en/stable/R-package/xgboostPresentation.html>\n  + <https://github.com/dmlc/xgboost>\n  + <https://cran.r-project.org/web/packages/xgboost/xgboost.pdf>\n\n- Bayesian Additive Regression Trees (BART) (ISL Sec. 8.2.4)\n\n\n\n:::notes\nhttps://jamleecute.web.app/gradient-boosting-machines-gbm/\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}