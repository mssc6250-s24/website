{
  "hash": "6a9b02357a67a988db70e3239a957c8c",
  "result": {
    "markdown": "---\ntitle: \"Regression Diagnostics: Unusual Data üë©‚Äçüíª\"\nsubtitle: \"MATH 4780 / MSSC 5780 Regression Analysis\"\nauthor: \"Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University\"\n# date: \"August 02 2023\"\n# macros: _macros.tex # import a list of TeX/LaTeX definitions\nformat: \n  revealjs:\n    # code-line-numbers: false\n    #     - \"macros.tex\"\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n    # include-in-header:\n    highlight-style: arrow\n    code-block-bg: true\n    self-contained: false\n    slide-number: c/t    \n    incremental: false\n    width: 1800\n    height: 1000\n    margin: 0.05\n    logo: \"https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg\"\n    footer: \"[math4780-f23.github.io/website](https://math4780-f23.github.io/website/)\"\n    theme: [\"simple\", \"slides.scss\"]\n    multiplex: true\n    code-link: true\n    fig-cap-location: bottom\n    fig-align: center\n    transition: none ## fade slide convex concave zoom\n    title-slide-attributes:\n      data-background-color: \"#447099\"\n      # data-background-image: images/paper-texture.jpg\n      # data-background-size: cover\n      # data-background-color: \"#698ED5\"\neditor: source\nexecute:\n  freeze: true\n  echo: false\n  purl: true\n---\n\n\n#  {visibility=\"hidden\"}\n\n\\def\\bx{\\mathbf{x}}\n\\def\\bg{\\mathbf{g}}\n\\def\\bw{\\mathbf{w}}\n\\def\\balpha{\\boldsymbol \\alpha}\n\\def\\bbeta{\\boldsymbol \\beta}\n\\def\\beps{\\boldsymbol \\epsilon}\n\\def\\bX{\\mathbf{X}}\n\\def\\by{\\mathbf{y}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bW{\\mathbf{W}}\n\\def\\T{\\text{T}}\n\\def\\cov{\\mathrm{Cov}}\n\\def\\cor{\\mathrm{Corr}}\n\\def\\var{\\mathrm{Var}}\n\\def\\E{\\mathrm{E}}\n\\def\\bmu{\\boldsymbol \\mu}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\def\\Trace{\\text{Trace}}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n# Unusual Data\n<h2> Outliers </h2>\n<h2> Leverage </h2>\n<h2> Influence </h2>\n\n\n\n::: notes\n- We already learned SLR and MLR as well as the inference and prediction methods for the parameters or the variables we are interested.\n- And I promise, the most mathematical part has been passed. From now on, you should have more fun studying regression. Alright.\n- Remember our model are built upon some assumptions. Before talking about how we deal with violation of model assumptions, I'd like to spend some time talking about the unusual data.\n<!-- - However, all the methods we learned are based on the model assumptions. If any one of the model assumptions are violated, the inference or prediction results become unreliable. -->\n<!-- In fact these assumptions are quite restricted. -->\n<!-- - So it's not that uncommon to see violation of assumptions. -->\n- The quality of data affects the quality of analysis results. If our data contains some data points that are so different from other points, our inference or prediction results may be highly distorted by those unusual data. (Can be seen as violation of normality assumption)\n- So when doing data analysis, we need to pay attention to those unusual points if they exist, and understand how they affect our analysis result.\n- The showing up of the unusual data may be a signal that our regression model fails to capture some important characteristics of the data, indicating bad model fitting, or we need some other predcitor to capture that characteristic.\n<!-- - for the next cowe are going to learn how to check whether those assumptions are valid or satisfied.  -->\n<!-- - And probably the following two weeks, if the assumptions are not satisfied, how do we deal with it. -->\n\n:::\n\n\n## Outliers: Unusual $y$\n- An **outlier** is a case whose *response value is unusual given the value of the regressors*.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/07-diag-unusual/unnamed-chunk-3-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n::: notes\n\n- Neither the x nor y value of the outlier is individually unusual, as it is clear from the marginal distribution of each variable.\n\n:::\n\n\n## Leverage: Unusual $x$\n- Points that are away from the center of the data cloud of ${\\bf x}$ are called **leverage points**.\n\n:::: {.columns}\n\n::: {.column width=\"25%\"}\n::: small\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```{.my_class800}\n   time cases distance\n1  16.7     7      560\n2  11.5     3      220\n3  12.0     3      340\n4  14.9     4       80\n5  13.8     6      150\n6  18.1     7      330\n7   8.0     2      110\n8  17.8     7      210\n9  79.2    30     1460\n10 21.5     5      605\n11 40.3    16      688\n12 21.0    10      215\n13 13.5     4      255\n14 19.8     6      462\n15 24.0     9      448\n16 29.0    10      776\n17 15.3     6      200\n18 19.0     7      132\n19  9.5     3       36\n20 35.1    17      770\n21 17.9    10      140\n22 52.3    26      810\n23 18.8     9      450\n24 19.8     8      635\n25 10.8     4      150\n```\n:::\n:::\n\n:::\n:::\n\n\n\n::: {.column width=\"75%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/07-diag-unusual/unnamed-chunk-5-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n:::midi\nObservation 9 and 22 are away from the center of ${\\bf x}$ space, and are likely leverage points.   \n:::\n:::\n::::\n\n\n::: notes\n- leverage certainly will have a dramatic effect on the model summary statistics such as $R^2$ and the standard errors of the regression coefficients\n:::\n\n\n## Influence: Unusual $x$ and $y$\n\n- An **influential point** has great influence on estimated regression coefficients.\n$$\\text{Influence} = \\text{Leverage} \\times \\text{Outlyingness}$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/07-diag-unusual/unnamed-chunk-6-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n::: notes\n- What kind of data points have such great influence power?\n- Well if a point is an outlier with high leverage, then it becomes a influential point.\n- We got to be very careful about influential points because the estimated regression coefficients change a lot with and without these points.\n- Our analysis conclusion may be completely opposite => explain the figure.\n:::\n\n\n## Unusual Data: Outliers, Leverage and Influence\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/07-diag-unusual/unnamed-chunk-7-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n## Measuring Leverage: Hat Values\n- ${\\bf H} = {\\bf X(X'X)}^{-1} {\\bf X}'$. \n\n- The **hat value** $h_{ii}$ is the $i$th diagonal element of $\\bf H$.\n\n- $h_{ii} \\in \\left[\\frac{1}{n}, 1\\right]$ is a standardized measure of the **distance of the $i$th case from the centroid of the $\\bf x$ space**, taking into account the correlational structure of the $x$s.\n\n- The *larger* the $h_{ii}$ is, the *farther* the point ${\\bf x}_i$ lies to the centroid of the $\\bf x$ space.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/07-diag-unusual/unnamed-chunk-8-1.png){fig-align='center' width=50%}\n:::\n:::\n\n\n\n::: notes\n- The contours show the same or constant leverage in the 2-dim data ellipses.\n- The two black points are equally unusual and have equally large hat values.\n- example: one point at inner contour, the other outer contour\n:::\n\n\n## Measuring Leverage: Hat Values\n- In simple linear regression, \n$$h_{ii} = \\frac{1}{n} + \\frac{\\left(x_i - \\bar{x}\\right)^2}{S_{xx}}$$\n\n- $\\hat{y}_j = b_0 + b_1 ~x_j = h_{j1}y_1 + h_{j2}y_2 + \\cdots + h_{jn}y_n$\n\n- $h_{ii} = \\sum_{j=1}^nh_{ji}^2$ summarizes the influence (the *leverage*) of $y_i$ on *all* the fitted values $\\hat{y}_j$, $j = 1, \\dots, n$.\n\n- $\\bar{h} = \\frac{\\sum_{i=1}^n h_{ii}}{n} = p/n$\n\n::: alert\nA point with $h_{ii} > 2\\bar{h}$ is considered a **leverage point**.\n:::\n\n\n\n::: notes\n- $\\hat{y}_i = b_0 + b_1 ~x_i = h_{i1}y_1 + h_{i2}y_2 + \\cdots + h_{in}y_n$\n- $h_{ii} = \\sum_{j=1}^nh_{ji}^2$ summarizes the potential influence (the *leverage*) of $y_i$ on *all* the fitted values $\\hat{y}_j$, $j = 1, \\dots, n$.\n- $2\\bar{h}$ may be greater than 1, which is not applicable.\n- A point with $h_{ii} > 2\\bar{h}$ is remote enough to be considered a **leverage point**.\n- 2p/n corresponds to 5% of the data when x is multivariate normal and n and p are large.\n- Use 3p/n when n small.\n:::\n\n## [R Lab]{.pink} Delivery Time Leverage Points\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhat_i <- hatvalues(delivery_lm)\nsort(hat_i, decreasing = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    9    22    10    16    21    24    12     1    20     3    19    18    11 \n0.498 0.392 0.196 0.166 0.165 0.121 0.114 0.102 0.102 0.099 0.096 0.096 0.086 \n    4     7    14     5     2    25     8    13    17     6    23    15 \n0.085 0.082 0.078 0.075 0.071 0.067 0.064 0.061 0.059 0.043 0.041 0.041 \n```\n:::\n:::\n\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np <- 3\nn <- dim(delivery_data)[1]  ## n = 25\n2 * p/n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.24\n```\n:::\n\n```{.r .cell-code}\nhat_i[hat_i > 2 * p/n]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   9   22 \n0.50 0.39 \n```\n:::\n:::\n\n\n- Observation 9 and 22 are identified as high-leverage points.\n:::\n\n\n\n\n\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/07-diag-unusual/unnamed-chunk-11-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n::::\n\n\n\n::: notes\n\n- Most cutoff works better for large sample data\n- problem-specific unit change may be large after removing the influential point, even though the measures are not greater than the cutoff.\n:::\n\n\n## Detecting Outliers\n- Measure the *conditional* unusualness of $y$ given ${\\bf x}$.\n\n- ‚ùå Can't just use residual $e_i$ to measure the unusualness of $y_i$! \n- If the point is also a leverage point (large $h_{ii}$), it's residual tends to be small.\n\n::: notes\n- Can't just use residual $e_i$ to measure the unusualness of $y_i$! If the point is also a leverage point (large $h_{ii}$), it's residual tends to be small. -> Check plot: these points force the regression line or surface to be close to them.\n:::\n\n. . .\n\n- **R-student residual**: $$t_i = \\frac{e_i}{\\sqrt{s^2_{(i)}(1-h_{ii})}}$$ where $s^2_{(i)}$ estimates $\\sigma^2$ based on the data with the $i$th point removed.\n\n::: alert\n- $t_i \\sim t_{n-p-1}$ and a formal testing procedure can be used for detecting outliers.\n- A point with $|t_i| > 2$ needs to be examined with care.\n:::\n\n::: notes\n- (Eq (4.12) in LRA)\n- There are lots variants of residuals that are for several different purposes.\n- The so-called R-student residual is recommended for detecting outliers.\n- Bonferroni correction is better for testing n residuals simultaneously.\n- A point with $|t_i| > 2$ needs to be examined with care ( $5\\%$ of the data).\n:::\n\n\n## [R Lab]{.pink} Detecting Outliers - Residual Plot\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nr_student <- rstudent(delivery_lm)\nround(sort(r_student, decreasing = TRUE), 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    9     4    18    10    11    19     8     2    14    13     7    15    17 \n 4.31  1.64  1.12  0.81  0.71  0.57  0.36  0.36  0.33  0.32  0.26  0.21  0.13 \n    3    25     6     5    12    16    21    23    22    24     1    20 \n-0.02 -0.07 -0.09 -0.14 -0.19 -0.22 -0.87 -1.48 -1.49 -1.54 -1.70 -2.00 \n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/07-diag-unusual/unnamed-chunk-13-1.png){fig-align='center' width=56%}\n:::\n:::\n\n\n\n\n## Measuring Influence\n- The influence measures are those that measure the effect of deleting the $i$th observation.\n  - <span style=\"color:blue\"> $DFBETAS_{j, i}$ </span> measures the effect on coefficient ${b_j}$ when the $i$th observation is removed.\n  - <span style=\"color:blue\"> Cook's Distance $D_i$ </span> measures the effect of the $i$th observation on coefficient vector ${\\bf b}$.\n  - <span style=\"color:blue\"> $DFFITS_{i}$ </span> measures the effect on fitted value $\\hat{y}_i$.\n  - <span style=\"color:blue\"> $COVRATIO_{i}$ </span> measures the effect on the precision of estimates (variance).\n\n\n\n::: notes\n- Now we are going to learn some measures of influence.\n- With all these measures, we can quantify how large impact a point can make on the model coefficients or model fitting in general.\n- And the idea is to check the difference between model coefficients or model fitting results when the point is in the data and those when the point is deleted.\n:::\n\n\n## $DFBETAS_{j, i}$\n- $DFBETAS_{j, i}$ measures *__how much the regression coefficient $b_j$ changes in standard error units if the $i$th observation is removed.__*\n$$DFBETAS_{j, i} = \\frac{b_j - b_{j(i)}}{\\sqrt{s_{(i)}^2C_{jj}}} = \\frac{b_j - b_{j(i)}}{se_{(i)}(b_j)}$$\nwhere \n- $b_{j(i)}$ is the $j$th coefficient estimate computed without the $i$th observation.\n- $C_{jj}$ is the diagonal element of $( {\\bf X}'{\\bf X})^{-1}$ corresponding to $b_j$.\n\n::: alert\n- The $i$th point is considered influential on $j$th coefficient if $|DFBETAS_{j, i}| > 2/\\sqrt{n}$.\n- One issue: there are $np$ DFBETAS measures.\n:::\n\n\n\n::: notes\n- DF means difference; S means standardized.\n$$DFBETAS_{j, i} := \\frac{b_j - b_{j(i)}}{\\sqrt{S_{(i)}^2C_{jj}}} = \\frac{r_{j, i}}{\\sqrt{{\\bf r}_j'{\\bf r}_j}}{\\frac{1}{\\sqrt{1-h_{ii}}}}t_i$$\n- The denominator provides a standardization since it estimates the standard error of $b_j$.\n<!-- - $DFBETAS_{j, i}$ represents *the number of estimated standard errors that the coefficient $b_j$ changes if the $i$th point is removed.* -->\n- $DFBETAS_{j, i}$ represents the combination of *leverage measures* and the *impact of errors in the $y$ direction*.\n- The $n$ elements in ${\\bf r}_j'$ produce the *leverage* that the $n$ observations have on $b_j$.\n- $\\frac{r_{j, i}}{\\sqrt{{\\bf r}_j'{\\bf r}_j}}$ is a *normalized* measure of the impact of the $i$th observation on the $j$th coefficient.\n- The measure is swelled by the leverage score $h_{ii}$.\n- Point $i$ is considered influential on $j$th coefficient if $|DFBETAS_{j, i}| > 2/\\sqrt{n}$.\n  + $b_{j(i)}$: the $j$th coefficient computed without the $i$th observation\n  + $S_{(i)}^2$: the estimate of $\\sigma^2$ based on the data with no the $i$th point (Eq. (4.12))\n  + $C_{jj}$: the $j$th diagonal element of ${\\bf(X'X)}^{-1}$\n  + ${\\bf r}_j'$: the $j$th row of the $p \\times n$ matrix ${\\bf R = (X'X)^{-1}X'}$ $({\\bf b = Ry})$\n  + $r_{j, i}$: the $ji$th element of ${\\bf R}$\n  + $t_i$: the $i$th R-student residual\nThe n elements in the j th row of R produce the leverage that the n observations in the sample have on ÀÜŒ≤j\n:::\n\n\n## [R Lab]{.pink} $DFBETAS_{j, i}$\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n:::midi\n\n::: {.cell layout-align=\"center\" highlight-output='[10,23]'}\n\n```{.r .cell-code}\nround(dfbeta <- dfbetas(delivery_lm), 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.my_class800}\n   (Intercept) cases distance\n1        -0.19  0.41    -0.43\n2         0.09 -0.05     0.01\n3         0.00  0.00     0.00\n4         0.45  0.09    -0.27\n5        -0.03 -0.01     0.02\n6        -0.01  0.00     0.00\n7         0.08 -0.02    -0.01\n8         0.07  0.03    -0.05\n9        -2.58  0.93     1.51\n10        0.11 -0.34     0.34\n11       -0.03  0.09     0.00\n12       -0.03 -0.05     0.05\n13        0.07 -0.04     0.01\n14        0.05 -0.07     0.06\n15        0.02  0.00     0.01\n16        0.00  0.06    -0.08\n17        0.03  0.01    -0.02\n18        0.25  0.19    -0.27\n19        0.17  0.02    -0.10\n20        0.17 -0.21    -0.09\n21       -0.16 -0.30     0.34\n22        0.40 -1.03     0.57\n23       -0.16  0.04    -0.05\n24       -0.12  0.40    -0.47\n25       -0.02  0.00     0.01\n```\n:::\n:::\n\n:::\n:::\n\n\n\n\n::: {.column width=\"50%\"}\n\n::: midi\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## cut-off = 0.4\napply(dfbeta, 2, \n      function(x) x[abs(x) >  2 / sqrt(n)])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$`(Intercept)`\n    4     9 \n 0.45 -2.58 \n\n$cases\n    1     9    22    24 \n 0.41  0.93 -1.03  0.40 \n\n$distance\n    1     9    22    24 \n-0.43  1.51  0.57 -0.47 \n```\n:::\n:::\n\n:::\n\n- Point 9 is influential, as its deletion results in a displacement of every coefficient by at least 0.9 standard deviation of $b_j$.\n\n\n:::\n\n\n::::\n\n## Cook's Distance $D_i$\n\n- $D_i$ measures *__the squared distance that the vector of fitted values moves when the $i$th observation is deleted.__*\n\n$$D_i = \\frac{(\\hat{{\\bf y}}_{(i)} - \\hat{{\\bf y}})'(\\hat{{\\bf y}}_{(i)}-\\hat{{\\bf y}})}{p s^2} = \\frac{r_i^2}{p}\\frac{h_{ii}}{1-h_{ii}}, \\quad i = 1, 2, \\dots, n$$ where $r_i = \\frac{e_i}{\\sqrt{s^2(1-h_{ii})}}$ is the **Studentized residuals**.\n\n- What contributes to $D_i$:\n  + How well the model fits the $i$th observation (larger $r_i^2$ for poorer fit)\n  + How far the point is away from the remaining data (larger $h_{ii}$ for higher leverage)\n  + $\\text{Influence} = \\text{Leverage} \\times \\text{Outlyingness}$\n  \n::: alert\nConsider points with $D_{i} > 1$ to be influential.\n:::\n\n\n\n::: notes\n- $D_i > 1$ may risk missing unusual data. Use $4/(n-p)$ instead.\n$$\\begin{align} D_i& := \\frac{\\left( {\\bf b}_{(i)} - {\\bf b} \\right)' {\\bf X'X} \\left( {\\bf b}_{(i)} - {\\bf b} \\right)}{pMS_{res}}\\\\ &=\\frac{r_i^2}{p}\\frac{\\var(\\hat{y}_i)}{\\var(\\hat{e}_i)} =\\frac{r_i^2}{p}\\frac{h_{ii}}{1-h_{ii}}, \\quad i = 1, 2, \\dots, n \\end{align}$$\n- View $D_i$ as the standardized distance between $b$ and $b_{-i}$.\n- $D_i > 0$ since $X'X$ is positive definite.\n- The magnitude of $D_i$ is usually assessed by comparing it to $F_{\\alpha, p, n-p}$.\n- If $D_i = F_{0.5, p , n ‚àí p}$, then deleting point i would move $b_{-i}$ to the boundary of an approximate 50% confidence region for $\\beta$ based on the complete data set.\n- $\\frac{h_{ii}}{1-h_{ii}}$ This ratio can be shown to be the distance from the vector $x_i$ to the centroid of the remaining data.\n- It is the squared Euclidean distance (apart from pMS_Res) that the vector of fitted values moves when the $i$th observation is deleted.\n- D is a scale-invariant measure of the influence of the ith case on all of the regression coeffcients, that is the distance between b and b(-i).\n:::\n\n\n\n## [R Lab]{.pink} Cook's Distance $D_i$\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nDii <- cooks.distance(delivery_lm)\nDii[Dii > 1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  9 \n3.4 \n```\n:::\n\n```{.r .cell-code}\nround(sort(Dii, decreasing = TRUE), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    9    22    20    24     1     4    10    21    18    23    11    19     2 \n3.419 0.451 0.132 0.102 0.100 0.078 0.054 0.051 0.044 0.030 0.016 0.012 0.003 \n   14    16     8    13     7    12    15     5    17     6    25     3 \n0.003 0.003 0.003 0.002 0.002 0.002 0.001 0.001 0.000 0.000 0.000 0.000 \n```\n:::\n:::\n\n\n- Observation 9 is influential using the cutoff of one, and the point 22 is not.\n<!-- - These conclusions agree with those by examining $h_{ii}$ and $r_i$ separately. -->\n\n::: alert\n$D_i > 1$ may risk missing unusual data. Use $4/(n-p)$ instead.\n:::\n\n::: notes\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nround(pf(Dii, df1 = 3, 22), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    1     2     3     4     5     6     7     8     9    10    11    12    13 \n0.041 0.000 0.000 0.029 0.000 0.000 0.000 0.000 0.965 0.017 0.003 0.000 0.000 \n   14    15    16    17    18    19    20    21    22    23    24    25 \n0.000 0.000 0.000 0.000 0.013 0.002 0.060 0.016 0.281 0.007 0.042 0.000 \n```\n:::\n\n```{.r .cell-code}\npf(3.41835, 3, 22)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.96\n```\n:::\n\n```{.r .cell-code}\npf(0.45106, 3, 22)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.28\n```\n:::\n:::\n\nThe magnitude of D i is usually assessed by comparing it to F Œ± , p , n ‚àí p . If D i = F 0.5, p , n ‚àí p , then deleting point i would move ÀÜb(i) to the boundary of an approximate 50% confidence region for Œ≤ based on the complete data set.\n:::\n\n\n## $DFFITS_{i}$\n- $DFFITS_{i}$ measures the *__influence of the $i$th observation on the $i$th fitted value__, again in standard deviation units.*\n$$DFFITS_{i} := \\frac{\\hat{y}_i - \\hat{y}_{(i)}}{\\sqrt{s_{(i)}^2h_{ii}}} = \\left(\\frac{h_{ii}}{1-h_{ii}}\\right)^{1/2}t_i$$ where $\\hat{y}_{(i)}$ is the fitted value of $y_i$ computed without the $i$th observation.\n- The denominator provides a standardization since $\\var\\left( \\hat{y}_i\\right) = \\sigma^2h_{ii}$.\n<!-- - $DFFITS_{i}$ represents *the number of estimated standard errors that the fitted value $\\hat{y}_{(i)}$ changes if the $i$th point is removed.* -->\n- $DFFITS_{i}$is essentially the R-student residual *scaled* by the leverage $[h_{ii}/(1-h_{ii})]^{1/2}$.\n\n::: alert\nA point with $|DFFITS_{i}| > 2\\sqrt{p/n}$ needs attention.\n:::\n\n\n::: notes\n- Chatterjee and Hadi (1988): cutoff $2\\sqrt{p/(n-p)}$\n:::\n\n\n\n## [R Lab]{.pink} $DFFITS_{i}$\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nround(dffit <- dffits(delivery_lm), 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    1     2     3     4     5     6     7     8     9    10    11    12    13 \n-0.57  0.10 -0.01  0.50 -0.04 -0.02  0.08  0.09  4.30  0.40  0.22 -0.07  0.08 \n   14    15    16    17    18    19    20    21    22    23    24    25 \n 0.10  0.04 -0.10  0.03  0.37  0.19 -0.67 -0.39 -1.20 -0.31 -0.57 -0.02 \n```\n:::\n\n```{.r .cell-code}\n## cut-off = 0.69\ndffit[abs(dffit) > 2 * sqrt(p/n)]  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   9   22 \n 4.3 -1.2 \n```\n:::\n:::\n\n\n- Deleting point 9 displaces the predicted response $\\hat{y}_{(i)}$ by over four standard deviations of $\\hat{y}_i.$\n\n\n## $COVRATIO_{i}$ \n- $DFFITS_{i}$ and $DFBETAS_{j, i}$ reflect influence on $\\hat{y}_i$ and $b_j$, but do not indicate whether or not the presence of the $i$th point appreciably sharpened the estimation of the coefficient.\n<!-- - They provide no focus on whether or not the presence of the $i$th point appreciably sharpened the estimation of the coefficient. -->\n\n- A scalar measure of precision, called **generalized variance** of ${\\bf b}$ is $$GV({\\bf b}) = \\det\\left( \\cov({\\bf b}) \\right) = \\det\\left( \\sigma^2 ({\\bf X'X})^{-1} \\right) $$\n\n- To express the role of the $i$th observation on the **precision of estimation**, we use $$COVRATIO_{i} = \\frac{ \\det \\left( \\left( {\\bf X}_{(i)}' {\\bf X}_{(i)} \\right)^{-1} s_{(i)}^2 \\right) }{\\det \\left(  \\left( {\\bf X'X} \\right)^{-1} s^2\\right)} = \\frac{ \\left( s_{(i)}^2 \\right)^p}{\\left( s^2 \\right) ^ p}\\left( \\frac{1}{1-h_{ii}}\\right)$$\n  + ${\\bf X}_{(i)}$ denotes the $(n-1) \\times p$ data matrix with the $i$th observation eliminated.\n\n\n## $COVRATIO_{i}$ \n\n$$\\small COVRATIO_{i} = \\frac{ \\det \\left( \\left( {\\bf X}_{(i)}' {\\bf X}_{(i)} \\right)^{-1} s_{(i)}^2 \\right) }{\\det \\left(  \\left( {\\bf X'X} \\right)^{-1} s^2 \\right)} = \\frac{ \\left( s_{(i)}^2 \\right)^p}{\\left( s^2 \\right) ^ p}\\left( \\frac{1}{1-h_{ii}}\\right)$$\n\n- If $COVRATIO_{i} > 1$, the $i$th case *improves* the precision.\n\n- If $COVRATIO_{i} < 1$, the $i$th case *degrades* the precision.\n\n- Higher leverage $h_{ii}$ leads to larger $COVRATIO_{i}$ and improves the precision unless the point is an outlier in $y$ space.\n\n- If $i$th point is an outlier, $\\frac{s_{(i)}^2}{s^2} < 1$.\n\n::: alert\nCutoffs: \n\n- $COVRATIO_{i} > 1 + 3p/n$ or \n- $COVRATIO_{i} < 1 - 3p/n$ provided that $n > 3p$.\n:::\n\n\n::: notes\n- $\\frac{1}{1-h_{ii}}$ is the ratio of $\\small \\det \\left( \\left( {\\bf X}_{(i)}' {\\bf X}_{(i)} \\right)^{-1} \\right)$ to $\\small \\det \\left( \\left( {\\bf X}' {\\bf X} \\right)^{-1} \\right)$\n:::\n\n\n\n## [R Lab]{.pink} $COVRATIO_{i}$\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(covra <- covratio(delivery_lm))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n0.87 1.21 1.28 0.88 1.24 1.20 1.24 1.21 0.34 1.31 1.17 1.29 1.21 1.23 1.19 1.37 \n  17   18   19   20   21   22   23   24   25 \n1.22 1.07 1.22 0.76 1.24 1.40 0.89 0.95 1.23 \n```\n:::\n\n```{.r .cell-code}\n## cutoff 1.36\ncovra[covra > (1 + 3*p/n)]  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 16  22 \n1.4 1.4 \n```\n:::\n\n```{.r .cell-code}\n## cutoff 0.64\ncovra[covra < (1 - 3*p/n)] \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   9 \n0.34 \n```\n:::\n:::\n\n\n- Since $COVRATIO_{9} < 1$, this observation degrades precision of estimation.\n\n- Since $COVRATIO_{22} > 1$, this observation tends to improve precision of estimation.\n\n- Point 22 and 16 barely exceed the cutoff, so their influence is small.\n<!-- - Point 9 is much more clearly influential. -->\n\n\n## Jointly Influential Points\n- Subset of cases can be *jointly influential* or can offset each other's influence.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/07-diag-unusual/unnamed-chunk-20-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n::: notes\n- Sometimes subset of cases can be jointly influential or can offset each other's influence. One single point may not be that influential, but with another point, the fitting result changes a lot. Or one single point may be influential, but with another point, the influence power disappears.\n- Often, influential subsets of cases or multiple outliers can be identified by applying single -case deletion diagnostics sequentially.\n- (a) two jointly influential cases located close to one another. Deletion of both cases has a much greater impact than deletion of only one.\n- (b) two jointly influential cases located on opposite sides of the data. Deletion of both cases has a much greater impact than deletion of only one.\n- (c) cases that offset one another: the regression with both cases deleted is nearlythe same as for the whole data set.\n:::\n\n\n\n## Detecting Joint Influence: Added-Variable Plot\n\n- $y_i= \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_kx_{ik} + \\epsilon_i$.\n\nTo create the **added-variable plot** for $x_1$,\n\n- *Step 1*: <span style=\"color:blue\"> Regress $y$ on all predictors except $x_1$ and obtain residuals </span>\n$$\\small \\begin{align}\\hat{y}_i(x_{(1)}) &= b_0^{(1)} + b_2^{(1)}x_{i2} + \\dots + b_k^{(1)}x_{ik}\\\\e_i(y \\mid x_{(1)}) &= y_i - \\hat{y}_i(x_{(1)}) \\end{align}$$\n\n. . .\n\n- *Step 2*: <span style=\"color:blue\"> Regress $x_1$ on all other predictors and obtain residuals</span>\n$$\\small \\begin{align}\\hat{x}_{i1}(x_{(1)}) &= a_0^{(1)} + a_2^{(1)}x_{i2} + \\dots + a_k^{(1)}x_{ik}\\\\e_i(x_1 \\mid x_{(1)}) &= x_{i1} - \\hat{x}_{i1}(x_{(1)}) \\end{align}$$\n\n. . .\n\n- *Step 3*: <span style=\"color:blue\"> Plot $e_i(y \\mid x_{(1)})$ vs. $e_i(x_1 \\mid x_{(1)}), \\quad i = 1, \\dots, n$</span>\n  \n\n::: notes\n- Added-Variable Plot is a quite useful plot. We can use it for many other purposes, not just detecting joint influence. We'll talk about that later.\n- For each predictor, we can create its own Added-Variable Plot, so if we have k predictors, we can have k Added-Variable Plots.\n- The idea of the plot is that we would like to see the pure effect of a predictor on the response when all other predictors are in the model.\n- So how do we get the pure effect of $x_1$?\n- Step 1: Regress y on all predictors except x1 and obtain residuals. The residuals are the variation of y that cannot be explained by x2 to xk. So if later we see the variation further goes down, that must be due to the inclusion of x1.\n- Step 2: Regress x1 on all other predictors. The residuals are the variation of x1 that cannot be explained by x2 to xk. This gives us the \n:::\n\n. . .\n\n- For any $x_j$, $e_i(y \\mid x_{(j)})$ vs. $e_i(x_j \\mid x_{(j)})$\n\n::: notes\n- The collection of of added-variable plots for x1, ..., xk convert the graph for multiple regression into a sequence of 2-dimensional plot.\n- plotting $e_i(y \\mid x_{(j)})$ vs. $e_i(x_j \\mid x_{(j)}), \\quad i = 1, \\dots, n$ allows us to examine the leverage and influence of the cases on $b_j$.\n:::\n\n\n\n## [R Lab]{.pink} Duncan's Occupational Prestige Data\n:::: {.columns}\n\n::: {.column width=\"65%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(carData)\ncarData::Duncan[sample(dim(Duncan)[1], 16), ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.my_class800}\n                   type income education prestige\nplumber              bc     44        25       29\nchemist            prof     64        86       90\ngas.stn.attendant    bc     15        29       10\nstore.manager      prof     42        44       45\nteacher            prof     48        91       73\ninsurance.agent      wc     55        71       41\nbookkeeper           wc     29        72       39\nauthor             prof     55        90       76\nelectrician          bc     47        39       53\nlawyer             prof     76        98       89\npoliceman            bc     34        47       41\nmail.carrier         wc     48        55       34\nsoda.clerk           bc     12        30        6\nmachinist            bc     36        32       57\nbanker             prof     78        82       92\nstreetcar.motorman   bc     42        26       19\n```\n:::\n:::\n\n:::\n\n\n::: {.column width=\"35%\"}\n- `type`: Type of occupation\n  + `prof` (professional and managerial)\n  + `wc` (white-collar)\n  + `bc` (blue-collar)\n  \n- `income`: Percentage who earned $3,500 or more per year\n\n- `education`: Percentage who were high school graduates\n\n- `prestige`: Percentage of respondents in a survey who rated the occupation as \"good\" or better in prestige\n:::\n::::\n\n\n::: notes\n- 1950 data\n:::\n\n\n## [R Lab]{.pink} Added-Valued Plot\n- The slope of the least square simple regression line of $e_i(y \\mid x_{(1)})$ on $e_i(x_1 \\mid x_{(1)})$ is the same as the slope $b_1$ for $x_1$ in the full multiple regression.\n\n::: midi\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nduncan_fit <- lm(prestige ~ income + education, data = Duncan)\ncar::avPlot(model = duncan_fit, variable = \"income\", id = list(method = \"mahal\", n = 3), \n            xlab = \"e(Income | Education)\", ylab = \"e(Prestige | Education)\", pch = 16)\n```\n:::\n\n:::\n\n:::: {.columns}\n\n::: {.column width=\"62%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/07-diag-unusual/unnamed-chunk-23-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n\n\n::: {.column width=\"38%\"}\n\n- Minister's income is low given its education level.\n- Railroad engineer and railroad conductor's income is high given their education level.\n<!-- - The point for railroad engineer is more or less in line with most of the points in the plot. -->\n- The points for minister and conductor appear to be working jointly to decrease the income slope.\n:::\n::::\n\n::: notes\nidentifies the 3 points with the largest Mahalanobis distances from the center of the data.\n:::\n\n\n## [R Lab]{.pink} Bubble Plot\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- Each point is plotted as a circle with area proportional to Cook's distance.\n- Horizontal lines are drawn R-Student residuals of 0 and $\\pm 2$.\n- The vertical lines at $2\\bar{h}$ and $3\\bar{h}$.\n:::\n\n\n::: {.column width=\"55%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncar::influencePlot(duncan_fit)\n```\n\n::: {.cell-output-display}\n![](./images/07-diag-unusual/unnamed-chunk-24-1.png){fig-align='center' width=100%}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n            StudRes   Hat CookD\nminister       3.13 0.173 0.566\nreporter      -2.40 0.054 0.099\nconductor     -1.70 0.195 0.224\nRR.engineer    0.81 0.269 0.081\n```\n:::\n:::\n\n:::\n::::\n\n\n## Treatment of Unusual Data\n::: question\nShould unusual data be discarded?\n:::\n\n. . .\n\n- First investigate *why* data are unusual.\n\n- Error in recording:\n  + If the typo can be corrected, correct it and keep it. \n  + If it cannot be corrected, discard it.\n\n. . .\n\n- If the unusual point is known to be correct, understand why.\n\n- Outliers or influential data may also motivate model respecification.\n  + The pattern of outlying data may suggest introducing additional regressors.\n\n\n\n\n::: notes\nIt is tempting to remove outliers. Don‚Äôt do this without a very good reason. Models that ignore exceptional (and interesting) cases often perform poorly. For instance, if a financial firm ignored the largest market swings ‚Äì the ‚Äúoutliers‚Äù ‚Äì they would soon go bankrupt by making poorly thought-out investments.\nA compromise approach (Advanced): \n- Use **robust** estimation that **downweights** observations in proportion to residual magnitude or influence.\n- A highly influential observation will receive less weight than it would in a least-squares fit.\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}