{
  "hash": "9256fa54e7905dea115c5c48a5cf399a",
  "result": {
    "markdown": "---\ntitle: \"Logistic Regression ðŸ’»\"\nsubtitle: \"MATH 4780 / MSSC 5780 Regression Analysis\"\nauthor: \"Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University\"\n# date: \"October 23 2023\"\n# macros: _macros.tex # import a list of TeX/LaTeX definitions\nformat: \n  revealjs:\n    # code-line-numbers: false\n    #     - \"macros.tex\"\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n    # include-in-header:\n    highlight-style: arrow\n    code-block-bg: true\n    self-contained: false\n    slide-number: c/t    \n    incremental: false\n    width: 1800\n    height: 1000\n    margin: 0.05\n    logo: \"https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg\"\n    footer: \"[math4780-f23.github.io/website](https://math4780-f23.github.io/website/)\"\n    theme: [\"simple\", \"slides.scss\"]\n    multiplex: false\n    code-link: true\n    fig-cap-location: bottom\n    fig-align: center\n    transition: none ## fade slide convex concave zoom\n    title-slide-attributes:\n      data-background-color: \"#447099\"\n      # data-background-image: images/paper-texture.jpg\n      # data-background-size: cover\n      # data-background-color: \"#698ED5\"\neditor: source\nexecute:\n  freeze: true\n  echo: false\n  purl: true\n---\n\n\n#  {visibility=\"hidden\"}\n\n\\def\\bx{\\mathbf{x}}\n\\def\\bg{\\mathbf{g}}\n\\def\\bw{\\mathbf{w}}\n\\def\\balpha{\\boldsymbol \\alpha}\n\\def\\bbeta{\\boldsymbol \\beta}\n\\def\\beps{\\boldsymbol \\epsilon}\n\\def\\bLambda{\\boldsymbol \\Lambda}\n\\def\\bX{\\mathbf{X}}\n\\def\\by{\\mathbf{y}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bW{\\mathbf{W}}\n\\def\\T{\\text{T}}\n\\def\\cov{\\mathrm{Cov}}\n\\def\\cor{\\mathrm{Corr}}\n\\def\\var{\\mathrm{Var}}\n\\def\\E{\\mathrm{E}}\n\\def\\bmu{\\boldsymbol \\mu}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\def\\Trace{\\text{Trace}}\n\n\n\n\n\n\n\n# Classification\n\n\n\n\n## Regression vs. Classification\n- Linear regression assumes that the response $Y$ is *numerical*.\n- In many situations, $Y$ is **categorical**.\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n**Normal vs. COVID vs. Smoking**\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/16-logistic-reg/covid_lung.jpeg){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n\n::: {.column width=\"50%\"}\n**fake news vs. true news**\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/16-logistic-reg/fake_news.jpeg){fig-align='center' width=100%}\n:::\n:::\n\n:::\n::::\n\n- A process of predicting categorical response is known as **classification**.   \n\n\n## Regression Function $f(x)$ vs. Classifier $C(x)$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/16-logistic-reg/regression.png){fig-align='center' width=100%}\n:::\n:::\n\n\n. . .\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: https://daviddalpiaz.github.io/r4sl/classification-overview.html](images/16-logistic-reg/classification.png){fig-align='center' width=100%}\n:::\n:::\n\n\n## Classifiers\n- Often, we first predict the *probability* of each of the categories of $Y$, as a basis for making the classification (*soft* classifier).\n- We discuss the classifiers \n  + **logistic**\n  + *probit*\n  + *complementary log-log*\n- Other classifiers include (MSSC 6250)\n  + K-nearest neighbors\n  + trees/random forests/boosting\n  + support vector machines\n  + convolutional neural networks, etc.\n\n\n## Classification Example\n\n- Predict whether people will default on their credit card payment $(Y)$ `yes` or `no`, based on monthly credit card balance $(X)$.\n- We use the (training) sample data $\\{(x_1, y_1), \\dots, (x_n, y_n)\\}$ to build a classifier.\n\n:::: {.columns}\n\n::: {.column width=\"62%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/16-logistic-reg/unnamed-chunk-6-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: {.column width=\"38%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/16-logistic-reg/credit_card.jpeg){fig-align='center' width=100%}\n:::\n:::\n\n:::\n::::\n\n## Why Not Linear Regression?\n\n$$Y =\\begin{cases}\n    0  & \\quad \\text{if not default}\\\\\n    1  & \\quad \\text{if default}\n     \\end{cases}$$\n\n- $Y = \\beta_0 + \\beta_1X + \\epsilon$, $\\, X =$ credit card balance \n\n::: question\nWhat is the problem of this dummy variable approach?\n:::\n<!-- - Linear regression assumes that the response is -->\n<!--   + Normally distributed -->\n<!--   + Constant variance -->\n<!--   + Independent -->\n\n::: notes\n- $Y$ is categorical but coded as dummy variable or indicator variable\n- Fit linear regression and treat it as a numerical variable.\n:::\n\n## Why Not Linear Regression?\n- **Some estimates are outside $[0, 1]$.**\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/16-logistic-reg/lm-defualt-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n::: notes\n- **The dummy variable approach $(Y = 0, 1)$ cannot be easily extended to $Y$ with more than two categories.**\n:::\n\n\n\n## Why Not Linear Regression?\n- First predict the **probability** of each category of $Y$.\n- Predict probability of `default` using a <span style=\"color:blue\">**S-shaped** curve</span>.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/16-logistic-reg/glm-default-1.png){fig-align='center' width=65%}\n:::\n:::\n\n\n\n\n::: notes\nOften, we first predict the probability of each of the categories of $Y$, as a basis for making the classification.\n- The predicted probability should be like a S-shaped curve that first is in [0, 1] interval.\n- Second, the predicted probability of being defualted is increasing in the credit card balance.\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n:::\n\n\n# Logistic Regression\n<h2> Binary Response </h2>\n<h2> Binomial/Proportion Response </h2>\n<h2> Multinomial Response (MSSC 6250) </h2>\n\n\n::: notes\n- When we have several replicates of reposnse, meaning that we have several binary 0-1 outcomes of $y$ at any given level of $x$, we consider Binomial and Proportion Responses, that is the number of successes, or the proportion or the number of successes divided by the total number of trials or replicates.\n:::\n\n\n## Framing the Problem: Binary Responses\n\n- Treat each outcome (default $(y = 1)$ and not default $(y = 0)$) as success and failure arising from separate **Bernoulli** trials.\n\n::: question\nWhat is a Bernoulli trial?\n:::\n\n. . .\n\n- A Bernoulli trial is a special case of a binomial trial when the number of trials is $m = 1$:\n  - $Bernoulli(\\pi) = binomial(m = 1,\\pi)$\n  - **exactly two** possible outcomes, \"success\" and \"failure\"\n  - the probability of success $\\pi$ is **constant**\n\n\n. . .\n\n::: question\nIn the credit card example, \n\n- do we have exactly two outcomes? \n- do we have constant probability? $P(y_1 = 1) = P(y_2 = 1) = \\cdots = P(y_n = 1) = \\pi?$\n:::\n\n\n\n::: notes\n- The idea is that we can treat $Y$ as a categorical variable and each of its outcome is success or failure arising from separate Bernoulli trials\n- And what is a Bernoulli trial? We talked about this when we talked about Binomial distribution, right?\n- A Bernoulli trial is a random experiment with exactly two possible outcomes, \"success\" and \"failure\", in which the probability of success is the same every time the experiment is conducted\n:::\n\n\n## Binary Responses with Nonconstant Probability\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n- Two outcomes: default $(y = 1)$ and not default $(y = 0)$\n- The probability of success $\\pi$ *changes with* the value of predictor $X$!\n- With a different value of $x_i$, each Bernoulli trial outcome $y_i$ has a *different* probability of success $\\pi_i$:\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/16-logistic-reg/unnamed-chunk-11-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::::\n\n$y_i \\mid x_i \\stackrel{indep}{\\sim} Bernoulli(\\pi(x_i)) = binomial(m=1,\\pi = \\pi(x_i))$\n\n\n::: notes\n- Here, Each Bernoulli trial, with different value of $x_i$, the trial outcome $y_i$ can have a separate probability of success $ y_i \\mid x_i; p_i âˆ¼ Bern(p_i) $.\n- Actually this probability $\\pi_i$ is affected by the predictor $x_i$. A different value of $x$ will give you a different value of $\\pi$.\n- Like linear regression, different value of $x$ give us different mean of $Y$\n:::\n\n\n. . .\n\n\n- $X =$ `balance`. $x_1 = 2000$ has a larger $\\pi_1 = \\pi(2000)$ than $\\pi_2 = \\pi(500)$ with $x_2 = 500$ because credit cards with a higher balance tend to be default.\n\n\n## Bernoulli Variables {visibility=\"hidden\"}\n\n- $Y_i \\sim Bern(\\pi_i)$\n- $P(Y_i = 1) = \\pi_i$\n- $P(Y_i = 0) = 1 - \\pi_i$\n- $E[Y_i] = 1(\\pi_i) + 0(1-\\pi_i) = \\pi_i$\n- $\\var(Y_i) = E[(Y_i - E(Y_i))^2] = (1 - \\pi_i)^2\\pi_i + (0 - \\pi_i)^2(1 - \\pi_i) = \\pi_i(1-\\pi_i)$\n- Still, the linear regression model $y_i = \\beta_0 + \\beta_1x_{i1}+ \\cdots + \\beta_kx_{ik} + \\epsilon_i$ **does not** make sense.\n\n.alert[\n- $\\epsilon_i$ only take two values, so they are not Gaussian\n- The variance of $Y_i$ is a function of the mean $\\pi_i$ (not constant)\n- $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1+ \\cdots + \\hat{\\beta}_kx_k$ could fall outside the $(0, 1)$ range.\n]\n\n$E(y_i) = {\\bf x}_i'\\bbeta = \\pi_i$, and so \n\n\n\n## Logistic Regression\n\n- **Logistic regression** models a **binary** response $(Y)$ using predictors $X_1, \\dots, X_k$.\n  + $k = 1$: simple logistic regression\n  + $k > 1$: multiple logistic regression\n\n::: notes\n- Logistic regression models a **binary** categorical outcome $(Y)$ using numerical and categorical predictors $X_1, \\dots, X_k$.\n:::\n\n. . .\n\n::: center\nInstead of predicting $y_i$ directly, we use the predictors to model its *probability* of success, $\\pi_i$.\n:::\n\n\n::: center\nBut how?\n:::\n\n\n::: notes\n- But remember, we are not predicting $Y$ directly. Instead, our goal is to use predictors $X_1, \\dots, X_k$ to estimate the probability of success $\\pi$ of the Bernoulli variable $Y$. And if $\\pi > threshold$, say 0.5, $\\hat{Y} = 1$, if $\\pi < threshold$, $\\hat{Y} = 0$.\n:::\n\n. . .\n\n<!-- - Cannot just use a linear model for $\\pi_i \\in (0, 1)$, but can *transform* the model to have the appropriate range. -->\n<!-- - This is a very general way of addressing many problems in regression and the resulting models are called **generalized linear models (GLMs)** -->\n<!-- - **Logistic regression** is one example. -->\n<!-- - **Goal**: Use predictors $X_1, \\dots, X_k$ to estimate the probability of success $\\pi$ of the Bernoulli variable $Y$. -->\n\n- **Transform $\\pi \\in (0, 1)$ into another variable $\\eta \\in (-\\infty, \\infty)$. Then construct a linear predictor on $\\eta$:  $\\eta_i = \\beta_0 + \\beta_1x_i$**\n<!-- - To finish specifying the logistic regression model, we need to define a link function that connects $\\eta_i$ to $p_i$: -->\n\n::: notes\n- And the idea is that we transform $p \\in (0, 1)$ into another variable $\\eta \\in (-\\infty, \\infty)$. So that we can reasonably fit a linear regression on $\\eta$.\n<!-- - To finish specifying the logistic regression model, we need to define a link function that connects $\\eta_i$ to $p_i$: -->\\\n:::\n\n\n. . .\n\n- **Logit function:** For $0 < \\pi < 1$\n\n$$\\eta = logit(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)$$\n\n::: notes\n- And the function that transforms $p$ into $\\eta$ is the so called logit function defined as\n- $\\eta = logit(p) = \\ln\\left(\\frac{p}{1-p}\\right)$\n<!-- - And this is why the model is called logit model, or logistic regression. -->\n- You see when $p$ is approaching 0, p/1-p is also approaching 0, and so log of it is approaching -$\\infty$.\n- When p is close to 1, p/1-p is close to $\\infty$, so is log because log is an increasing function.\n:::\n\n\n\n## Logit function $\\eta = logit(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/16-logistic-reg/unnamed-chunk-12-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\n\n\n::: notes\n- Here visualizes the logit function.\n- It's an one-to-one increasing function of $\\pi$ and so the inference on $\\eta$ can be transformed back to the inference of $\\pi$ with no problems.\n:::\n\n\n## Logistic Function\n\n- The *logit* function $\\eta = logit(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)$ takes a value $\\pi \\in (0, 1)$ and maps it to a value $\\eta \\in (-\\infty, \\infty)$.\n- **Logistic function**:\n$$\\pi = logistic(\\eta) = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)} = \\frac{1}{1+\\exp(-\\eta)} \\in (0, 1)$$\n- The *logistic* function takes a value $\\eta \\in (-\\infty, \\infty)$ and maps it to a value $\\pi \\in (0, 1)$.\n\n. . .\n\n- So once $\\eta$ is estimated by the linear regression, we use the logistic function to transform $\\eta$ back to the probability.\n\n::: notes\n- The logit function $\\eta = logit(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)$ takes a value $\\pi \\in (0, 1)$ and maps it to a value $\\eta \\in (-\\infty, \\infty)$.\n- **Inverse logit (logistic) function**:\n$$\\pi = logistic(\\eta) = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)} = \\frac{1}{1+\\exp(-\\eta)} \\in (0, 1)$$\n- The inverse logit function takes a value $\\eta$ between $-\\infty$ and $\\infty$ and maps it to a value $\\pi$ between 0 and 1.\n:::\n\n\n## Logistic Function $\\pi = logistic(\\eta) = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)}$\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/16-logistic-reg/unnamed-chunk-13-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\n\n::: notes\n- We are almost there. The value of logistic function is what we need for predicting the probability that $Y = 1$. \n- Given any value of $\\eta$, there is a corresponding estimated probability. \n- So if we can use our predictors to get eta first, then we can use the eta to predict the probability of $Y$ being equal to when the predictors are at the level for getting $\\eta$. \n:::\n\n## Simple Logistic Regression Model\n\nFor $i = 1, \\dots, n$ and with one predictor $X$:\n  $$(Y_i \\mid X = x_i) \\stackrel{indep}{\\sim} Bernoulli(\\pi(x_i))$$\n  $$\\text{logit}(\\pi_i) = \\ln \\left( \\frac{\\pi(x_i)}{1 - \\pi(x_i)} \\right) = \\eta_i = \\beta_0+\\beta_1 x_{i}$$\n<!-- - The $\\text{logit}(\\pi_i)$ is a **link function** that *links* the linear predictor and the mean of $Y_i$. -->\n\n\n::: notes\n- with sample size $n$ and with $k$ predictors, we have the logistic regression model like this\n- First, we have a probability distribution **Bernoulli** describing how the outcome or response data are generated. \n  - $Y_i \\mid {\\bf x}_i; \\pi_i \\sim \\text{Bern}(p_i)$, ${\\bf x}_i = (x_{1,i}, \\cdots, x_{k,i})$, $i = 1, \\dots, n$\n- Then we have a link function, logit function, that relates the linear regression to the parameter of the outcome distribution, which is the parameter $p$, the probability of success in the Bernoulli distribution.\n  - $\\text{logit}(\\pi_i) = \\eta_i = \\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i}$\n\n:::\n\n\n. . .\n\n<!-- $$\\small \\pi_i = \\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)} = \\frac{\\exp(\\beta_0+\\beta_1 x_{i})}{1+\\exp(\\beta_0+\\beta_1 x_{i})}$$ -->\n\nOnce we get the estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$,\n$$\\small \\hat{\\pi}_i = \\frac{\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_{i} )}{1+\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_{i})} = \\frac{1}{1+\\exp(-\\hat{\\beta}_0-\\hat{\\beta}_1 x_{i}))}$$\n\n\n::: notes\n.alert[\nIn general, if $E(Y_i) = \\mu_i$, $g(\\mu_i) = \\eta_i = {\\bf x}_i'\\bbeta$, $\\mu_i= g^{-1}(\\eta_i) = g^{-1}({\\bf x}_i'\\bbeta)$. Here $\\mu_i = \\pi_i$, $g(\\cdot) = \\text{logit}(\\cdot)$.\n]\n-  From which we get the probability of success derived by the logistic function\n$$E(y_i) = \\pi_i = \\frac{\\exp(\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i})}{1+\\exp(\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i})}$$\nor the predicted probability is the one that replaces $\\beta$ with estimates $b$.\n$$\\hat{\\pi}_i = \\frac{\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_{1,i} + \\cdots + \\hat{\\beta}_k x_{k,i})}{1+\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_{1,i} + \\cdots + \\hat{\\beta}_k x_{k,i})}$$\n:::\n\n\n## [R Lab]{.pink} Credit Card Default\n:::: {.columns}\n::: midi\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nlibrary(ISLR2)\nhead(Default, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   default student balance income\n1       No      No     730  44362\n2       No     Yes     817  12106\n3       No      No    1074  31767\n4       No      No     529  35704\n5       No      No     786  38463\n6       No     Yes     920   7492\n7       No      No     826  24905\n8       No     Yes     809  17600\n9       No      No    1161  37469\n10      No      No       0  29275\n```\n:::\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nstr(Default)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t10000 obs. of  4 variables:\n $ default: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ student: Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 1 1 2 1 2 1 1 ...\n $ balance: num  730 817 1074 529 786 ...\n $ income : num  44362 12106 31767 35704 38463 ...\n```\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\ntable(Default$default)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  No  Yes \n9667  333 \n```\n:::\n\n```{.r .cell-code  code-line-numbers=\"false\"}\ntable(Default$student)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  No  Yes \n7056 2944 \n```\n:::\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nsummary(Default$balance)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0     482     824     835    1166    2654 \n```\n:::\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nsummary(Default$income)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    772   21340   34553   33517   43808   73554 \n```\n:::\n:::\n\n:::\n:::\n::::\n\n\n\n## [R Lab]{.pink} Simple Logistic Regression\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nlibrary(tidyverse)\nDefault |>  \n    group_by(default) |>\n    summarise(avg_balance = mean(balance))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 2\n  default avg_balance\n  <fct>         <dbl>\n1 No             804.\n2 Yes           1748.\n```\n:::\n:::\n\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nlogit_fit <- glm(default ~ balance, data = Default, family = binomial)\nsumm_logit_fit <- summary(logit_fit)\nsumm_logit_fit$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Estimate Std. Error z value  Pr(>|z|)\n(Intercept) -10.6513    0.36116   -29.5 3.62e-191\nbalance       0.0055    0.00022    25.0 1.98e-137\n```\n:::\n:::\n\n\n- $\\hat{\\eta} = \\text{logit}(\\hat{\\pi}) = \\ln \\left( \\frac{\\hat{\\pi}}{1 - \\hat{\\pi}}\\right) = -10.651 + 0.0055 \\times \\text{balance}$\n\n\n## $\\eta$ vs. $x$\n- $\\hat{\\eta} = \\text{logit}(\\hat{\\pi}) = \\ln \\left( \\frac{\\hat{\\pi}}{1 - \\hat{\\pi}}\\right) = -10.651 + 0.0055 \\times \\text{balance}$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/16-logistic-reg/unnamed-chunk-18-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Interpretation of Coefficients\nThe ratio $\\frac{\\pi}{1-\\pi} \\in (0, \\infty)$ is called the **odds** of some event.\n\n\n- Example: If 1 in 5 people will default, the odds is 1/4 since $\\pi = 0.2$ implies an odds of $0.2/(1âˆ’0.2) = 1/4$.\n\n\n\n$$\\ln \\left( \\frac{\\pi(x)}{1 - \\pi(x)} \\right)= \\beta_0 + \\beta_1x$$\n- Increasing $x$ by one unit changes the **log-odds** by $\\beta_1$, or it multiplies the odds by $e^{\\beta_1}$.\n\n\n\n::: alert\n- $\\beta_1$ does *not* correspond to the change in $\\pi(x)$ associated with a one-unit\nincrease in $x$.\n- $\\beta_1$ is the change in **log odds** associated with one-unit increase in $x$.\n:::\n\n\n## [R Lab]{.pink} Interpretation of Coefficients\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n            Estimate Std. Error z value  Pr(>|z|)\n(Intercept) -10.6513    0.36116   -29.5 3.62e-191\nbalance       0.0055    0.00022    25.0 1.98e-137\n```\n:::\n:::\n\n\n- $\\hat{\\eta} = \\text{logit}(\\hat{\\pi}) = \\ln \\left( \\frac{\\hat{\\pi}}{1 - \\hat{\\pi}}\\right) = -10.651 + 0.0055 \\times \\text{balance}$\n\n. . .\n\n- $\\hat{\\eta}(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1x$\n- $\\hat{\\eta}(x+1) = \\hat{\\beta}_0 + \\hat{\\beta}_1(x+1)$\n- $\\hat{\\eta}(x+1) - \\hat{\\eta}(x) = \\hat{\\beta}_1 = \\ln(\\text{odds}_{x+1}) - \\ln(\\text{odds}_{x})$\n- One-unit increase in `balance` increases the *log odds* of `default` by 0.0055 units.\n\n. . .\n\n- The **odds ratio**, $\\widehat{OR} = \\frac{\\text{odds}_{x+1}}{\\text{odds}_{x}} = e^{\\hat{\\beta}_1} = e^{0.0055} = 1.005515$.\n\n- The odds of `default` increases by 0.55% with additional one unit of credit card `balance`.\n\n\n\n\n## Probability Curve\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n- The relationship between $\\pi(x)$ and $x$ is not linear!\n$$\\pi(x) = \\frac{\\exp(\\beta_0+\\beta_1 x)}{1+\\exp(\\beta_0+\\beta_1 x)}$$\n- The amount that $\\pi(x)$ changes due to a one-unit change in $x$ depends on the current value of $x$.\n- Regardless of the value of $x$, if $\\beta_1 > 0$, increasing $x$ will be increasing $\\pi(x)$.\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/16-logistic-reg/unnamed-chunk-20-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n::::\n\n\n\n::: notes\n\n::: {.cell layout-align=\"center\" hash='16-logistic-reg_cache/revealjs/unnamed-chunk-21_17fb10f49aed028b055021ef40e3ea59'}\n::: {.cell-output-display}\n![](./images/16-logistic-reg/unnamed-chunk-21-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n## Pr(default) When Balance is 2000\n\n$$\\log\\left(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\right) = -10.651+0.0055\\times 2000$$\n<!-- $$\\frac{\\hat{\\pi}}{1-\\hat{\\pi}} = \\exp(0.3487) = 1.417 \\rightarrow \\hat{\\pi} = 1.417 \\times (1 - \\hat{\\pi})$$ -->\n<!-- $$\\hat{\\pi} = 1.417 - 1.417\\hat{\\pi} \\rightarrow 2.417\\hat{\\pi} = 1.417$$ -->\n<!-- $$\\hat{\\pi} = 1.417 / 2.417 = 0.586$$ -->\n\n$$ \\hat{\\pi} = \\frac{1}{1+\\exp(-(-10.651+0.0055 \\times 2000)} = 0.586$$\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\npi_hat <- predict(logit_fit, type = \"response\")\neta_hat <- predict(logit_fit, type = \"link\")  ## default gives us b0 + b1*x\npredict(logit_fit, newdata = data.frame(balance = 2000), type = \"response\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    1 \n0.586 \n```\n:::\n:::\n\n\n## Probability Curve\n\n::: question\nWhat is the probability of default when the balance is 500? What about balance 2500?\n:::\n\n:::: {.columns}\n\n::: {.column width=\"65%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/16-logistic-reg/default-predict-viz-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: {.column width=\"35%\"}\n- [500 balance: Pr(default) = 0]{.pink}\n- [2000 balance, Pr(default) = 0.59]{.yellow}\n- [2500 balance, Pr(default) = 0.96]{.green}\n:::\n::::\n\n\n## Multiple Logistic Regression Model\n\nFor $i = 1, \\dots, n$ and with $k$ predictors:\n  $$Y_i \\mid \\pi_i({\\bf x}_i) \\stackrel{indep}{\\sim} \\text{Bernoulli}(\\pi_i), \\quad {\\bf x}_i' = (x_{i1}, \\dots, x_{ik})$$\n  $$\\text{logit}(\\pi_i) = \\ln \\left( \\frac{\\pi_i}{1 - \\pi_i} \\right) = \\eta_i = \\beta_0+\\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik} = {\\bf x}_i'\\bbeta$$\n\n- The $\\text{logit}(\\pi_i)$ is a **link function** that *links* the linear predictor and the mean of $Y_i$.\n\n\n::: notes\n- with sample size $n$ and with $k$ predictors, we have the logistic regression model like this\n- First, we have a probability distribution **Bernoulli** describing how the outcome or response data are generated. \n  - $Y_i \\mid {\\bf x}_i; \\pi_i \\sim \\text{Bern}(p_i)$, ${\\bf x}_i = (x_{1,i}, \\cdots, x_{k,i})$, $i = 1, \\dots, n$\n- Then we have a link function, logit function, that relates the linear regression to the parameter of the outcome distribution, which is the parameter $p$, the probability of success in the Bernoulli distribution.\n- $\\text{logit}(\\pi_i) = \\eta_i = \\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i}$\n  \n:::\n\n\n. . .\n\n$$\\small E(Y_i) = \\pi_i = \\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)} = \\frac{\\exp(\\beta_0+\\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik})}{1+\\exp(\\beta_0+\\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik})} = \\frac{\\exp( {\\bf x}_i'\\bbeta)}{1 + \\exp({\\bf x}_i'\\bbeta )}$$\n$$\\small \\hat{\\pi}_i = \\frac{\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_{i1} + \\cdots + \\hat{\\beta}_k x_{ik})}{1+\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_{i1} + \\cdots + \\hat{\\beta}_k x_{ik})}$$\n\n\n\n::: notes\n-  From which we get the probability of success derived by the logistic function\n$$E(y_i) = \\pi_i = \\frac{\\exp(\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i})}{1+\\exp(\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i})}$$\nor the predicted probability is the one that replaces $\\beta$ with estimates $b$.\n$$\\hat{\\pi}_i = \\frac{\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_{1,i} + \\cdots + \\hat{\\beta}_k x_{k,i})}{1+\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_{1,i} + \\cdots + \\hat{\\beta}_k x_{k,i})}$$\n\nIn general, if $E(Y_i) = \\mu_i$, $g(\\mu_i) = \\eta_i = {\\bf x}_i'\\bbeta$, $\\mu_i= g^{-1}(\\eta_i) = g^{-1}({\\bf x}_i'\\bbeta)$. Here $\\mu_i = \\pi_i$, $g(\\cdot) = \\text{logit}(\\cdot)$.\n\n:::\n\n\n## [R Lab]{.pink} Multiple Logistic Regression\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nmulti_logit_fit <- glm(default ~ balance + I(income/1000), data = Default, \n                       family = binomial)\nsumm_multi_logit_fit <- summary(multi_logit_fit)\nsumm_multi_logit_fit$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                Estimate Std. Error z value  Pr(>|z|)\n(Intercept)    -11.54047   0.434756  -26.54 2.96e-155\nbalance          0.00565   0.000227   24.84 3.64e-136\nI(income/1000)   0.02081   0.004985    4.17  2.99e-05\n```\n:::\n:::\n\n\n- $\\hat{\\eta} = \\text{logit}(\\hat{\\pi}) = \\ln \\left( \\frac{\\hat{\\pi}}{1 - \\hat{\\pi}}\\right) = -11.54 + 0.0056 \\times \\text{balance} + 0.021 \\times \\text{income}$\n\n- One with a credit card balance of $1,500 and an income of $40,000 has an estimated probability of default of $$\\hat{\\pi} = \\frac{1}{1+ \\exp(-(-11.54 + 0.0056(1500) + 0.021(40)))} = 0.091$$\n\n\n::: question\nWhy we multiply the `income` coefficient by 40, rather than 40,000?\n:::\n\n\n\n## Multiple Binary Outcomes \n- So far we consider only **one** binary outcome for each combination of predictors.\n\n- Often we have **repeated** observations or trials at each level of the regressors.\n\n- Originally, at $X = x_i$, we have one single observation $y_i = 0$ or $1$. \n\n- Now, at $X = x_i$, we have $m_i$ trials and $y_i$ of them are ones (successes).\n\n- Let $y_{i,j}$ be an indicator (Bernoulli) variable taking value $0$ or $1$ for the $j$-th trial at $x_i$.\n\n- $y_i = y_{i,1} + y_{i,2} + \\cdots + y_{i, m_i} = \\sum_{j=1}^{m_i} y_{i,j}$.\n\n\n\n## Bernoulli to binomial\n\n- Example:\n  + 4 dosages of a combination of drugs, $(10, 15, 20, 25)$\n  + 10 patients for each dosage\n  + see how dosage level affects the number of cure of some disease among patients\n  \n\n\n- $i = 1, 2, 3, 4$, $x_1 = 10, \\dots, x_4 = 25$, $n = 4$\n- $m_i = 10$ for each $i$\n- $y_i = y_{i, 1} + \\cdots + y_{i, 10}$ is the number of patients whose disease is cured at $i$-th dosage, where $y_{i, j} = 1$ if $j$-th patient is cured, $0$ otherwise.\n\n\n\n\n::: question\nWhat is our response and any distribution can be used to model that?\n:::\n\n\n::: notes\n- So far we consider only **one** binary outcome for each combination of predictors, and the response is Bernoulli distributed. \n- Often we have more than one or **repeated** observations or trials at each level of the $x$ variables.\n- Originally, at $X = x_i$, we have one single observation $y_i = 0$ or $1$. Now, at $X = x_i$, we have $m_i$ trials and $s_i$ of them are ones (successes).\n  + Regressors are dosages of a combination of drugs and a success represents a cure of a disease.\n\n:::\n\n## Binomial Responses\n\n- The responses $Y_i \\sim binomial(m_i, \\pi_i)$.\n\n- Assume $Y_1, Y_2, \\dots, Y_n$ are independent. ( $m_i$ trials, $y_{i, 1}, \\dots, y_{i, m_i}$, are independent too by the definition of a binomial experiment )\n\n\n<br>\n\n\n| Number of trials | Number of successes | Regressors |\n|:-------:|:-------:|:-------:|\n| $m_1$ | $y_1$ | $x_{11}, x_{12}, \\dots, x_{1k}$\n| $m_2$ | $y_2$ | $x_{21}, x_{22}, \\dots, x_{2k}$\n| $\\vdots$ | $\\vdots$| $\\vdots$\n| $m_n$ | $y_n$ | $x_{n1}, x_{n2}, \\dots, x_{nk}$\n\n\n::: notes\n- $Pr(Y_i = y_i) = {m_i \\choose y_i} \\pi_i^{y_i} (1-\\pi_i)^{m_i - y_i}$\n- $E[Y_i] = m_i\\pi_i$\n- $\\var[Y_i] = m_i\\pi_i(1-\\pi_i)$\n:::\n\n\n## [R Lab]{.pink} [Strength of Fastener](./data/data-prob-13-3.csv) (LRA 13.3)\n\n- The compressive strength of an alloy fastener used in aircraft construction is being studied.\n\n- Ten loads were selected over the range 2500 â€“ 4300 psi and a number of fasteners were tested at those loads.\n\n- The numbers of fasteners failing at each load were recorded. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nfastener <- read.csv(\"./data/data-prob-13-3.csv\")\ncolnames(fastener) <- c(\"load\", \"m\", \"y\")\nfastener\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   load   m  y\n1  2500  50 10\n2  2700  70 17\n3  2900 100 30\n4  3100  60 21\n5  3300  40 18\n6  3500  85 43\n7  3700  90 54\n8  3900  50 33\n9  4100  80 60\n10 4300  65 51\n```\n:::\n:::\n\n\n\n## [R Lab]{.pink} Binomial Response Logistic Regression\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nbinom_fit <- glm(cbind(y, m - y) ~ load,  #<<\n                 data = fastener, family = binomial)\nbinom_summ <- summary(binom_fit)\nbinom_summ$coef\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept) -5.33971   0.545693   -9.79 1.30e-22\nload         0.00155   0.000158    9.83 8.45e-23\n```\n:::\n:::\n\n\n$$\\hat{\\pi} = \\frac{e^{-5.34 + 0.0015x}}{1 + e^{-5.34 + 0.0015x}}$$\n\n::: notes\nThe complete test data are shown below.\n:::\n\n# Evaluation Metrics\n\n::: notes\n- OK we learn the logistic regression model, we know how to do estimation using R, and now it's time to introduce some evaluation metrics or performance measures for classification problems.\n- Given a predicted probability, we may correctly classify the label, or mis-classify the label.\n- And here we are going to talk about two metrics sensitivity and specificity.\n- It's hard for me to remember which is which, so I always open their Wiki page when I am working on them.\n- All right let's see what they are.\n:::\n\n\n\n\n## Sensitivity and Specificity\n\n\n|                        | 0               | 1            |\n|------------------------|-------------------------------|-------------------------------|\n| **Labeled 0** |  **True Negative  (TN)** | **False Negative (FN)**|\n| **Labeled 1**  |  **False Positive (FP)**|  **True Positive  (TP)**           | \n\n<br>\n\n- **Sensitivity (True Positive Rate)** $= P( \\text{Labeled 1} \\mid \\text{1}) = \\frac{TP}{TP+FN}$\n\n- **Specificity (True Negative Rate)** $= P( \\text{Labeled 0} \\mid \\text{0}) = \\frac{TN}{FP+TN}$ \n\n- **Accuracy** $= \\frac{TP + TN}{TP+FN+FP+TN}$\n<!-- - **F1 score** $= \\frac{2TP}{2TP+FP+FN}$ -->\n- More on [Wiki page](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)\n\n## [R Lab]{.pink} Confusion Matrix\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred_prob <- predict(logit_fit, type = \"response\")\ntable(pred_prob > 0.5, Default$default)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       \n          No  Yes\n  FALSE 9625  233\n  TRUE    42  100\n```\n:::\n:::\n\n\n- Packages:\n  + [caret](https://topepo.github.io/caret/) package (**C**lassification **A**nd **RE**gression **T**raining)\n  + [yardstick](https://yardstick.tidymodels.org/index.html) of [tidymodels](https://www.tidymodels.org/)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\ncaret::confusionMatrix()\nyardstick::conf_mat()\n```\n:::\n\n\n\n::: notes\n- [caret](https://topepo.github.io/caret/) package (**C**lassification **A**nd **RE**gression **T**raining) provides tools for predictive modeling.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n## Classification And REgression Training\nlibrary(caret) \ncaret::confusionMatrix(data = a factor of predicted classes, \n                       reference = a factor of classes to be used as the true results)\n```\n:::\n\n\n- [yardstick](https://yardstick.tidymodels.org/index.html) is a package of [tidymodels](https://www.tidymodels.org/) for estimating how well models are working.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nyardstick::conf_mat(data = a data frame, \n                    truth = true class column that is a factor,\n                    estimate = predicted class column that is a factor)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n:::\n\n\n## [R Lab]{.pink} Receiver Operating Characteristic (ROC) Curve\n\n- **Receiver operating characteristic (ROC) curve** plots True Positive Rate (Sensitivity) vs. False Positive Rate (1 - Specificity)\n- Packages: [ROCR](http://ipa-tys.github.io/ROCR/), [pROC](https://web.expasy.org/pROC/), [yardstick::roc_curve()](https://yardstick.tidymodels.org/reference/roc_curve.html)\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nlibrary(ROCR)\n\n# create an object of class prediction \npred <- ROCR::prediction(\n    predictions = pred_prob, \n    labels = Default$default)\n\n# calculates the ROC curve\nroc <- ROCR::performance(\n    prediction.obj = pred, \n    measure = \"tpr\",\n    x.measure = \"fpr\")\n```\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nplot(roc, colorize = TRUE)\n```\n\n::: {.cell-output-display}\n![](./images/16-logistic-reg/unnamed-chunk-33-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n::::\n\n::: notes\nhttps://rviews.rstudio.com/2019/03/01/some-r-packages-for-roc-curves/\n.footnote[\n.small[\n<sup>+</sup>Originally developed for operators of military radar receivers, hence the name.\n]\n]\n:::\n\n\n\n## [R Lab]{.pink} Area Under Curve (AUC)\nFind the area under the curve:\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n## object of class 'performance'\n\nauc <- ROCR::performance(\n    prediction.obj = pred, \n    measure = \"auc\")\nauc@y.values\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[1] 0.948\n```\n:::\n:::\n\n:::\n\n::: {.column width=\"60%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/16-logistic-reg/unnamed-chunk-35-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n::::\n\n\n\n## [R Lab]{.pink} ROC Curve Comparison\n::: question\nWhich model performs better?\n:::\n\n::: alert\nRemember! Compare the candidates using the test data.\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/16-logistic-reg/unnamed-chunk-36-1.png){fig-align='center' width=62%}\n:::\n:::\n\n\n\n::: notes\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}