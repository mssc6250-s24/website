{
  "hash": "dde60ffcca0afadaea53a2665f33d9b8",
  "result": {
    "markdown": "---\ntitle: \"Nonparametric Regression ðŸ› \"\nsubtitle: \"MATH 4780 / MSSC 5780 Regression Analysis\"\nauthor: \"Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University\"\n# date: \"October 27 2023\"\n# macros: _macros.tex # import a list of TeX/LaTeX definitions\nformat: \n  revealjs:\n    code-line-numbers: false\n    #     - \"macros.tex\"\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n    # include-in-header:\n    highlight-style: arrow\n    code-block-bg: true\n    self-contained: false\n    slide-number: c/t    \n    incremental: false\n    width: 1800\n    height: 1000\n    margin: 0.05\n    logo: \"https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg\"\n    footer: \"[math4780-f23.github.io/website](https://math4780-f23.github.io/website/)\"\n    theme: [\"simple\", \"slides.scss\"]\n    multiplex: true\n    code-link: true\n    fig-cap-location: bottom\n    fig-align: center\n    transition: none ## fade slide convex concave zoom\n    title-slide-attributes:\n      data-background-color: \"#447099\"\n      # data-background-image: images/paper-texture.jpg\n      # data-background-size: cover\n      # data-background-color: \"#698ED5\"\neditor: source\nexecute:\n  freeze: true\n  echo: false\n  purl: true\n---\n\n\n#  {visibility=\"hidden\"}\n\n\\def\\bx{\\mathbf{x}}\n\\def\\bg{\\mathbf{g}}\n\\def\\bw{\\mathbf{w}}\n\\def\\bbeta{\\boldsymbol \\beta}\n\\def\\bX{\\mathbf{X}}\n\\def\\by{\\mathbf{y}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bW{\\mathbf{W}}\n\\def\\T{\\text{T}}\n\\def\\cov{\\mathrm{Cov}}\n\\def\\cor{\\mathrm{Corr}}\n\\def\\var{\\mathrm{Var}}\n\\def\\E{\\mathrm{E}}\n\\def\\bmu{\\boldsymbol \\mu}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\def\\Trace{\\text{Trace}}\n\n\n\n\n\n\n\n# Nonparametric Regression\n\n<h2> Nonparametric Kernel Smoother </h2>\n<h2> Local Regression </h2>\n\n## Nonparametric Statistics\n- A general regression model $y = f(x) + \\epsilon$.\n- **Parametric** model: make an assumption about the shape of $f$, e.g., $f(x) = \\beta_0 + \\beta_1 x$, then learn the **parameters** $\\beta_0$ and $\\beta_1$.\n\n. . .\n\n- **Nonparametric** methods do NOT make assumptions about the form of $f$.\n  + Seek an estimate of $f$ that gets close to the data points without being too rough or wiggly.\n  + Avoid the possibility that the functional form used to estimate $f$ is very different from the true $f$.\n  + Do not reduce the problem of estimating $f$ to a small number of parameters, so more data are required to obtain an accurate estimate of $f$.\n\n\n::: notes\n- So far, with a general regression model $y = f(x) + \\epsilon$, we make an assumption about the form or shape of $f$, for example, $f(x) = \\beta_0 + \\beta_1 x$, then learn the **parameters** $\\beta_0$ and $\\beta_1$ to understand the relationship between $y$ and $x$. This is a **parametric** model.\n- **Nonparametric** methods do not make assumptions about the form of $f$.\n  + They seek an estimate of $f$ that gets close to the data points without being too rough or wiggly.\n  + The methods avoid the possibility that the functional form used to estimate $f$ is very different from the true $f$.\n  + Since they do not reduce the problem of estimating $f$ to a small number of parameters, more observations are required to obtain an accurate estimate for $f$.\n\n:::\n\n\n## Parametric vs. Nonparametric Models\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n**Parametric** (Linear regression)\n<br>\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/12-nonpara-reg/unnamed-chunk-2-1.png){fig-align='center' width=90%}\n:::\n:::\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n**Nonparametric** (Kernel smoother)\n<br>\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/12-nonpara-reg/unnamed-chunk-3-1.png){fig-align='center' width=90%}\n:::\n:::\n\n:::\n::::\n\n\n## Nonparametric Regression\n- In (parametric) linear regression, $\\small \\hat{y}_i = \\sum_{j=1}^n h_{ij}y_j$.\n<!-- - The fitted value for the $i$th response is a linear combination of the original data. -->\n- Nonparametric regression, with no assumption on $f$, is trying to estimate $y_i$ using *the weighted average of the data*:\n$$\\small \\hat{y}_i = \\sum_{j=1}^n w_{ij}y_j$$ where $\\sum_{j=1}^nw_{ij} = 1$.\n\n\n::: alert\n$w_{ij}$ is larger when $x_i$ and $x_j$ are closer. $y_i$ is affected more by its *neighbors*.\n:::\n\n\n::: notes\n$$\\small {\\bf \\hat{y} = Xb = X(X'X)^{-1}X'y = Hy},$$\n-  $w_{ij}$: the influence power of $y_j$ on $y_i$.\n:::\n\n\n\n## Kernel Smoother\n\n- In nonparametric statistics, a **kernel** $K(t)$ is used as a *weighting* function satisfying\n  + $K(t) \\ge 0$ for all $t$\n  + $\\int_{-\\infty}^{\\infty} K(t) \\,dt = 1$\n  + $K(-t) = K(t)$ for all $t$\n\n::: question\nCan you give me an kernel function?\n:::\n\n\n\n## Kernel Smoother\n\n- In nonparametric statistics, a **kernel** $K(t)$ is used as a *weighting* function satisfying\n  + $K(t) \\ge 0$ for all $t$\n  + $\\int_{-\\infty}^{\\infty} K(t) \\,dt = 1$\n  + $K(-t) = K(t)$ for all $t$\n  \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/12-nonpara-reg/unnamed-chunk-4-1.png){fig-align='center' width=50%}\n:::\n:::\n\n\n## Kernel Smoother\n- Let $\\tilde{y}_i$ be the **kernel smoother** of the $i$th response. Then\n$$\\small \\tilde{y}_i = \\sum_{j=1}^n w_{ij}y_j$$ where $\\sum_{j=1}^nw_{ij} = 1$.\n\n- The Nadarayaâ€“Watson kernel regression uses the weights given by $$\\small w_{ij} = \\frac{K \\left( \\frac{x_i - x_j}{b}\\right)}{\\sum_{k=1}^nK \\left( \\frac{x_i - x_k}{b}\\right)}$$\n  + Parameter $b$ is the **bandwidth** that controls the smoothness of the fitted curve.\n  - Closer points are given higher weights: $w_{ij}$ is larger if $x_i$ and $x_j$ are closer.\n\n::: notes\n<!-- or ${\\bf \\tilde{y} = S y}$ where ${\\bf S} = \\left[ w_{ij}\\right]$ is the smoothing matrix created by a kernel function $K(\\cdot)$. -->\n- $\\tilde{y}_i$s will not be as much variationed as $y_j$.\n- By weighted averaging, the value $\\tilde{y}_i$ is synthesized or integrated by other data points. The average value tends to wash out some unexplained noises, and look more like its other data points.\n- These kernel smoothers use a bandwidth, $b$, to define this neighborhood of interest.\n:::\n\n\n## Gaussian Kernel Smoother Example\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nksmooth(x, y, bandwidth = 1, kernel = \"normal\")\nKernSmooth::locpoly(x, y, degree = 0, kernel = \"normal\", bandwidth = 1)\n```\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n- $y = 2\\sin(x) + \\epsilon$\n- $K_b(x_i, x_j) = \\frac{1}{b \\sqrt{2\\pi}}\\exp \\left( - \\frac{(x_i - x_j)^2}{2b^2}\\right)$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/12-nonpara-reg/unnamed-chunk-6-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n\n::: {.column width=\"50%\"}\n$$\\small w_{ij} = \\frac{K \\left( \\frac{x_i - x_j}{b}\\right)}{\\sum_{k=1}^nK \\left( \\frac{x_i - x_k}{b}\\right)}$$ \nBandwidth $b$ defines *\"neighbors\"* of $x_i$, and controls the smoothness of the estimated $f$.\n\n::: alert\n- **Large $b$**: More data points have large weights. The fitted curve becomes smoother\n- **Small $b$**: Less of the data are used, and the resulting curve looks wiggly.\n:::\n\n:::\n::::\n\n\n::: notes\n- Bandwidth $b$ defines *\"neighbors\"* of the specific location of interest, and control the smoothness of the estimated function $f$.\n- When $b$ is large, more data points with large weights are used to predict the response $y_i$ at the specific $x_i$. The fitted curve becomes smoother as $b$ increases.\n- As $b$ decreases, less of the data are used, and the resulting curve looks more wiggly.\n- When $b$ is large, more points having large weights to predict the response at the specific $x$. The resulting plot of predicted values becomes smoother as $b$ increases.\n- These kernel smoothers use a bandwidth, $b$, to define this neighborhood of interest. \n- A large value for $b$ results in more of the data being used to predict the response at the specific location. Consequently, the resulting plot of predicted values becomes much smoother as $b$ increases. \n- Conversely, as $b$ decreases, less of the data are used to generate the prediction, and the resulting plot looks more \"wiggly\" or bumpy.\n:::\n\n\n## Gaussian Kernel Smoother Example\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/12-nonpara-reg/unnamed-chunk-7-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n::: notes\n- The kernel function shows the weights and how fast the weights decay.\n- Point sizes correspond to their kernel weight, or the influence on the fitted or predicted value of $y$ at $x$.\n- To get the estimated fitted curve, we can create a grid of $x$ points. For each $x$, find its response value by taking weighted average of the data points whose weight is determined by the kernel function.\n- The estimated regression function $f$ is the result of connecting all the weighted average responses of the $x$s in the grid.\n:::\n\n\n\n\n## Gaussian Kernel Smoother Example\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/12-nonpara-reg/unnamed-chunk-8-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n## Local Regression\n- **Local regression** is another nonparametric regression alternative.\n\n. . .\n\n- In ordinary least squares, minimize $\\sum_{i=1}^n(y_i - \\beta_0 - \\beta_1x_i)^2$\n- In weighted least squares, minimize $\\sum_{i=1}^nw_i(y_i - \\beta_0 - \\beta_1x_i)^2$\n\n. . .\n\nIn local weighted linear regression, \n\n- Use a *kernel as a weighting function* to define neighborhoods and weights to perform weighted least squares.\n\n- Find the estimates of $\\beta_0$ and $\\beta_1$ at $x_0$ by minimizing $$\\sum_{i=1}^nK_b(x_0, x_i)(y_i - \\beta_0 - \\beta_1x_i)^2$$\n\n\n\n::: notes\n- **Local regression** (local polynomial regression, moving regression) is another nonparametric regression alternative.\n- Idea: *Use a kernel as a weighting function to define neighborhoods and weights to perform weighted least squares.*\n  + Local weighted linear regression\n  + Local weighted polynomial regression\n\n:::\n\n\n## Local Regression\nIn locally weighted linear regression, we find the estimates of $\\beta_0$ and $\\beta_1$ at $x_0$ by minimizing $$\\sum_{i=1}^nK_b(x_0, x_i)(y_i - \\beta_0 - \\beta_1x_i)^2$$\n\n  + Pay more attention to the points that are closer to the target point $x_0$.\n\n. . .\n  \n  + The estimated (local) linear function and $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are only valid at the local point $x_0$.\n\n  \n. . .\n\n  + If interested in a different target $x_0$, we need to refit the model. \n\n. . .\n\n- In locally weighted polynomial regression, minimize $$\\sum_{i=1}^nK_b(x_0, x_i)(y_i - \\beta_0 - \\sum_{r=1}^d\\beta_rx_i^r)^2$$\n  <!-- + It is very sensitive to the choice of bandwidth $b$. -->\n  <!-- + Do not use $r > 2$. (overfitting) -->\n  \n\n::: notes\n- $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are a function of $x_0$ (of course a function of $(x_i, y_i)_{i=1}^n$).\n:::\n\n\n## Local Linear Regression w/ Gaussian Kernel Weights\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/12-nonpara-reg/unnamed-chunk-9-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Local Quadratic Regression w/ Gaussian Weights\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/12-nonpara-reg/unnamed-chunk-10-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n## Local Polynomial Regression in R\nUse `KernSmooth` or `locfit` package.\n<!-- - We can use WLS formula with weights computed by a specified kernel. -->\n\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(KernSmooth)\nlocpoly(x, y, degree, \n        kernel = \"normal\", \n        bandwidth, ...)\n```\n:::\n\n\n- `degree = 1`: local linear\n- `degree = 2`: local quadratic\n- `degree = 0`: **kernel smoother**\n\n:::\n\n\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(locfit)\nlocfit(y ~ lp(x, nn = 0.2, \n              h = 0.5, deg = 2), \n       weights = 1, subset, ...)\n# weights: Prior weights (or sample sizes) \n#          for individual observations.\n# subset: Subset observations in the \n#         data frame.\n# nn: Nearest neighbor component of \n#     the smoothing parameter. \n# h: The constant component of \n#    the smoothing parameter.\n# deg: Degree of polynomial to use.\n```\n:::\n\n\n- Various ways to specify the bandwidth.\n\n:::\n::::\n\n\n::: notes\n\n::: question\nHow does local regression determine the weights?\n:::\n\n- But why local regression?\n  + The Nadaraya-Watson kernel is notorious for boundary effects.\n  + There is a substantial bias at the boundaries.\n  + *Intuition*: all neighbors are smaller/larger than the boundary point.\n  + *Solution*: Locally weighted regression can (partially) correct it.\n- Its most common methods are **LOESS (locally estimated scatterplot smoothing)** and **LOWESS (locally weighted scatterplot smoothing)**.\n- Like kernel regression, LOESS uses the data from a neighborhood around the specific location.\n- The neighborhood $N(x_0)$ is defined by the **span** parameter $\\alpha$, the fraction of the total points closest to $x_0$.\n- The LOESS uses the points in $N(x_0)$ to generate a weighted least-squares estimate\nof $y(x_0)$.\n\n:::\n\n## LOESS\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n**LOESS (LOcally Estimated Scatterplot Smoothing)** uses the **tricube** kernel $K(x_0, x_i)$ defined as $$K\\left( \\frac{|x_0 - x_i|}{\\max_{k \\in N(x_0)} |x_0 - x_k|}\\right)$$ where $$K(t) = \\begin{cases} (1-t^3)^3       & \\quad \\text{for } 0 \\le t \\le 1\\\\ 0 & \\quad \\text{otherwise } \\end{cases}$$\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/12-nonpara-reg/unnamed-chunk-13-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::::\n\n\n- The neighborhood $N(x_0)$ is defined by the **span** parameter $\\alpha$, the fraction of the total points closest to $x_0$.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nloess(y ~ x, span = 0.75, degree = 2) ## Default setting\n```\n:::\n\n- Larger $\\alpha$ means more neighbors and smoother fitting.\n\n\n\n::: notes\nLOESS is a special case of Local Polynomial Regression Fitting\n:::\n\n\n## LOESS Example\n- LOESS uses the points in $N(x_0)$ to generate a WLS estimate of $y(x_0)$.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/12-nonpara-reg/unnamed-chunk-15-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n::: notes\n- LOESS is a special case of local polynomial regression.\n:::\n\n\n## R Implementation\n\n- `loess()`, `KernSmooth::locploy()`, `locfit::locfit()`, `ksmooth()`. **Not all of them uses the same definition of the bandwidth**.\n\n- `ksmooth`: The kernels are scaled so that their quartiles are at $\\pm 0.25 * \\text{bandwidth}$.\n\n- `KernSmooth::locpoly` uses the raw value that we directly plug into the kernel.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/12-nonpara-reg/unnamed-chunk-16-1.png){fig-align='center' width=82%}\n:::\n:::\n\n\n\n::: notes\n- h=1.06Ïƒxnâˆ’1/5\n- ksmooth: The kernels are scaled so that their quartiles (viewed as probability densities) are at Â± 0.25*bandwidth.\n- span: In this case, the bandwidth is decided by first finding the closes k neighbors and then use a tri-cubic weighting on the range of these neighbors. the bandwidth essentially varies depending on the target point\n- the kknn() also utilize such a feature as default, so when you want to fit the standard KNN, you should specify method = \"rectangular\", otherwise, the neighboring points will not receive the same weight.\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}