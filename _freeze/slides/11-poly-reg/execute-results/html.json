{
  "hash": "25e07556c6a5f39377e8ed38b99835e2",
  "result": {
    "markdown": "---\ntitle: \"Polynomial Regression ‚ñ∂Ô∏è\"\nsubtitle: \"MATH 4780 / MSSC 5780 Regression Analysis\"\nauthor: \"Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University\"\n# date: \"October 27 2023\"\n# macros: _macros.tex # import a list of TeX/LaTeX definitions\nformat: \n  revealjs:\n    # code-line-numbers: false\n    #     - \"macros.tex\"\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n    # include-in-header:\n    highlight-style: arrow\n    code-block-bg: true\n    self-contained: false\n    slide-number: c/t    \n    incremental: false\n    width: 1800\n    height: 1000\n    margin: 0.05\n    logo: \"https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg\"\n    footer: \"[math4780-f23.github.io/website](https://math4780-f23.github.io/website/)\"\n    theme: [\"simple\", \"slides.scss\"]\n    multiplex: true\n    code-link: true\n    fig-cap-location: bottom\n    fig-align: center\n    transition: none ## fade slide convex concave zoom\n    title-slide-attributes:\n      data-background-color: \"#447099\"\n      # data-background-image: images/paper-texture.jpg\n      # data-background-size: cover\n      # data-background-color: \"#698ED5\"\neditor: source\nexecute:\n  freeze: true\n  echo: false\n  purl: true\n---\n\n\n#  {visibility=\"hidden\"}\n\n\\def\\bx{\\mathbf{x}}\n\\def\\bg{\\mathbf{g}}\n\\def\\bw{\\mathbf{w}}\n\\def\\balpha{\\boldsymbol \\alpha}\n\\def\\bbeta{\\boldsymbol \\beta}\n\\def\\beps{\\boldsymbol \\epsilon}\n\\def\\bX{\\mathbf{X}}\n\\def\\by{\\mathbf{y}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bW{\\mathbf{W}}\n\\def\\T{\\text{T}}\n\\def\\cov{\\mathrm{Cov}}\n\\def\\cor{\\mathrm{Corr}}\n\\def\\var{\\mathrm{Var}}\n\\def\\E{\\mathrm{E}}\n\\def\\bmu{\\boldsymbol \\mu}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\def\\Trace{\\text{Trace}}\n\n\n\n\n\n\n\n<!-- ```{r} -->\n<!-- #| echo: false -->\n<!-- CIA <- read.table(\"./data/CIA.txt\", header = TRUE) -->\n<!-- ciafit <- lm(infant ~ gdp + health + gini, data = CIA) -->\n<!-- r_stud <- rstudent(ciafit) -->\n<!-- logciafit <- lm(log(infant) ~ gdp + health + gini, data = CIA) -->\n<!-- ``` -->\n\n\n\n\n# Polynomial Regression\n<h2> Polynomial Models in One Variable </h2>\n<h2> Piecewise Regression </h2>\n<h2> Splines </h2>\n\n\n## Why Polynomial Regression\n\n- Polynomials are widely used in situations where the response surface is curvilinear.\n- Many complex nonlinear relationships can be adequately modeled by polynomials over reasonably small ranges of the $x$'s.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/11-poly-reg/unnamed-chunk-2-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n::: notes\n- They are also useful as approximating functions to unknown and possibly very complex nonlinear relationships.\n- In this sense, the polynomial model is just the Taylor series expansion of the unknown function.\n:::\n\n\n## Polynomial Regression Models\n- A **second**-order (degree) polynomial in **one** variable or a **quadratic** model is\n$$y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\epsilon$$\n\n- A **second**-order polynomial in **two** variables is\n$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{11}x_1^2 + \\beta_{22}x_2^2 + \\beta_{12}x_1x_2 + \\epsilon$$\n\n- The **$k$th-order** polynomial model in **one** variable is\n$$y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\cdots + \\beta_kx^k + \\epsilon$$\n\n- If we set $x_j = x^j$, this is just a multiple linear regression model with $k$ predictors $x_1, x_2, \\dots, x_k$!\n\n\n\n\n## Important Considerations\n::: large\n_Keep the order of the model **as low as possible**_.\n:::\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n- Transform data to keep the model 1st order.\n\n- If fails, try a 2nd order model.\n\n- Avoid higher-order polynomials unless they can be justified for reasons *outside the data*.\n\n- üëâ **Occam's Razor**: among competing models that predict equally well, choose the \"simplest\" one, i.e., a **parsimonious** model.\n  + This avoids **overfitting** that leads to nearly perfect fit to the data, but bad prediction performance.\n  \n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: Wikiversity](./images/11-poly-reg/polynomial.png){fig-align='center' width=70%}\n:::\n:::\n\n:::\n\n::::\n  \n::: notes\nThere are several important considerations that arise when fitting a polynomial\nin one variable.\n- A low-order model in a transformed variable is almost always preferable to a high-order model in the original metric. \n- Arbitrary fitting of high-order polynomials is a serious abuse of regression analysis.\n- You can always fit a polynomial model with very high order. It may give you a pretty good fit, but with very high chance, it will end up with very bad prediction, leading to overfitting.\n  + **Occam's Razor**: among competing models that predict equally well, choose the \"simplest\" one, i.e., **parsimonious** model.\n  + This avoid **overfitting** that leads to bad prediction performance.\n\n:::\n\n. . .\n\n::: alert\n*\"Bayesian Deep Learning and a Probabilistic Perspective of Generalization\"* Wilson and Izmailov (2020) for the rationale of choosing a super high-order polynomial as the regression model.\n:::\n\n::: notes\n- the ground truth explanation for the data is out of class for any of these choices, but there is some setting of the coefficients in choice (3) which provides a better description of reality than could be managed by choices (1)\nand (2).\n- our beliefs about the generative processes for our observations, which are often very sophisticated, typically ought to be independent of how many data points we happen to observe.\n- we often use neural networks with millions of parameters to fit datasets with thousands of points\n:::\n\n\n## Important Considerations\n::: large\n*Model building strategy*\n:::\n\n- üëâ **Forward selection**: successively fit models of increasing order until the $t$-test for the highest order term is non-significant.\n\n- üëâ **Backward elimination**: fit the highest order model and then delete terms one at a time until the highest order remaining term has a significant $t$ statistic.\n\n- üëâ They do not necessarily lead to the same model.\n\n- üëâ Restrict our attention to low-order polynomials.\n\n## Important Considerations\n::: large\n*Extrapolation*\n:::\n\n- Can be extremely dangerous when the model is higher-order polynomial. \n- The nature of the true underlying relationship may change or be completely different from the system that produced the data used to fit the model.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/11-poly-reg/fig72_extrapolation.png){fig-align='center' width=40%}\n:::\n:::\n\n\n\n## Important Considerations\n::: large\n*Ill-conditioning*\n:::\n\n  + **Ill-conditioning**: *as the order of the model increases, ${\\bf X'X}$ matrix inversion will become inaccurate*, and error may be introduced into the parameter estimates\n  + Centering the predictors may remove some ill conditioning but not all.\n  + One solution is to use **orthogonal polynomials** (LRA Sec 7.5).\n  \n  \n::: notes\n  + **Ill-conditioning** refers to the fact that *as the order of the model increases, ${\\bf X'X}$ matrix inversion will become inaccurate*, and error can be introduced into the parameter estimates\n  + Centering the predictors may remove some ill conditioning but not all.\n:::\n\n## Example 7.1: [Hardwood Data](./data/data-ex-7-1.csv) (LRA)\n- Strength of kraft paper vs. the percentage of hardwood in the batch of pulp from which the paper was produced.\n- A quadratic model may adequately describe the relationship between tensile strength and hardwood concentration.\n\n:::: {.columns}\n\n::: {.column width=\"33%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhardwood[1:9, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  conc strength\n1  1.0      6.3\n2  1.5     11.1\n3  2.0     20.0\n4  3.0     24.0\n5  4.0     26.1\n6  4.5     30.0\n7  5.0     33.8\n8  5.5     34.0\n9  6.0     38.1\n```\n:::\n:::\n\n:::\n\n::: {.column width=\"66%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/11-poly-reg/unnamed-chunk-6-1.png){fig-align='center' width=90%}\n:::\n:::\n\n:::\n::::\n\n\n## [R Lab]{.pink} Hardwood Data Model Fitting\n- Following the suggestion that centering the data may remove nonessential ill-conditioning:\n$$y = \\beta_0 + \\beta_1 (x - \\bar{x}) + \\beta_2 (x - \\bar{x}) ^ 2 + \\epsilon$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconc_cen <- hardwood$conc - mean(hardwood$conc)\nlm(strength ~ conc_cen + I(conc_cen ^ 2), data = hardwood)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = strength ~ conc_cen + I(conc_cen^2), data = hardwood)\n\nCoefficients:\n  (Intercept)       conc_cen  I(conc_cen^2)  \n       45.295          2.546         -0.635  \n```\n:::\n:::\n\n\n- $y = 45.3 + 2.55 (x - 7.26) - 0.63 (x - 7.26) ^ 2 + \\epsilon$\n- Inference, prediction and residual diagnostics procedures are the same as multiple linear regression.\n\n\n\n::: notes\n- We don't need to create an extra predictor concentration ^ 2 in the data set.\n- We can construct the x_sq term in the lm() function. \n- We use I() function to wrap up the operation concen_cen ^ 2, so that concen_cen ^ 2 is not interpreted as a part of formula, but actual arithmetic operation.\n- To avoid this confusion, the function I() can be used to bracket those portions of a model formula where the operators are used in their arithmetic sense. For example, in the formula y ~ a + I(b+c), the term b+c is to be interpreted as the sum of b and c.\n:::\n\n\n## Piecewise (Polynomial) Regression\n- A polynomial regression may provide a poor fit, and increasing the order does not improve the situation.\n- This may happen when *the regression function behaves differently in different parts of the range of $x$*.\n\n. . .\n\n- SOLUTION: üëâ *piecewise* polynomial regression that **fits separate polynomials over different regions of $x$**.\n- Example:\n$$y=\\begin{cases}\n    \\beta_{01} + \\beta_{11}x+ \\beta_{21}x^2+\\beta_{31}x^3 +\\epsilon     & \\quad \\text{if } x < c\\\\\n    \\beta_{02} + \\beta_{12}x+ \\beta_{22}x^2+\\beta_{32}x^3+\\epsilon      & \\quad \\text{if } x \\ge c\n  \\end{cases}$$\n  \n- The joint points of pieces are called **knots**.\n\n. . .\n\n- Using more knots leads to a more flexible piecewise polynomial. \n\n::: question\nWith $K$ different knots, how many different polynomials do we have?\n:::\n\n<!-- - With $K$ different knots, we fit $K + 1$ different polynomials. -->\n<!-- - Piecewise polynomials of order $k$ are called **splines**. -->\n\n::: notes\n- There might be tow different systems or schemes that govern the relationship between response and predictor variables.\n- Any issue of fitting a piecewise polynomial regression?\n- In other words, we fit two different polynomial functions to the data, one on the subset of the observations with $xi < c$, and one on the subset of\nthe observations with $xi ‚â• c$. The first polynomial function has coefficients Œ≤01, Œ≤11, Œ≤21, Œ≤31, and the second has coefficients Œ≤02, Œ≤12, Œ≤22, Œ≤32. \n- Each of these polynomial functions can be fit using least squares applied to simple functions of the original predictor.\n- the function is discontinuous\n- locations of the knots are known, otherwise it is a nonlinear regression problem.\n- WE could fit piecewise regression with order 1.\n- (or more constrained?) \n:::\n\n\n\n## [U.S. Birth Rate from 1917 to 2003](./data/birthrates.Rda)\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```{.my_class800}\n     Year Birthrate\n1917 1917       183\n1918 1918       184\n1919 1919       163\n1920 1920       180\n1921 1921       181\n1922 1922       173\n1923 1923       168\n1924 1924       177\n1925 1925       172\n1926 1926       170\n1927 1927       164\n1928 1928       152\n1929 1929       145\n1930 1930       145\n1931 1931       139\n1932 1932       132\n1933 1933       126\n1934 1934       130\n1935 1935       130\n1936 1936       130\n```\n:::\n:::\n\n:::\n\n::: {.column width=\"70%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/11-poly-reg/unnamed-chunk-9-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n::::\n\n\n## [R Lab]{.pink} A Polynomial Regression Provide a Poor Fit\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlmfit3 <- lm(Birthrate ~ poly(Year - mean(Year), degree = 3, raw = TRUE),  \n             data = birthrates)\n```\n:::\n\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/11-poly-reg/unnamed-chunk-11-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/11-poly-reg/unnamed-chunk-12-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n::::\n\n\n::: notes\nIt might be interesting to fit a linear regression with high order polynomials to approximate this curve. This can be carried out using the poly() function, which calculates all polynomials up to a certain power. Please note that this is a more stable method compared with writing out the powers such as I(Year^2), I(Year^3) etc because the Year variable is very large, and is numerically unstable.\n:::\n\n\n## [R Lab]{.pink} Piecewise Polynomials: 3 knots at 1936, 60, 78\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/11-poly-reg/unnamed-chunk-13-1.png){fig-align='center' width=65%}\n:::\n:::\n\n\n\n::: question\nAny issue of piecewise polynomials?\n:::\n\n\n::: notes\nthese functions are not continuous. Hence we use a trick to construct continuous basis\n:::\n\n\n\n## Splines\nSplines of degree $k$ are piecewise polynomials of degree $k$ with **continuity in derivatives** (smoothing) up to degree $k-1$ at each knot.\n\n- Use `bs()` function in the **splines** package.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlin_sp <- lm(Birthrate ~ splines::bs(Year, degree = 1, knots = c(1936, 1960, 1978)), \n             data = birthrates)\n```\n\n::: {.cell-output-display}\n![](./images/11-poly-reg/unnamed-chunk-14-1.png){fig-align='center' width=55%}\n:::\n:::\n\n\n\n\n## Cubic Splines\n- The cubic spline is a spline of degree 3 with *first 2 derivatives are continuous* at the knots.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncub_sp <- lm(Birthrate ~ splines::bs(Year, degree = 3, knots = c(1936, 1960, 1978)), \n             data = birthrates)\n```\n\n::: {.cell-output-display}\n![](./images/11-poly-reg/unnamed-chunk-15-1.png){fig-align='center' width=55%}\n:::\n:::\n\n\n\n## Practical Issue\n\n- *How many knots should be used*\n  + As few knots as possible\n  + At least 5 data points per segment\n\n- *Where to place the knots*\n  + No more than one extreme point per segment\n  + If possible, the extreme points should be centered in the segment\n\n- *What is the degree of functions in each region*\n  + Cubic spline is popular",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}