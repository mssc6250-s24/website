{
  "hash": "bef85c4ba5d2a6f934fca82ba78a3c7b",
  "result": {
    "markdown": "---\ntitle: \"Probability and Statistics ðŸŽ²\"\nsubtitle: \"MATH 4780 / MSSC 5780 Regression Analysis\"\nauthor: \"Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University\"\n# date: \"August 20 2023\"\n# macros: _macros.tex # import a list of TeX/LaTeX definitions\nformat: \n  revealjs:\n    # code-line-numbers: false\n    #     - \"macros.tex\"\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n    # include-in-header:\n    highlight-style: arrow\n    code-block-bg: true\n    self-contained: false\n    slide-number: c/t    \n    incremental: false\n    width: 1800\n    height: 1000\n    margin: 0.05\n    logo: \"https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg\"\n    footer: \"[math4780-f23.github.io/website](https://math4780-f23.github.io/website/)\"\n    theme: [\"simple\", \"slides.scss\"]\n    multiplex: false\n    code-link: true\n    fig-cap-location: bottom\n    fig-align: center\n    transition: none ## fade slide convex concave zoom\n    title-slide-attributes:\n      data-background-color: \"#447099\"\n      # data-background-image: images/paper-texture.jpg\n      # data-background-size: cover\n      # data-background-color: \"#698ED5\"\neditor: source\nexecute:\n  freeze: true\n  echo: false\n  purl: true\n---\n\n\n#  {visibility=\"hidden\"}\n\n\\def\\bx{\\mathbf{x}}\n\\def\\bg{\\mathbf{g}}\n\\def\\bw{\\mathbf{w}}\n\\def\\bbeta{\\boldsymbol \\beta}\n\\def\\bX{\\mathbf{X}}\n\\def\\by{\\mathbf{y}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bW{\\mathbf{W}}\n\\def\\T{\\text{T}}\n\\def\\cov{\\mathrm{Cov}}\n\\def\\cor{\\mathrm{Corr}}\n\\def\\var{\\mathrm{Var}}\n\\def\\E{\\mathrm{E}}\n\\def\\bmu{\\boldsymbol \\mu}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\def\\Trace{\\text{Trace}}\n\n\n\n\n\n\n\n\n# Random Variables\n\n## Discrete Random Variables\n\n- A **discrete** variable $Y$ has *countable* possible values, e.g. $\\mathcal{Y} = \\{0, 1, 2\\}$\n\n- **Probability (mass) function** (pf or pmf) $$P(Y = y) = p(y), \\,\\, y \\in \\mathcal{Y}$$\n  + $0 \\le p(y) \\le 1$ for all $y \\in \\mathcal{Y}$\n  \n  + $\\sum_{y \\in \\mathcal{Y}}p(y) = 1$\n  \n  + $P(a < Y < b) = \\sum_{y: a<y<b}p(y)$\n\n<!-- - **Cumulative distribution function** (cdf) $$F(y) := P(Y \\le y) = \\sum_{z \\le y}p(z)$$ -->\n\n\n::: notes\ngive an example.\n:::\n\n::: question\nGive me an example of a discrete variable/distribution!\n:::\n\n## Binomial Probability Function\n\n$P(Y = y; m, \\pi) = \\frac{m!}{y!(m-y)!}\\pi^y(1-\\pi)^{m-y}, \\quad y = 0, 1, 2, \\dots, m$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/03-prob-stat/binomial_plot-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n## Continuous Random Variables\n\n- A **continuous** variable $Y$ has *infinite* possible values, e.g. $\\mathcal{Y} = [0, \\infty)$\n\n- **Probability density function** (pdf) $$f(y), \\,\\, y \\in \\mathcal{Y}$$\n\n  + $f(y) \\ge 0$ for all $y \\in \\mathcal{Y}$\n  \n  + $\\int_{\\mathcal{Y}}f(y) \\, dy= 1$\n  \n  + $P(a < Y < b) = \\int_{a}^bf(y)\\,dy$\n\n<!-- - **Cumulative distribution function** (cdf) $$F(y) := P(Y \\le y) = \\int_{y}^{\\infty}f(t)\\,dt$$ -->\n\n. . .\n\n::: question\nGive me an example of continuous variable/distribution!\n:::\n\n\n## Normal (Gaussian) Density Curve\n\nFor continuous variables, **$P(a < Y < b)$ is the area under the density curve between $a$ and $b$.**\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/03-prob-stat/norm_den-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n\n\n## Expected Value and Variance\nFor a random variable $Y$,\n\n- The **expected value** or **mean**: $E(Y)$ or $\\mu$.\n\n- The **variance**: $\\var(Y)$ or $\\sigma^2$.\n\n. . .\n\n- The mean measures the center of the distribution, or the balancing point of a seesaw.\n\n- The variance measures the mean squared distance from the mean, or dispersion  of a distribution.\n\n\n<!-- - The mean of a discrete random variable is the weighted average of possible values weighted by their corresponding probability. -->\n\n<!-- - The variance of a discrete random variable is the weighted sum of squared deviation from the mean weighted by probability values. -->\n\n. . . \n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n*Discrete* $Y$:\n\n::: midi\n$$E(Y) := \\sum_{y \\in \\mathcal{Y}}yP(Y = y)$$\n$$\\begin{align} \\var(Y) &:= E\\left[(Y - E(Y))^2 \\right] \\\\&= \\sum_{y \\in \\mathcal{Y}}(y - \\mu)^2P(Y = y)\\end{align}$$\n:::\n\n:::\n\n\n\n::: {.column width=\"50%\"}\n*Continuous* $Y$:\n\n::: midi\n$$E(Y) := \\int_{-\\infty}^{\\infty}yf(y)\\, dy$$\n$$\\begin{align} \\var(Y) &:= E\\left[(Y - E(Y))^2 \\right] \\\\&= \\int_{-\\infty}^{\\infty}(y - \\mu)^2f(y)\\, dy \\end{align}$$\n:::\n\n:::\n::::\n\n\n::: notes\n- The mean of a discrete random variable is the weighted average of possible values weighted by their corresponding probability.\n\n- The variance of a discrete random variable is the weighted sum of squared deviation from the mean weighted by probability values.\n\n- give an normal example\n:::\n\n\n::: alert\nThis is **NOT** the sample mean $\\overline{y}$ or sample variance $s^2$.\n:::\n\n::: notes\nIf $Y \\sim binomial(n, \\pi)$, what are the mean and variance?\n:::\n\n\n## [R Lab]{.pink} dpqr Functions\n\nFor some distribution (`dist`), \n\n- **d**`dist(x, ...)`: density value $f(x)$ or probability value $P(X = x)$.\n- **p**`dist(q, ...)`: cdf $F(q) = P(X \\le q)$.\n- **q**`dist(p, ...)`: quantile of probability $p$.\n- **r**`dist(n, ...)`: generate $n$ random numbers.\n\n::: notes\n- In practice, we are not gonna calculate those properties by hand. Instead, we use computing software.\n- In R we can use dpqr Functions to calculate probabilities or generate values from some distribution. Let's see how.\n:::\n\n. . .\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## 10 binomial variable values with m = 5\nrbinom(n = 10, size = 5, prob = 0.4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 2 2 2 2 2 2 0 1 3 3\n```\n:::\n\n```{.r .cell-code}\n## P(X = 3) of binom(5, 0.4)\ndbinom(x = 3, size = 5, prob = 0.4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.23\n```\n:::\n:::\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## P(X <= 2) of binom(5, 0.4)\npbinom(q = 2, size = 5, prob = 0.4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.683\n```\n:::\n:::\n\n:::\n::::\n\n\n## [R Lab]{.pink} dpqr Functions\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## the default mean = 0 and sd = 1 (standard normal)\nrnorm(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -0.151  0.259 -0.649  0.846 -0.660\n```\n:::\n:::\n\n\n. . .\n\n- $100$ random draws from $N(0, 1)$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/03-prob-stat/nor-den-draw-1.png){fig-align='center' width=72%}\n:::\n:::\n\n\n## [R Lab]{.pink} dpqr Functions\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# P(0.5 < Z < 1) where Z ~ N(0, 1)\npnorm(1) - pnorm(0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.15\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/03-prob-stat/nor-den-ab-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\n\n## [R Lab]{.pink} dpqr Functions\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm <- 5\np <- 0.4\n## mean\n(mu <- sum(0:5 * dbinom(0:5, size = m, prob = p)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n```\n:::\n\n```{.r .cell-code}\nm * p\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n```\n:::\n\n```{.r .cell-code}\n## var\nsum((0:5 - mu) ^ 2 * dbinom(0:5, size = m, prob = p))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.2\n```\n:::\n\n```{.r .cell-code}\nm * p * (1 - p)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.2\n```\n:::\n:::\n\n\n\n##\n\n::: tiny\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![https://statisticsglobe.com/probability-distributions-in-r](./images/03-prob-stat/dist-fcn-r.png){fig-align='center' width=75%}\n:::\n:::\n\n:::\n\n# Distributions\n\n\n## Some of Normals is Normal\n- If $Y \\sim N(\\mu, \\sigma^2)$, $Z = \\frac{Y - \\mu}{\\sigma} \\sim N(0, 1)$.\n\n. . .\n\n- If $X \\sim N(\\mu_X, \\sigma_X^2)$ and $Y \\sim N(\\mu_Y, \\sigma_Y^2)$ and $X$ and $Y$ are independent. Then for $a, b \\in \\mathbf{R}$,\n$$aX + bY \\sim N\\left(a\\mu_X+b\\mu_Y, \\color{red}{a^2} \\color{black} \\sigma_X^2 + \\color{red}{b^2} \\color{black} \\sigma_Y^2\\right)$$\n\n. . .\n\n::: question\nWhat is the distribution of $a_1Y_1 + a_2Y_2 + \\cdots + a_nY_n$ if $Y_i \\sim N(\\mu_i, \\sigma^2_i)$ and $Y_i$s are independent?\n:::\n\n\n\n::: notes\n- If $Z \\sim N(0, 1)$, then $$Z^2 \\sim \\chi^2_1$$\n- If $Z_i \\stackrel{iid}{\\sim} N(0, 1), i = 1, 2, \\dots, k$, then $$\\sum_{i=1}^k Z_i^2 \\sim \\chi^2_k$$\n\n- $iid$ means <span style=\"color:blue\"> * **i**ndependent **i**dentically **d**istributed*</span>.\n\n- Let $Y_i \\stackrel{indep}{\\sim} N(\\mu_i, \\sigma_i^2), i = 1, 2, \\dots, n$. Then the random variable $U = \\sum_{i=1}^n a_iY_i$ has the distribution\n:::\n\n\n## Normal, Chi-Squared, Student's-t and F {visibility=\"hidden\"}\n\n- If $Z \\sim N(0, 1)$, $V \\sim \\chi^2_v$ and $Z$ and $V$ are independent, then $$\\frac{Z}{\\sqrt{V/v}} \\sim t_{v}$$\n\n- If $V \\sim \\chi^2_v$, $W \\sim \\chi^2_w$ and $V$ and $W$ are independent, then $$\\frac{V/v}{W/w} \\sim F_{v, w}$$\n\n\n## Statistics Comes In\nSuppose each data point $Y_i$ of the sample $(Y_1, Y_2, \\dots, Y_n)$ is a random variable from the same population whose distribution is $N(\\mu, \\sigma^2)$, and  $Y_i$s are independent each other: $$Y_i \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2), \\quad i = 1, 2, \\dots, n$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/03-prob-stat/unnamed-chunk-12-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\n## Statistics Comes In: Sampling Distribution\n\nIf $Y_i \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2), \\quad i = 1, 2, \\dots, n$, \n\n- $\\overline{Y} \\sim N\\left(\\mu,\\frac{\\sigma^2}{n} \\right)$\n\n- $Z = \\frac{\\overline{Y} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0, 1)$\n\n. . .\n\n- Let the sample variance of $Y$ be $S^2 = \\frac{\\sum_{i=1}^n(Y_i - \\overline{Y})^2}{n-1}$. \n\n- $\\frac{\\overline{Y} - \\mu}{S/\\sqrt{n}} \\sim t_{n-1}$\n\n. . .\n\n- **Inference**: $\\mu$ and $\\sigma^2$ are unknown, and $\\overline{y}$ and $s^2$ are point estimates for $\\mu$ and $\\sigma^2$, respectively.\n\n\n\n## Why Use Normal? <span style=\"color: red\">**Central Limit Theorem (CLT)**</span>\n\n- $X_1, X_2, \\dots, X_n$ are i.i.d. variables with mean $\\mu$ and variance $\\sigma^2 < \\infty$. \n\n- As $n$ increases, the sampling distribution of $\\overline{X}_n = \\frac{\\sum_{i=1}^nX_i}{n}$ looks **more and more like $N(\\mu, \\frac{\\sigma^2}{n})$, regardless of the distribution from which we are sampling $X_i$!**\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/03-prob-stat/clt.png){fig-align='center' width=89%}\n:::\n:::\n\n\n\n::: notes\n- Alright. We know why we want large sample. You will find that we use normal distribution quite often. It is not because it is called normal or more normal than other distributions. There is a reason why we use it. \n- The reason is Central Limit Theorem.\n- The CLT says that Suppose is $\\overline{X}_n$ is from a random sample of size $n$ and from a population distribution having mean $\\mu$ and finite standard deviation $\\sigma$. As $n$ increases, the sampling distribution of $\\overline{X}_n$ looks **more and more like $N(\\mu, \\sigma^2/n)$, regardless of the distribution from which we are sampling!**\n- Look at this figure. Your population distribution can be of any shape. As long as the distribution has mean and variance, its sampling distribution of sample mean will always look like a normal distribution as long as n is large. \n:::\n\n\n##\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Nature Methods 10, 809â€“810 (2013)](images/03-prob-stat/sampling_dist.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## $(1-\\alpha)100\\%$ Confidence Interval for $\\mu$\n\n- $T = \\frac{\\overline{Y} - \\mu}{S/\\sqrt{n}} \\sim t_{n-1}$\n\n$$\\small \\begin{align} & \\quad \\quad P(-t_{\\alpha/2, n-1} < T  < t_{\\alpha/2, n-1}) = 1 - \\alpha \\\\ & \\iff P(-t_{\\alpha/2, n-1} < \\frac{\\overline{Y} - \\mu}{S/\\sqrt{n}} < t_{\\alpha/2, n-1}) = 1 - \\alpha \\\\ & \\iff P(\\mu-t_{\\alpha/2, n-1}S/\\sqrt{n} < \\overline{Y} < \\mu + t_{\\alpha/2, n-1}S/\\sqrt{n}) = 1 - \\alpha \\end{align}$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/03-prob-stat/unnamed-chunk-15-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n## $(1-\\alpha)100\\%$ Confidence Interval for $\\mu$: Probability\n:::: {.columns}\n\n\n::: {.column width=\"45%\"}\n\n::: small\n$$P\\left(\\mu-t_{\\alpha/2, n-1}\\frac{S}{\\sqrt{n}} < \\overline{Y} < \\mu + t_{\\alpha/2, n-1}\\frac{S}{\\sqrt{n}} \\right) = 1-\\alpha$$\n:::\n\n::: question\nIs the interval $\\left(\\mu-t_{\\alpha/2, n-1}\\frac{S}{\\sqrt{n}}, \\mu + t_{\\alpha/2, n-1}\\frac{S}{\\sqrt{n}} \\right)$ our confidence interval?\n:::\n\n:::\n\n\n::: {.column width=\"55%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/03-prob-stat/ci_95-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::::\n\n. . .\n\n**No! We don't know $\\mu$, the quantity we'd like to estimate!** But we almost there!\n\n\n\n## $(1-\\alpha)100\\%$ Confidence Interval for $\\mu$: Formula\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\n::: small\n$$\\begin{align}\n&P\\left(\\mu-t_{\\alpha/2, n-1}\\frac{S}{\\sqrt{n}} < \\overline{Y} < \\mu + t_{\\alpha/2, n-1}\\frac{S}{\\sqrt{n}} \\right) = 1-\\alpha\\\\\n&P\\left( \\boxed{\\overline{Y}- t_{\\alpha/2, n-1}\\frac{S}{\\sqrt{n}} < \\mu < \\overline{Y} + t_{\\alpha/2, n-1}\\frac{S}{\\sqrt{n}}} \\right) = 1-\\alpha\n\\end{align}$$\n\n:::\n\n:::\n\n\n\n::: {.column width=\"55%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/03-prob-stat/ci_95-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::::\n\n\n- <span style=\"color:blue\"> With sample data of size $n$, $\\left( \\overline{y}- t_{\\alpha/2, n-1}\\frac{s}{\\sqrt{n}}, \\overline{y} + t_{\\alpha/2, n-1}\\frac{s}{\\sqrt{n}} \\right)$ is our $(1-\\alpha)100\\%$ CI for $\\mu$. </span>\n\n## Hypothesis Testing\n- <span style=\"color:blue\"> $H_0: \\mu = \\mu_0  \\text{   vs.   }  H_1: \\mu > \\mu_0$, or $\\mu < \\mu_0$, or $\\mu \\ne \\mu_0$ </span>\n- The significant level $\\alpha = P(\\text{Reject } H_0 \\mid H_0 \\text{ is true}) = P(\\text{Type I error})$\n- The test statistic is $t_{test} = \\frac{\\overline{y} - \\color{blue}{\\mu_0}}{s/\\sqrt{n}}$, a value from $T \\sim t_{n-1}$.\n- When calculating a test statistic, we assume $H_0$ is **true**.\n\n. . .\n\n**Reject $H_0$ if** \n\n |         Method   &nbsp; &nbsp;        | **Right**-tailed $(H_1: \\mu > \\mu_0)$  | **Left**-tailed  $(H_1: \\mu < \\mu_0)$  | **Two-tailed** $(H_1: \\mu \\ne \\mu_0)$|\n|:------------------------------:|:--------------:|:---------------:|:-----------:|\n| Critical value | $t_{test} > t_{\\alpha, n-1}$ | $t_{test} < -t_{\\alpha, n-1}$ | $\\mid t_{test}\\mid \\, > t_{\\alpha/2, n-1}$ |\n|  $p$-value | $\\small P(T > t_{test} \\mid H_0) < \\alpha$  | $\\small P(T < t_{test} \\mid H_0) < \\alpha$ | $\\small 2P(T > \\,\\mid t_{test}\\mid) \\mid H_0) < \\alpha$ |\n\n## Both Methods Lead to the Same Conclusion\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/03-prob-stat/pvalue_criticalvalue-1.png){fig-align='center' width=85%}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}