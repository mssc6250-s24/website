{
  "hash": "d7f9306e064a8d057060234b7d2af026",
  "result": {
    "markdown": "---\ntitle: 'Linear Regression `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 512 512\" style=\"height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M64 64c0-17.7-14.3-32-32-32S0 46.3 0 64V400c0 44.2 35.8 80 80 80H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H80c-8.8 0-16-7.2-16-16V64zm406.6 86.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L320 210.7l-57.4-57.4c-12.5-12.5-32.8-12.5-45.3 0l-112 112c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L240 221.3l57.4 57.4c12.5 12.5 32.8 12.5 45.3 0l128-128z\"/></svg>`{=html}'\nsubtitle: \"MATH/COSC 3570 Introduction to Data Science\"\nauthor: \"Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University\"\n# date: \"December 23 2023\"\n# macros: _macros.tex # import a list of TeX/LaTeX definitions\nformat: \n  revealjs:\n    #     - \"macros.tex\"\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n    # include-in-header:\n    highlight-style: arrow\n    code-block-bg: true\n    self-contained: false\n    slide-number: c/t    \n    incremental: false\n    width: 1800\n    height: 1000\n    margin: 0.05\n    logo: \"https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg\"\n    footer: \"[math3570-s24.github.io/website](https://math3570-s24.github.io/website/)\"\n    theme: [\"simple\", \"slides.scss\"]\n    multiplex: true\n    code-link: true\n    fig-cap-location: bottom\n    fig-align: center\n    transition: none ## fade slide convex concave zoom\n    title-slide-attributes:\n      data-background-color: \"#447099\"\n      # data-background-image: images/paper-texture.jpg\n      # data-background-size: cover\n      # data-background-color: \"#698ED5\"\neditor: source\nexecute:\n  freeze: true\n  echo: true\n---\n\n\n#  {visibility=\"hidden\"}\n\n\\def\\bx{\\mathbf{x}}\n\\def\\bg{\\mathbf{g}}\n\\def\\bw{\\mathbf{w}}\n\\def\\bbeta{\\boldsymbol \\beta}\n\\def\\bX{\\mathbf{X}}\n\\def\\by{\\mathbf{y}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bW{\\mathbf{W}}\n\\def\\T{\\text{T}}\n\\def\\cov{\\mathrm{Cov}}\n\\def\\cor{\\mathrm{Corr}}\n\\def\\var{\\mathrm{Var}}\n\\def\\E{\\mathrm{E}}\n\\def\\bmu{\\boldsymbol \\mu}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\def\\Trace{\\text{Trace}}\n\n\n\n\n\n\n\n\n## {visibility=\"hidden\"}\n\n<br>\n\n<br>\n\n<br>\n\n<br>\n\n<br>\n\n:::{.center}\n### Simple Linear Regression (MATH 4720)\n### Categorical Predictors (MATH 2780/4780)\n### Multiple Linear Regression (MATH 2780/4780)\n:::\n\n## What is Regression\n- **Regression** models the relationship between a **numerical response variable $(Y)$** and one or more **numerical/categorical predictors $(X)$**, which is a **supervised learning** method in **machine learning**.\n\n- A **regression function** $f(X)$ describes how a response variable $Y$ generally changes as an explanatory variable $X$ changes.\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\nExamples:\n\n- <span style=\"color:blue\"> college GPA $(Y)$ vs. ACT/SAT score $(X)$</span>\n\n- <span style=\"color:blue\"> sales $(Y)$ vs. advertising expenditure $(X)$</span>\n\n- <span style=\"color:blue\"> crime rate $(Y)$ vs. median income level $(X)$ </span>\n:::\n\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/16-linear-reg/unnamed-chunk-2-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n::::\n\n\n## Simple Linear Regression\n<!-- Given the **training data** $(x_1, y_1), \\dots, (x_n, y_n)$, we *learn* (estimate) -->\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n\\begin{align*}\ny_i &= f(x_i) +  \\epsilon_i \\\\\n    &= \\beta_0 + \\beta_1~x_{i} + \\epsilon_i, \\quad i = 1, 2, \\dots, n\n\\end{align*}\n\n- $\\beta_0$ and $\\beta_1$ are *unknown parameters* to be learned or estimated.\n\n:::\n\n::: {.column width=\"60%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/16-linear-reg/unnamed-chunk-3-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n::::\n\n:::{.question}\nWhat are the assumption on $\\epsilon_i$?\n:::\n\n. . .\n\n$\\epsilon_i \\sim N(0, \\sigma^2)$ and hence $y_i \\mid x_i \\sim N(\\beta_0+\\beta_1x_i, \\sigma^2)$ or $\\mu_{y\\mid x_i} = \\beta_0+\\beta_1x_i$.\n\n\n\n::: notes\n- OK we start with the simple linear regression, the linear regression a Single Predictor $x$.\n- In linear regression, our regression function $f$ is a linear function $\\beta_0 + \\beta_1~x$. \n- Epsilon, the random error is there to capture any random measurement errors or any variations in $y$ that cannot be explained by the predictor $x$.\n- Given this model, we're interested in $\\beta_0$ (population parameter for the intercept) and $\\beta_1$ (population parameter for the slope) because once we know $\\beta_0$ and $\\beta_1$, we know the exact shape of $f$ and we know the relationship of $y$ and $x$, and given any value of $x$, we can predict its corresponding value of $y$ using the regression line $\\hat{y}_{i} = \\beta_0 + \\beta_1~x_{i}$.\n- But unfortunately, the population parameters are typically unknown to us.\n:::\n\n## Simple Linear Regression Assumptions\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/16-linear-reg/regression_line_sig_red.png){fig-align='center' width=100%}\n:::\n:::\n\n\n## Simple Linear Regression Assumptions\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/16-linear-reg/regression_line_data.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Simple Linear Regression Assumptions\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/16-linear-reg/regression_line_data_blue.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Ordinary Least Squares (OLS)\nGiven the **training data** $(x_1, y_1), \\dots, (x_n, y_n)$, use *sample statistics* $b_0$ and $b_1$ computed from the training data to\n\n- [inference:]{.green} estimate $\\beta_0$ and $\\beta_1$ \n\n- [fitting:]{.green} estimate $y_i$ or $f(x_i)$ at $x_i$ by its **fitted value** $$\\hat{y}_{i} = \\hat{f}(x_i) = b_0 + b_1~x_{i}$$\n\n- [prediction:]{.green} predict $y_j$ or $f(x_j)$ at $x_j$ by its **predicted value** $$\\hat{y}_{j} = \\hat{f}(x_j) = b_0 + b_1~x_{j}$$ where $(x_j, y_j)$ is *never seen and used in training* before.\n\n. . .\n\n- [Ordinary Least Squares:]{.green} We find $b_0$ and $b_1$, or regression line $b_0 + b_1x$ that minimizes the **sum of squared residuals**.\n- The residual $e_i = y_i - \\hat{y}_i$. The sample regression line minimizes $\\sum_{i = 1}^n e_i^2$.\n\n\n::: notes\n- How do we get  $b_0$ and $b_1$ that sort of well estimate $\\beta_0$ and $\\beta_1$? \n- We choose $b_0$ and $b_1$, or regression line $b_0 + b_1x$ that minimizes the **sum of squared residuals**.\n- If we define residual as $e_i = y_i - \\hat{y}_i$, then the sum of squared residuals is $\\sum_{i = 1}^n e_i^2$.\n- And this approach that estimates the population parameters $\\beta_0$ and $\\beta_1$ or the population regression line is called Ordinary Least Squares method.\n:::\n\n\n## Visualizing Residuals\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/16-linear-reg/vis-res-1-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n::: notes\nOK. That's see the idea of Ordinary Least Squares visually. Here just showed the data.\n- Do you see why some points are darker than some others?\n- A darker point means that there are several identical (x, y) pairs, or replicates in the data set.\n:::\n\n\n## Visualizing Residuals (cont.)\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/16-linear-reg/vis-res-2-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n::: notes\n- All right, with the data, this figure also shows the least squares regression line, and the fitted (predicted) value of $y$ for each $x$ in the training data, which are those red points.\n- The predicted values of y are right on the regression line.\n- Now the question is, how do we find this line?\n- Given a line, we can have predicted values of y, right?\n- Then what is residual on the plot? The residual will be the difference between the true observation y and the predicted value of y given any value of x.\n- So a residual in the plot will be a vertical bar at the value of x with two ends of the bar $y$ and $\\hat{y}$, right?\n- (Show on board)\n- (add $y_i = b_0+b_1x_i$ and residual line)\n:::\n\n\n\n## Visualizing Residuals (cont.)\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/16-linear-reg/vis-res-3-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n::: notes\n- Here shows all the residuals in vertical bars.\n- least squares line is the line such that the sum of all the squared residuals is minimized.\n- Why we square the residuals? \n- It's mathematically more convenient.\n- Squaring emphasizes larger differences\n:::\n\n\n# Fitting and Interpreting Regression Models\n\n\n## Predict Highway MPG `hwy` from Displacement `displ`\n\n$$\\widehat{hwy}_{i} = b_0 + b_1 \\times displ_{i}$$\n\n\n::: {.cell layout-align=\"center\" hash='16-linear-reg_cache/revealjs/hwy-displ-plot_47f4f3766e5107aad5f2833a0da9ce4e'}\n::: {.cell-output-display}\n![](images/16-linear-reg/hwy-displ-plot-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n::: notes\n- The data I just show you is the mpg data set in ggplot2.\n- Here we are trying to Predict Highway MPG `hwy` from Engine Displacement `displ`\n:::\n\n## R Built-in `lm()`  {visibility=\"hidden\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## lm_reg <- lm(formula = mpg$hwy ~ mpg$displ)\n(lm_reg <- lm(formula = hwy ~ displ, data = mpg))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nCoefficients:\n(Intercept)        displ  \n      35.70        -3.53  \n```\n:::\n:::\n\n\n$$\\widehat{hwy}_{i} = 35.7  -3.53 \\times displ_{i}$$\n\n<!-- ## Slope and Intercept -->\n\n<!-- $$\\widehat{hwy}_{i} = 35.7  -3.53 \\times displ_{i}$$ -->\n\n\n- **Slope:** When the engine displacement volume of a car is increased by one litre, the highway miles per gallon is expected to be lower, **on average**, by 3.53 miles.\n- **Intercept:** Cars that has engine displacement 0 litres are expected to have highway miles per gallon 35.7, on average. *(Does this make sense?)*\n\n\n::: notes\n- All right, so how do we interpret the regression line.\n- The interpretation of the slope is that When the engine displacement volume of a car is increased by one litre, the highway miles per gallon is expected to be lower, on average, by 3.351 miles.\n- Notice that I said on average. When displacement volume increases one unit, the highway MPG will not decrease 3.351 miles for every single car. Each car will have a different MPG, but on average, MPG decreases by 3.351 miles\n- And the intercept means Cars that has engine displacement 0 litres are expected to have highway miles per gallon 35.698, on average. *(Does this make sense?)*\n- Mathematically speaking, it's valid to say something like this. But in reality, there is no cars having zero displacement, it does not make sense to give this kind of statement. \n- When you say something about your model beyond the scope or range of your data, you got to be very careful. The statement may not be realistic, and the model assumption like linear relationship may not be reasonable or violated.\n:::\n\n\n## R Built-in `lm()`  {visibility=\"hidden\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntypeof(lm_reg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"list\"\n```\n:::\n\n```{.r .cell-code}\nnames(lm_reg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm_reg$fitted.values[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   1    2    3    4    5 \n29.3 29.3 28.6 28.6 25.8 \n```\n:::\n\n```{.r .cell-code}\nmpg$hwy[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 29 29 31 30 26\n```\n:::\n\n```{.r .cell-code}\nmpg$displ[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.8 1.8 2.0 2.0 2.8\n```\n:::\n:::\n\n\n\n## R Built-in `lm()`  {visibility=\"hidden\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(lm_reg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.my_class800}\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.104 -2.165 -0.224  2.059 15.010 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   35.698      0.720    49.5   <2e-16 ***\ndispl         -3.531      0.195   -18.1   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.84 on 232 degrees of freedom\nMultiple R-squared:  0.587,\tAdjusted R-squared:  0.585 \nF-statistic:  329 on 1 and 232 DF,  p-value: <2e-16\n```\n:::\n:::\n\n\n\n\n\n# {background-color=\"#A7D5E8\"}\n\n![](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/tidymodels.png){width=\"380\"}\n\n\n![](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/rsample.png){width=\"180\"}![](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/parsnip.png){width=\"180\"}![](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/recipes.png){width=\"180\"}![](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/workflows.png){width=\"180\"}![](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/tune.png){width=\"180\"}![](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/yardstick.png){width=\"180\"}![](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/broom.png){width=\"180\"}![](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/dials.png){width=\"180\"}\n\n\n<h1>[Tidymodels](https://www.tidymodels.org/)</h1>\n\n\n\n\n\n\n## Step 1: Specify Model: [`linear_reg()`](https://parsnip.tidymodels.org/reference/linear_reg.html)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nparsnip::linear_reg()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n:::\n:::\n\n\n[**parsnip**](https://parsnip.tidymodels.org/) package provides a tidy, unified interface for fitting models\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/parsnip.png){fig-align='center' width=30%}\n:::\n:::\n\n\n\n::: notes\nOne challenge with different modeling functions available in R that do the same thing is that they can have different interfaces and arguments. Note that the model syntax can be very different and that the argument names (and formats) are also different. This is a pain if you switch between implementations. \n- So how do we use tidymodels to actually fit a model. \n- Step 1 is to specify the model that we are building.\n- We are going to build a linear regression model, so the function we start with is linear_reg()\n- And you can see that the output is saying OK I'm ready to use this model specification.\n:::\n\n\n\n## Step 2: Set Model Fitting *Engine*\n\n- Use `lm()` in the built-in **stats** package.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg() |> \n    set_engine(\"lm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n:::\n:::\n\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nshow_engines(\"linear_reg\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 7 × 2\n  engine mode      \n  <chr>  <chr>     \n1 lm     regression\n2 glm    regression\n3 glmnet regression\n4 stan   regression\n5 spark  regression\n6 keras  regression\n7 brulee regression\n```\n:::\n:::\n\n\n\n::: notes\n- Step 2 is to define the model fitting engine.\n- For now we are going to use \"lm\" that is a function in the stats package to fit a linear model. \n- Basically, we ask R or tidymodel to use the lm() function as our computational engine to fit a linear regression model.\n- So you can think tidymodel as an platform or interface with many other R packages that do the work of model fitting, and tidymodel provides a consistent interface to them.\n- So you can actually use different R packages as different computational engines to fit the same model.\n- With tidymodel, you don't need to worry about different syntax should be used in different packages because the syntax is the same, and its outputs also remain the same even different computational engine is used.\n:::\n\n\n\n\n## Step 3: Fit Model & Estimate Parameters\n\n... using **formula syntax**\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(hwy ~ displ, data = mpg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\n\nCall:\nstats::lm(formula = hwy ~ displ, data = data)\n\nCoefficients:\n(Intercept)        displ  \n      35.70        -3.53  \n```\n:::\n:::\n\n\n\n$$\\widehat{hwy}_{i} = 35.7  -3.53 \\times displ_{i}$$\n\n\n- **Slope:** When the engine displacement volume of a car is increased by one litre, the highway miles per gallon is expected to be lower, *on average*, by 3.53 miles.\n\n<!-- - **Intercept:** Cars that has engine displacement 0 litres are expected to have highway miles per gallon 35.7, on average. *(Does this make sense?)* -->\n\n\n\n::: notes\n- Step 3, after selecting the computational engine, we fit the model and estimate parameters.\n- we use the fit() function, and inside the function, we provide the model fitting formula, response variable ~ predictors, followed by the data set's name.\n- Here our response is hwy, and the predictor is displ, and the data set is mpg.\n- After fitting the model, we can the model output. We can see our estimated coefficients intercept and slope for displ, which are $b_0$ and $b_1$ in previous slides.\n- And with the coefficients, we now can have the regression line, $\\widehat{hwy}_{i} = 35.698  -3.531 \\times displ_{i}$, where $\\widehat{hwy}_{i}$ is the predicted value of hwy MPG of the $i$-th observation.\n:::\n\n\n\n## Tidy Look at Model Output\n\n- Tidymodels output (**tibble**)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(hwy ~ displ, data = mpg) |> \n    tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)    35.7      0.720      49.6 2.12e-125\n2 displ          -3.53     0.195     -18.2 2.04e- 46\n```\n:::\n:::\n\n\n$$\\widehat{hwy}_{i} = 35.7  -3.53 \\times displ_{i}$$\n\n::: notes\n- We can print the model output as a tidy table using the tidy() function. \n- Here we not only have the point estimates in the fist column, but also have the standard error that measure the uncertainty of these coefficients, the test statistics, and p-values that are used in hypothesis testing.\n- We'll use standard error when quantifying the uncertainty about the regression line, but we won't talk about test statistics and p-values in this course because they are covered in 4720.\n:::\n\n\n\n\n\n\n##\n:::{.lab}\n\n<span style=\"color:blue\"> **20-Simple Linear Regression** </span>\n\nIn **lab.qmd** `## Lab 20` section,\n\n- Use the `mpg` data to fit a simple linear regression where $y$ is `hwy` and $x$ is `cty`.\n\n- Produce the plot below. (add the layer `geom_smooth(method = \"lm\", se = FALSE)`)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels); library(ggplot2)\n```\n\n::: {.cell-output-display}\n![](images/16-linear-reg/unnamed-chunk-20-1.png){fig-align='center' width=50%}\n:::\n:::\n\n:::\n\n\n\n\n::: notes\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nparsnip::linear_reg() %>% \n    set_engine(\"lm\") %>% \n    fit(hwy ~ displ, data = mpg) %>% \n    tidy()\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels); library(ggplot2)\nggplot(data = mpg, aes(x = cty, y = hwy)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"#003366\") + \n    labs(title = \"Highway MPG vs. City MPG\",\n         x = \"City MPG\",\n         y = \"Highway MPG\")\n```\n:::\n\n:::\n\n\n## Quantify Uncertainty about Coefficients\n\n- Uncertainty about regression coefficients $\\beta_0$ and $\\beta_1$\n\n::: {.cell layout-align=\"center\" hash='16-linear-reg_cache/revealjs/unnamed-chunk-23_fd41acaa086e8119cdd57fc9f98ca774'}\n\n```{.r .cell-code}\ntidymdl <- linear_reg() |> \n    set_engine(\"lm\")\n\nreg_out <- tidymdl |> \n    fit(hwy ~ displ, data = mpg)\n\nreg_out$fit$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)       displ \n      35.70       -3.53 \n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconfint(reg_out$fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            2.5 % 97.5 %\n(Intercept) 34.28  37.12\ndispl       -3.91  -3.15\n```\n:::\n:::\n\n\n<!-- . . . -->\n\n<!-- :::{.alert} -->\n<!-- $b_0$ and $b_1$ are negatively related. A larger (smaller) $b_0$ implies a smaller (larger) $b_1$. -->\n<!-- ::: -->\n\n\n\n## Quantify Uncertainty about Mean of $y$ \n:::: {.columns}\n\n::: {.column width=\"40%\"}\n- Uncertainty about the [**mean**]{.green} value of $y$ given $X = x$\n$$\\mu_{Y \\mid X = x} = \\beta_0 + \\beta_1x$$\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnew_input <- \n    data.frame(displ = 3:6)\n\npredict(reg_out$fit,\n        newdata = new_input,\n        interval = \"confidence\",\n        level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   fit  lwr  upr\n1 25.1 24.6 25.6\n2 21.6 21.0 22.1\n3 18.0 17.3 18.8\n4 14.5 13.4 15.6\n```\n:::\n:::\n\n:::\n\n\n\n::: {.column width=\"60%\"}\n\n::: {.panel-tabset}\n\n## Fit with uncertainty band\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(p_ci <- p + geom_smooth(method = \"lm\", \n                         color = \"#003366\", \n                         fill = \"blue\",\n                         se = TRUE))\n```\n\n::: {.cell-output-display}\n![](images/16-linear-reg/unnamed-chunk-26-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n## ggplot object p\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np <- ggplot(data = reg_out$fit, \n            aes(x = displ, y = hwy)) +\n     geom_point(alpha = 0.3) + \n     labs(title = \"Highway MPG vs. Engine Displacement\",\n          x = \"Displacement (litres)\",\n          y = \"Highway miles per gallon\") +\n      coord_cartesian(ylim = c(11, 44))\n```\n:::\n\n\n\n:::\n\n:::\n::::\n\n\n::: notes\n- We are also interested in uncertainty about the mean value of $y$ given some value of $x$, especially when we are predicting $y$.\n- To get the CI for the mean of $y$ at any value of $x$, we can use the predict() function.\n- The first argument in the function is the regression fitted result.\n- Then and argument new data is a data frame of predictor values.\n- Here, the predictor is at 1, 2, 3, up to 8.\n- And interval is confidence, and confidence level is 95%.\n- The output will have 3 columns. The first column shows the predicted or fitted value of $y$ given $x$, these are values right on the regression line.\n- The second and third columns are lower and upper bound respectively.\n- How do we show the confidence interval for the mean of y?\n- Again we use geom_smooth() layer, and set se = TRUE, so that geom_smooth() uses the standard error info to obtain the confidence interval for us. \n:::\n\n\n## Quantify Uncertainty about Individual $y$\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n- Uncertainty about the [**individual**]{.green} value of $y$ given $X = x$, $Y \\mid X = x$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(reg_out$fit, \n        newdata = new_input, \n        interval = \"prediction\", \n        level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   fit   lwr  upr\n1 25.1 17.53 32.7\n2 21.6 14.00 29.2\n3 18.0 10.45 25.6\n4 14.5  6.88 22.1\n```\n:::\n\n```{.r .cell-code}\n## predict at current inputs\ndf <- as.tibble(\n    predict(\n        reg_out$fit, \n        interval = \"prediction\"))\n```\n:::\n\n:::\n\n::: {.column width=\"60%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np_ci + \n    geom_line(aes(x = displ, y = df$lwr), \n              color = \"red\") +\n    geom_line(aes(x = displ, y = df$upr), \n              color = \"red\")\n```\n\n::: {.cell-output-display}\n![](images/16-linear-reg/unnamed-chunk-29-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n::::\n\n\n::: notes\n- We can also quantify the uncertainty about the value individual $y$ given $x$.\n- As you can imagine, this is more difficult to predict value of individual $y$ than to predict value of mean of $y$, right?\n- Because individual $y$ value varies more than the mean of $y$ due to the random error noises added to it.\n- And so with the same confidence level, the CI for individual $y$ will be much wider than the CI for the mean of $y$.\n- How do we get the CI?\n- The code is basically the same as before. But here instead of \"confidence\", here we use \"prediction\" in the interval argument.\n- You can see that the 95% interval is much wider, trying to capture or contain most of the observed values of $y$.\n:::\n\n\n\n# Model Checking\n\n::: notes\n- OK. Model checking. How do we know linear regression model is a good model for fitting our data. Well, we can assess the quality of the model by looking at residuals. Let's see how.\n:::\n\n\n\n<!-- ## Model Checking -->\n<!-- - We assumes a **linear** relationship between our predictors and responses. -->\n<!-- - But how do we assess the assumption? -->\n\n\n\n## Graphical Diagnostics: Residual Plot\n- Residuals distributed randomly around 0.\n- Check it by plotting residuals against the fitted value of $y$: $e_i$ vs. $\\hat{y}_i$\n- With no visible pattern along the x or y axis.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/16-linear-reg/unnamed-chunk-30-1.png){fig-align='center' width=56%}\n:::\n:::\n\n\n::: notes\nWe are looking for\n- Residuals distributed randomly around 0.\n- With no visible pattern along the $x$ or $y$ axes.\n- This can be checked by plotting residuals against the fitted value of $y$.\n- Here shows a good residual plot. Residuals are around 0, and its variation is more or less the same across different values of $\\hat{y}$.\n- There is no significant pattern in the plot.\n:::\n\n\n\n## Not looking for...\n\n:::{.large}\n**Fan shapes**\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/16-linear-reg/unnamed-chunk-31-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n::: notes\n- We are not looking for a plot that has a fan shape.\n:::\n\n## Not looking for...\n\n:::{.large}\n**Groups of patterns**\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/16-linear-reg/unnamed-chunk-32-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\n::: notes\n- We are not looking for a plot that has **Groups of patterns**\n:::\n\n\n## Not looking for...\n\n:::{.large}\n**Residuals correlated with predicted values**\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/16-linear-reg/unnamed-chunk-33-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\n::: notes\n- We also don't want Residuals to be correlated with predicted values\n:::\n\n\n\n## Not looking for...\n\n:::{.large}\n**Any patterns!**\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/16-linear-reg/unnamed-chunk-34-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n::: notes\n- We basically don't want the residuals show Any patterns!\n- If $X$ and $Y$ are not linearly related, or the residual plot exists some pattern, some data transformation is needed. Or use another model.\n:::\n\n\n## MPG Data Residuals\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(reg_out$fit, which = 1, col = \"blue\", las = 1)\n```\n\n::: {.cell-output-display}\n![](images/16-linear-reg/unnamed-chunk-35-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n\n\n# Models with Categorical Predictors\n\n\n\n## Categorical Predictor with 2 Categories\n\n:::: {.columns}\n\n::: {.column width=\"31%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmpg |>  \n  select(hwy, trans) |>  \n  print(n = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 234 × 2\n    hwy trans     \n  <int> <chr>     \n1    29 auto(l5)  \n2    29 manual(m5)\n3    31 manual(m6)\n4    30 auto(av)  \n# ℹ 230 more rows\n```\n:::\n:::\n\n\n<!-- ```{r echo=FALSE, cache=TRUE, eval=FALSE} -->\n<!-- mpg_data <- mpg -->\n<!-- mpg_data$trans[grepl(\"auto\", mpg_data$trans)] <- \"auto\" -->\n<!-- mpg_data$trans[grepl(\"manual\", mpg_data$trans)] <- \"manual\" -->\n\n<!-- mpg_data |>  -->\n<!--     select(hwy, trans) |>  -->\n<!--     print(n = 8) -->\n<!-- ``` -->\n:::\n\n\n\n::: {.column width=\"69%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmpg_new <- mpg\nmpg_new$trans[grepl(\"auto\", mpg_new$trans)] <- \"auto\"\nmpg_new$trans[grepl(\"manual\", mpg_new$trans)] <- \"manual\"\n\nmpg_new |> \n    select(hwy, trans) |> \n    print(n = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 234 × 2\n    hwy trans \n  <int> <chr> \n1    29 auto  \n2    29 manual\n3    31 manual\n4    30 auto  \n# ℹ 230 more rows\n```\n:::\n:::\n\n:::\n::::\n\n- `trans = auto`: Automatic transmission\n- `trans = manual`: Manual transmission\n\n\n::: notes\n- If you look at the `mpg` data set, you'll find that the variable transmission is a categorical variable. We either have Automatic transmission or Manual transmission. \n- Here I clean the data a little bit, so that the trans variable has values either auto or manual.\n- And I am going to use this trans variable as our predictor to predict the hwy MPG. OK.\n:::\n\n\n## Highway MPG & Transmission Type\n\n- Make sure that your categorical variable is of type **character** or **factor**.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntypeof(mpg_new$trans)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"character\"\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidymdl |> \n    fit(hwy ~ trans, data = mpg_new) |> \n    tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)    22.3      0.458     48.7  8.60e-124\n2 transmanual     3.49     0.798      4.37 1.89e-  5\n```\n:::\n:::\n\n\n- The baseline level is chosen to be `auto` transmission. \n\n\n::: notes\n- Before fitting the regression model, Make sure that your categorical variable is of type character or factor. If not, convert the type before fitting.\n- After fitting the model, you can see that the name in the second row for the slope is transmanual, and it means something because R actually does something for us when fitting a regression model.\n- Actually, it means that the baseline level is chosen to be auto transmission. \n:::\n\n\n## Highway MPG & Transmission Type\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)    22.3      0.458     48.7  8.60e-124\n2 transmanual     3.49     0.798      4.37 1.89e-  5\n```\n:::\n:::\n\n\n$$\\widehat{hwy_{i}} = 22.3 + 3.49~trans_i$$\n\n- **Slope:** Cars with **manual** transmission are expected, on average, to be 3.49 more miles per gallon than cars with **auto** transmission.\n    - Compare baseline level (`trans = auto`) to the other level (`trans = manual`)\n- **Intercept:** Cars with **auto** transmission are expected, on average, to have 22.3 highway miles per gallon.\n\n\n::: notes\n- Here the regression line is $\\widehat{hwy_{i}} = 22.3 + 3.49~trans_i$.\n- And we interpret the slope and intercept as follows.\n- Cars with **manual** transmission are expected, on average, to be 3.48 more miles per gallon than cars with **auto** transmission.\n- We Compare baseline level (`trans = auto`) to the other level (`trans = manual`).\n- **Intercept** here is Cars with **auto** transmission are expected, on average, to have 22.3 highway miles per gallon.\n- So basically, the intercept value 22.3 here is the average hwy MPG for cars with auto transmission, and intercept + slope (22.3 + 3.49) is the average hwy MPG for cars with manual transmission.\n:::\n\n\n\n# {background-color=\"#ffde57\" background-image=\"https://upload.wikimedia.org/wikipedia/commons/0/05/Scikit_learn_logo_small.svg\" background-size=\"40%\" background-position=\"90% 50%\"}\n\n\n::: {.left}\n<h1> sklearn.linear_model.LinearRegression </h1>\n\n:::\n\n## [sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(reticulate)\npy_install(\"scikit-learn\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n```\n:::\n\n\n. . .\n\n<br>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nmpg1 = pd.read_csv('./data/mpg.csv')\n# x = np.array(mpg1['displ']).reshape(-1, 1)\nx = np.array(mpg1[['displ']]) ## 2d array with one column\ny = np.array(mpg1['hwy'])\nx[0:4]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([[1.8],\n       [1.8],\n       [2. ],\n       [2. ]])\n```\n:::\n\n```{.python .cell-code}\ny[0:4]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([29, 29, 31, 30])\n```\n:::\n:::\n\n\n\n\n\n\n::: notes\n-1 in reshape function is used when you dont know or want to explicitly tell the dimension of that axis. E.g,\nIf you have an array of shape (2,4) then reshaping it with (-1, 1), then the array will get reshaped in such a way that the resulting array has only 1 column and this is only possible by having 8 rows, hence, (8,1).\n:::\n\n\n\n## sklearn.linear_model.LinearRegression\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nreg = LinearRegression().fit(x, y)\nreg.coef_\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([-3.53058881])\n```\n:::\n\n```{.python .cell-code}\nreg.intercept_\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n35.6976510518446\n```\n:::\n:::\n\n\n\n\n. . .\n\n<br>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nnew_input = np.arange(3, 7, 1).reshape(-1, 1)\npred = reg.predict(new_input)\npred\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([25.10588463, 21.57529583, 18.04470702, 14.51411821])\n```\n:::\n:::\n\n\n<!-- ## Prediction -->\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}