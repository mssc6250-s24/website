{
  "hash": "0fc930d792020c384c3d70ba4c9ecd83",
  "result": {
    "markdown": "---\ntitle: \"Artificial Neural Networks ðŸ™Œ\"\nsubtitle: \"MSSC 6250 Statistical Machine Learning\"\nauthor: \"Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University\"\n# date: \"January 08 2024\"\n# macros: _macros.tex # import a list of TeX/LaTeX definitions\nformat: \n  revealjs:\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n    # include-in-header:\n    #     - \"macros.tex\"\n    highlight-style: github\n    code-block-bg: true\n    self-contained: false\n    slide-number: c/t\n    incremental: false\n    width: 1800\n    height: 1000\n    margin: 0.05\n    logo: \"https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg\"\n    footer: \"[mssc6250-s24.github.io/website](https://mssc6250-s24.github.io/website/)\"\n    theme: [\"simple\", \"styles.scss\"]\n    echo: false\n    multiplex: true\n    code-link: true\n    fig-cap-location: bottom\n    fig-align: center\n    transition: none ## fade slide convex concave zoom\n    title-slide-attributes:\n      data-background-color: \"#447099\"\n      # data-background-image: images/paper-texture.jpg\n      # data-background-size: cover\n      # data-background-color: \"#698ED5\"\neditor: source\nexecute:\n  freeze: true\n  echo: false\n---\n\n\n\n# {visibility=\"hidden\"}\n\n\n\n\n\\def\\cD{{\\cal D}}\n\\def\\cL{{\\cal L}}\n\\def\\cX{{\\cal X}}\n\\def\\cF{{\\cal F}}\n\\def\\cH{{\\cal H}}\n\\def\\bA{\\mathbf{A}}\n\\def\\bB{\\mathbf{B}}\n\\def\\bX{\\mathbf{X}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bU{\\mathbf{U}}\n\\def\\bD{\\mathbf{D}}\n\\def\\bV{\\mathbf{V}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bW{\\mathbf{W}}\n\\def\\bZ{\\mathbf{Z}}\n\\def\\bK{\\mathbf{K}}\n\\def\\bx{\\mathbf{x}}\n\\def\\by{\\mathbf{y}}\n\\def\\bs{\\mathbf{s}}\n\\def\\br{\\mathbf{r}}\n\\def\\bu{\\mathbf{u}}\n\\def\\be{\\mathbf{e}}\n\\def\\bv{\\mathbf{v}}\n\\def\\bp{\\mathbf{p}}\n\\def\\bw{\\mathbf{w}}\n\\def\\bz{\\mathbf{z}}\n\\def\\bzero{\\mathbf{0}}\n\\def\\balpha{\\boldsymbol \\alpha}\n\\def\\bbeta{\\boldsymbol \\beta}\n\\def\\btheta{\\boldsymbol \\theta}\n\\def\\bSigma{\\boldsymbol \\Sigma}\n\\def\\bxi{\\boldsymbol \\xi}\n\\def\\bmu{\\boldsymbol \\mu}\n\\def\\bep{\\boldsymbol \\epsilon}\n\\def\\T{\\text{T}}\n\\def\\Trace{\\text{Trace}}\n\\def\\Cov{\\text{Cov}}\n\\def\\Corr{\\text{Corr}}\n\\def\\Var{\\text{Var}}\n\\def\\E{\\text{E}}\n\\def\\pr{\\text{pr}}\n\\def\\Prob{\\text{P}}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\n\n\n\n\n\n\n\n\n\n\n\n# Feed-forward Neural Networks \n\n<!-- {background-color=\"#447099\"} -->\n\n\n## Neural Networks\n- An (artifical) **neural network** is a machine learning model inspired by the biological neural networks that constitute animal brains.\n\n::: columns\n::: {.column width=\"70%\"}\n![Source: Wiki](https://upload.wikimedia.org/wikipedia/commons/4/44/Neuron3.png){width=\"1000\"}\n:::\n\n:::{.column width=\"30%\"}\n![](https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg){width=\"600\"}\n:::\n\n:::\n\n\n## Deep Learning\n- A neural network takes an input vector of $p$ variables $X = (X_1, X_2, \\dots, X_p)$ and builds a nonlinear function $f(X)$ to predict the response $Y$.\n\n- A neural network with several hidden layers is called a deep neural network, or **deep learning**.\n\n![Source: ISL Ch 10](./images/17-neural-net/deeplearning.png){fig-align=\"center\" width=\"1600\"}\n\n\n##\n\n<!-- <center> -->\n<!-- [<img src=\"neural-network-banner.gif\" height=\"250\"/>](http://brainstormingbox.org/a-beginners-guide-to-neural-networks/){target='_blank'} -->\n<!-- </center> -->\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: http://brainstormingbox.org/a-beginners-guide-to-neural-networks/](http://brainstormingbox.org/wp-content/uploads/2020/08/nural-network-banner.gif){fig-align='center' width=90%}\n:::\n:::\n\n\n\n\n\n## Single (Hidden) Layer Neural Network with One Output\n\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\nStarts from inputs $X$, for each **hidden neuron** $A_k$, $k = 1, \\dots, K$,\n\n- $A_k(X) = g(w_{k0} + w_{k1}X_1 + \\cdots + w_{kp}X_p)$\n\n- $f(X) = g_f(\\beta_0 + \\beta_1A_1(X) + \\cdots + \\beta_KA_K(X))$\n\n- $g_k(z)$ and $g_f(z)$ are (non)linear **activation functions** that are specified in advance.\n\n- $\\beta_0, \\dots, \\beta_K$ and $w_{10}, \\dots, w_{1p}, \\dots, w_{K0}, \\dots, w_{Kp}$ are parameters to be estimated.\n\n\n\n:::\n\n::: {.column width=\"40%\"}\n\n- $p = 4, K = 5$\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: ISL Fig. 10.1](./images/17-neural-net/10_1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::::\n\n\n::: notes\nWe can think of each Ak as a different transformation hk(X) of the original function\nfeatures, much like the basis functions\n:::\n\n\n\n## Linear Regression as Neural Network\n\n:::{.question}\n\nCan we represent a linear regression model as a neural network?\n\n:::\n\n\n. . .\n\n- YES! Linear regression is a single layer neural network with \n\n  + identity activation $g(z) = z$ and $g_f(z) = z$\n  + $p = K$\n  + $w_{kk} = 1$ and $w_{kj} = 0$ for all $k \\ne j$\n\n\n\n\n::: notes\nhttps://ml-explained.com/blog/kernel-pca-explained\n:::\n\n## Logistic Regression as Neural Network\n\n:::{.question}\n\nCan we represent a binary logistic regression model as a neural network?\n\n:::\n\n\n. . .\n\n- YES! Binary logistic regression is a single layer neural network with \n\n  + identity activation $g(z) = z$\n  + **sigmoid activation** $g_f(z) = \\frac{e^{z}}{1+e^z}$\n  + $p = K$\n  + $w_{kk} = 1$ and $w_{kj} = 0$ for all $k \\ne j$\n  \n  \n## Activation Functions\n\nActivation functions are usually continuous for optimization purpose.\n\n* sigmoid: $\\frac{1}{1+e^{-z}} = \\frac{e^z}{1+e^z}$\n* hyperbolic tangent: $\\frac{e^z - e^{-z}}{e^z + e^{-z}}$\n* rectified linear unit (ReLU): $\\max(0, z)$; $\\ln(1 + e^z)$ (Soft version)\n* Gaussian-error linear unit (GELU): $z \\Phi(z)$, where $\\Phi(\\cdot)$ is the $N(0, 1)$ CDF\n\n<!-- ## Convolutional Neural Networks -->\n\n## \n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/17-neural-net/unnamed-chunk-4-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Choose a Activation Function\n\n- The activation function used in hidden layers is typically chosen based on the type of neural network architecture.\n\n  + Multilayer Perceptron (MLP): ReLU (leaky ReLU)\n  + Convolutional Neural Network: ReLU\n  + Recurrent Neural Network: Tanh and/or Sigmoid\n  \n- never use softmax and identity functions in the hidden layers.\n\n\n- For output activation function,\n\n  + Regression: One node, Linear\n  + Binary Classification: One node, Sigmoid\n  + Multiclass Classification: One node per class, Softmax\n  + Multilabel Classification: One node per class, Sigmoid\n\n::: notes\nFinally, we make an remark that a single hidden layer neural network is sufficient for approximating any continuous function\n- https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/\n- https://towardsdatascience.com/how-to-choose-the-right-activation-function-for-neural-networks-3941ff0e6f9c\n:::\n\n\n## When to Use Deep Learning\n\n- The signal to noise ratio is high.\n\n- The sample size is huge.\n\n- Interpretability of the model is not a priority.\n\n. . .\n\n- When possible, try the simpler models as well, and then make a choice\nbased on the performance/complexity tradeoff.\n\n- Occamâ€™s razor principle: when faced with several methods that give roughly equivalent performance, pick the simplest.\n\n\n## Fitting Neural Networks\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: ISL Fig. 10.1](./images/17-neural-net/10_1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n$$\\min_{\\bbeta, \\{\\bw\\}_1^K} \\frac{1}{2}\\sum_{i=1}^n\\left(y_i - f(x_i) \\right)^2,$$ where $$f(x_i) = \\beta_0 + \\sum_{k=1}^K\\beta_Kg\\left( w_{k0} + \\sum_{j=1}^pw_{kj}x_{xj}\\right).$$\n\n- This problem is difficult because the objective is non-convex: there are multiple solutions.\n\n:::\n\n::::\n\n\n## Gradient Descent\n\n- For the objective function $R(\\theta)$ and the parameter vector $\\theta$ to be estimated, gradient descent keeps finding new parameter value that reduces the objective until the objective fails to decrease.\n\n- $\\theta^{(t+1)} = \\theta^{(t)} -\\rho\\nabla R(\\theta^{(t)})$, where $\\rho$ is the learning rate that is typically small like 0.001.\n\n\n- $\\nabla R(\\theta^{(t)}) = \\left. \\frac{\\partial R(\\theta)}{\\partial \\theta} \\right  \\rvert_{\\theta = \\theta^{(t)}}$\n\n\n## [Backpropagation](https://www.nature.com/articles/323533a0)\n\n::: {.midi}\n\\begin{align}\nR(\\theta) \\overset{\\triangle}{=} \\sum_{i=1}^nR_i(\\theta) =& \\frac{1}{2}\\sum_{i=1}^n \\big(y_i - f_{\\theta}(x_i)\\big)^2\\\\\n=& \\frac{1}{2} \\sum_{i=1}^n \\big(y_i - \\beta_0 - \\beta_1 g(w_1' x_i) - \\cdots - \\beta_K g(w_K' x_i) \\big)^2 \\\\\n\n\\end{align}\n:::\n. . .\n\n- Nothing but chain rule for differentiation:\n\nWith $z_{ik} = w_k' x_i$,\n\n::: {.midi}\n$$\\frac{\\partial R_i(\\theta)}{\\partial \\beta_{k}} = \\frac{\\partial R_i(\\theta)}{\\partial f_{\\theta}(x_i)} \\cdot \\frac{\\partial f_{\\theta}(x_i)}{\\partial \\beta_{k}} =  {\\color{red}{-\\big( y_i - f_{\\theta}(x_i)\\big)}} \\cdot g(z_{ik})$$\n:::\n\n::: {.midi}\n$$\\frac{\\partial R_i(\\theta)}{w_{kj}} = \\frac{\\partial R_i(\\theta)}{\\partial f_{\\theta}(x_i)} \\cdot \\frac{\\partial f_{\\theta}(x_i)}{\\partial g(z_{ik})} \\cdot \\frac{\\partial g(z_{ik})}{\\partial z_{ik}} \\cdot \\frac{\\partial z_{ik}}{\\partial w_{kj}} =  {\\color{red}{-\\big( y_i - f_{\\theta}(x_i)\\big)}} \\cdot \\beta_k \\cdot g'(z_{ik}) \\cdot x_{ij}$$\n:::\n\n. . .\n\n- Reuse the red part in different layers of gradients to reduce computational burden.\n\n\n\n<!-- \\begin{align} -->\n<!-- \\frac{\\partial R_i}{\\partial \\beta_{k}} =& \\frac{\\partial R_i}{\\partial f_{\\theta}} \\frac{\\partial f_{\\theta}}{\\partial \\beta_{k}} = \\color{red}{-\\big( y_i - f_{\\theta}(x_i)\\big)} g(z_{ik}), \\\\ -->\n\n<!-- \\quad \\frac{\\partial R_i}{w_{kj}} = \\frac{\\partial R_i}{\\partial f_{\\theta}}\\frac{\\partial f_{\\theta}}{\\partial g(z_{ik}} \\frac{\\partial g(z_{ik}}{\\partial z_{ik} \\frac{\\partial z_{ik}{\\partial w_{kj} &= \\color{red}{\\big( y_i - f_{\\theta}(x_i)\\big)} \\beta_k g'(z_{ik}) x_{ij}. -->\n<!-- \\end{align} -->\n\n\n\n\n\n## Other Topics\n\n- Stochastic Gradient Descent\n\n- Dropout Learning\n\n- Convolutional Neural Network (Spatial modeling)\n\n- Recurrent Neural Network (Temporal modeling)\n\n- Bayesian Deep Learning\n\n\n::: notes\nhttps://stackoverflow.com/questions/70977755/could-not-find-a-version-that-satisfies-the-requirement-tensorflow-python3-9-6\nhttps://developer.apple.com/metal/tensorflow-plugin/\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}