{
  "hash": "ead63b2bd14ba582b51a476e37929548",
  "result": {
    "markdown": "---\ntitle: \"Regression Diagnostics - Linearity ‚èØ\"\nsubtitle: \"MATH 4780 / MSSC 5780 Regression Analysis\"\nauthor: \"Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University\"\n# date: \"November 02 2023\"\n# macros: _macros.tex # import a list of TeX/LaTeX definitions\nformat: \n  revealjs:\n    code-line-numbers: false\n    #     - \"macros.tex\"\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n    # include-in-header:\n    highlight-style: arrow\n    code-block-bg: true\n    self-contained: false\n    slide-number: c/t    \n    incremental: false\n    width: 1800\n    height: 1000\n    margin: 0.05\n    logo: \"https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg\"\n    footer: \"[math4780-f23.github.io/website](https://math4780-f23.github.io/website/)\"\n    theme: [\"simple\", \"slides.scss\"]\n    multiplex: true\n    code-link: true\n    fig-cap-location: bottom\n    fig-align: center\n    transition: none ## fade slide convex concave zoom\n    title-slide-attributes:\n      data-background-color: \"#447099\"\n      # data-background-image: images/paper-texture.jpg\n      # data-background-size: cover\n      # data-background-color: \"#698ED5\"\neditor: source\nexecute:\n  freeze: true\n  echo: false\n  purl: true\n---\n\n\n#  {visibility=\"hidden\"}\n\n\\def\\bx{\\mathbf{x}}\n\\def\\bg{\\mathbf{g}}\n\\def\\bw{\\mathbf{w}}\n\\def\\balpha{\\boldsymbol \\alpha}\n\\def\\bbeta{\\boldsymbol \\beta}\n\\def\\beps{\\boldsymbol \\epsilon}\n\\def\\bX{\\mathbf{X}}\n\\def\\by{\\mathbf{y}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bW{\\mathbf{W}}\n\\def\\T{\\text{T}}\n\\def\\cov{\\mathrm{Cov}}\n\\def\\cor{\\mathrm{Corr}}\n\\def\\var{\\mathrm{Var}}\n\\def\\E{\\mathrm{E}}\n\\def\\bmu{\\boldsymbol \\mu}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\def\\Trace{\\text{Trace}}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n# Model Adequacy Checking and Correction\n\n<h2> Non-normality</h2>\n<h2> Non-constant Error Variance</h2>\n<h2> <span style=\"color:red\"> Non-linearity and Lack of Fit</span> </h2>\n\n\n\n## Assumptions of Linear Regression\n$Y_i= \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\dots + \\beta_kX_{ik} + \\epsilon_i$\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n- $E(Y \\mid X)$ and $X$ are linearly related.\n- $\\small E(\\epsilon_i) = 0$\n- $\\small \\var(\\epsilon_i) = \\sigma^2$\n- $\\small \\cov(\\epsilon_i, \\epsilon_j) = 0$ for all $i \\ne j$.\n- $\\small \\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)$ (for statistical inference)\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/10-diag-linearity/regression_line_sig.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n::::\n\n\n::: alert\n- Assuming $E(\\epsilon) = 0$ implies that the regression surface captures the dependency of the conditional mean of $y$ on the $x$s.\n- Violating linearity implies that the model fails to represent the relationship between the mean response and the regressors. (Lack of fit)\n:::\n\n\n\n## Detecting Nonlinearity (CIA Example)\n- Scatterplot $y$ against each $x$ can be *misleading*! It shows the **marginal** relationship between $y$ and each $x$, without controlling the level of other regressors.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/10-diag-linearity/unnamed-chunk-4-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n## Detecting Nonlinearity: Residual Plots\n- Care about the **partial** relationship between $y$ and each $x$ with impact of other $x$s controlled.\n\n- Residual-based plots are more relevant in detecting the departure of linearity.\n\n. . .\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n- Residual plots cannot distinguish between monotone and non-monotone nonlinearity.\n- The distinction:\n  + Monotone: just transform $x$ to $x^2$\n  + Non-monotone: need quadratic form\n:::\n\n::: {.column width=\"70%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/10-diag-linearity/unnamed-chunk-5-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n::::\n\n\n::: notes\n- The OLS ensures that the residuals and fitted values are uncorrelated.\n:::\n\n\n## Detecting Nonlinearity: Partial Residual Plots\n\n- **Partial residual plots** (Component-plus-Residual Plot) are for diagnosing nonlinearity.\n\n- The partial residuals for $x_j$: $$e_i^{(j)} = b_jx_{ij} + e_i$$ \n  + $b_j$ is the coefficient of $x_j$ in the full multiple regression\n  + $e_i$s are the residuals from the full multiple regression\n \n- Partial residual plot $e_i^{(j)}$ vs. $x_{ij}$ for $x_j$\n\n::: notes\n- The nonlinear pattern can be highlighted or emphasized \n- Added-variable plots, for detecting influential data, are partial plots, but they don't work well for detecting nonlinearity because they are biased towards linearity\n- Cook (1993): if the regressions of x_j on the other xs are approximately linear, then the regression function in the crPlot provides a visualization of transformation.\n- If the regressions among the predictors are strongly nonlinear and not well described by polynomials, then cvPlot may not be effective in recovering nonlinear partial relationships. => Use CERES plots (ceresPlots())\n- And cvPlots can appear nonlinear even when the true partial regression is linear => called leakage.\n\n:::\n\n\n## [R Lab]{.pink} Partial Residual Plots\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nlogciafit <- lm(log(infant) ~ gdp + health + gini, data = CIA)\n# Component-plus-Residual Plot \ncar::crPlots(logciafit, ylab = \"partial residual\", layout = c(1, 3), grid = FALSE, main = \"\") \n```\n\n::: {.cell-output-display}\n![](./images/10-diag-linearity/unnamed-chunk-6-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n## Transformation for Linearity\n\n- **Monotone, simple**: Power transformation on $x$ and/or $y$\n- **Monotone, not simple**: Polynomial regression (next week) or regression splines (MSSC 6250)\n- **Non-Monotone, simple**: Quadratic regression $y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\epsilon$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/10-diag-linearity/unnamed-chunk-7-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n::: notes\n- not simple: direction of curvature changes\n- Transform an $x$ rather than $y$, unless we see a common pattern of nonlinearity in the partial relationships of $y$ to many $x$s.\n- Transforming $y$ changes the shape of its relationship to **all** of the $x$s, and also changes the shape of the residual distribution.\n:::\n\n\n## Bulging Rule for Simple Monotone Nonlinearity\n\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n| The bulge points  | Transform  | Ladder of powers/roots  |\n|:---------------:|:-------------:|:------:|\n| **left**        | $x$           | **down**, e.g., $\\log(x)$ |\n| **right**       | $x$           |  **up** |\n| **down**        | $y$           |   **down** |\n| **up**          | $y$           |   **up** |\n\n\n\n<!-- - **left**: transform $x$ **down** the ladder of powers and roots, e.g. $\\log(x)$. -->\n\n<!-- - **right**: transform $x$ **up** the ladder, e.g. $x^2$. -->\n\n<!-- - **down**: transform $y$ **down** the ladder. -->\n\n<!-- - **up**: transform $y$ **up** the ladder. -->\n\n::: alert\n- Prefer to transform an $x$ rather than $y$, unless we see a common pattern of nonlinearity in the partial relationships of $y$ to many $x$s.\n:::\n\n\n:::\n\n\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/10-diag-linearity/unnamed-chunk-8-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n::::\n\n\n\n\n::: notes\nMosteller and Tukey's (1977)\n- Transform an $x$ rather than $y$, unless we see a common pattern of nonlinearity in the partial relationships of $y$ to many $x$s.\n- Transforming $y$ changes the shape of its relationship to **all** of the $x$s, and also changes the shape of the residual distribution.\n:::\n\n\n\n## Transformation on $x$s\n\n- `gdp` to `log(gdp)`\n- `health` to `health + health^2`\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/10-diag-linearity/unnamed-chunk-9-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## [R Lab]{.pink} Partial Residual Plots\n\n::: midi\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nlogciafit2 <- update(logciafit, . ~ log(gdp) + poly(health, degree = 2, raw = TRUE) + gini)\ncar::crPlots(logciafit2, ylab = \"partial residual\", layout = c(1, 3), grid = FALSE, main = \"\") \n```\n\n::: {.cell-output-display}\n![](./images/10-diag-linearity/unnamed-chunk-10-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: notes\nif true, use raw and not orthogonal polynomials.\n- If the partial relationship of `log(infant)` to health expenditure is quadratic, its partial residual plot should be linear.\n:::\n\n## [R Lab]{.pink} Improving Model Performance\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\ncar::brief(logciafit, digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           (Intercept)     gdp health   gini\nEstimate          3.02 -0.0439 -0.055 0.0216\nStd. Error        0.29  0.0037  0.022 0.0061\n\n Residual SD = 0.59 on 130 df, R-squared = 0.71 \n```\n:::\n\n```{.r .cell-code  code-line-numbers=\"false\"}\ncar::brief(logciafit2, digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           (Intercept) log(gdp) poly(health, degree = 2, raw = TRUE)1\nEstimate          4.65   -0.720                                -0.221\nStd. Error        0.32    0.038                                 0.058\n           poly(health, degree = 2, raw = TRUE)2   gini\nEstimate                                  0.0096 0.0191\nStd. Error                                0.0034 0.0044\n\n Residual SD = 0.44 on 129 df, R-squared = 0.84 \n```\n:::\n:::\n\n\n\n## [R Lab]{.pink} Plotting against Original Untransformed $x$\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-line-numbers=\"false\"}\nlibrary(effects)\npar(mar = c(2, 2, 0, 0))\nplot(Effect(\"gdp\", logciafit2, residuals = TRUE), \n     lines = list(col = c(\"blue\", \"black\"), lty = 2), \n     axes = list(grid = TRUE), confint = FALSE, \n     partial.residuals = list(plot = TRUE, smooth.col = \"magenta\", \n                              lty = 1, \n                              span = 3/4), \n     xlab = \"GDP per Capita\", ylab = \"Partial Residual\", main = \"\", cex.lab = 2)\n```\n\n::: {.cell-output-display}\n![](./images/10-diag-linearity/unnamed-chunk-12-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-line-numbers=\"false\"}\npar(mar = c(2, 2, 0, 0))\nplot(Effect(\"health\", logciafit2, residuals = TRUE), \n     lines = list(col = c(\"blue\", \"black\"), lty = 2), \n     axes = list(grid = TRUE), confint = FALSE, \n     partial.residuals = list(plot = TRUE, smooth.col = \"magenta\", \n                              lty = 1, \n                              span = 3/4),\n     xlab = \"Health Expenditures\", ylab = \"Partial Residual\", main = \"\", cex.lab = 2)\n```\n\n::: {.cell-output-display}\n![](./images/10-diag-linearity/unnamed-chunk-13-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n::::\n\n\n::: notes\n- plot(Effect(\"gdp\", logciafit2, residuals = TRUE))\n- $\\epsilon^{(1)}_i = f_1(x_{i1}) + e_i$ either if the partial-regression function $f_1(x_1)$ is linear after all or if the other $x$s are each linearly related to $x_1$.\n\n- In practice, it's only strongly nonlinearly related $x$s that seriously threaten the validity of component-plus-residuals plots.\n\n- Correlation between x1 and x2 can induce spurious nonlinearity in the CR plot for x1.\n\n- trying to correct nonlinearity for one x at a time, but in my experience, it's rarely necessary to proceed sequentially.\n:::\n\n\n## Transforming $x$s Analytically: Box and Tidwell (1962)\n\n- Box and Tidwell (1962) proposed a procedure for estimating $\\lambda_1, \\lambda_2, \\dots, \\lambda_k$ in the model\n$$y = \\beta_0 + \\beta_1x_1^{\\lambda_1} + \\cdots + \\beta_kx_k^{\\lambda_k}+ \\epsilon$$\n\n- All $x_j$s are positive.\n\n- $\\beta_0, \\beta_1, \\dots,  \\beta_k$ are estimated *after and conditional on* the transformations.\n\n- $x_j^{\\lambda_j} = \\log_e(x_j)$ if $\\lambda_j = 0$.\n\n\n\n::: notes\n- If some of the xs (for instance, dummy regressors) aren't candidates for transformation, then these xs can simply enter the model linearly.\n- all positive xs\n:::\n\n\n## [R Lab]{.pink} Box and Tidwell (1962)\n\nConsider the model $$\\log(Infant) = \\beta_0 + \\beta_1 GDP^{\\lambda_1} + \\beta_2Gini^{\\lambda_2} + \\beta_3 Health + \\beta_4 Health ^ 2 + \\epsilon$$\n\n\n::: {.cell layout-align=\"center\" output.lines='[1,2,3,4,5]'}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\ncar::boxTidwell(log(infant) ~ gdp + gini, \n                other.x = ~poly(health, 2, raw = TRUE), data = CIA)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n...\n     MLE of lambda Score Statistic (t) Pr(>|t|)    \ngdp            0.2                10.6   <2e-16 ***\ngini          -0.5                -0.4      0.7    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n...\n```\n:::\n:::\n\n\n- The point estimate of $\\lambda$ is $\\hat{\\lambda}_1 = 0.2$ and $\\hat{\\lambda}_2 = -0.5$\n\n- The test is for $H_0:$ No transformation is needed $(\\lambda = 1)$.\n  + Strong evidence to transform $GDP$\n  + Little evidence of the need to transform the Gini coefficient\n  \n\n## Other Methods for Dealing with Nonlinearity\n\n- Lack-of-fit test (LRA Sec 4.5, CMR Sec. 3.6): Need repeated observations\n\n. . .\n\n- Transform a nonlinear function into a linear one (LRA Sec 5.3)\n\n::: question\nCan the nonlinear model $y = \\beta_0e^{\\beta_1x}\\epsilon$ be transformed into a linear one (**intrinsically linear**)?\n:::\n\n. . .\n\n- Polynomial Regression, Regression Splines or other nonparametric regression (MSSC 6250)\n\n. . .\n\n- A (pure) nonlinear model may be needed if the model assumptions cannot be satisfied.\n\n\n\n",
    "supporting": [
      "10-diag-linearity_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}